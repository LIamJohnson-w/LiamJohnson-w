{"meta":{"title":"Hexo","subtitle":"","description":"","author":"JonhsonLiam","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"","slug":"2023.09.17","date":"2023-10-06T14:12:41.765Z","updated":"2023-09-16T12:46:21.000Z","comments":true,"path":"2023/10/06/2023.09.17/","link":"","permalink":"http://example.com/2023/10/06/2023.09.17/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"【FLink教育】Hudi整合Hive实现湖仓一体","slug":"2023.09.16","date":"2023-09-15T16:00:00.000Z","updated":"2023-09-16T12:42:24.000Z","comments":true,"path":"2023/09/16/2023.09.16/","link":"","permalink":"http://example.com/2023/09/16/2023.09.16/","excerpt":"","text":"数据湖简介数仓和数据湖数据仓库数据仓库（英语：Data Warehouse，简称数仓、DW），是一个用于存储、分析、报告的数据系统。 数据仓库的目的是构建面向分析的集成化数据环境，分析结果为企业提供决策支持（DecisionSupport）。 数据仓库的特点是本身不产生数据，也不最终消费数据。 每个企业根据自己的业务需求可以分成不同的层次。但是最基础的分层思想，理论上分为三个层：操作型数据层（ODS）、数据仓库层（DW）和数据应用层（DA）。 数据湖数据湖是一个集中式数据存储库，用来存储大量的原始数据，使用平面架构来存储数据。 数据湖一个以原始格式（通常是对象块或文件）存储数据的系统或存储库，通常是所有企业数据的单一存储。 数据湖可以包括来自关系数据库的结构化数据（行和列）、半结构化数据（CSV、日志、XML、JSON）、非结构化数据（电子邮件、文档、pdf）和二进制数据（图像、音频、视频）。 数据湖中数据，用于报告、可视化、高级分析和机器学习等任务。 数据仓库VS数据湖 湖仓一体 湖仓一体（LakeHouse）：是新出现的一种数据架构，它同时吸收了数据仓库和数据湖的优势，数据分析师和数据科学家可以在同一个数据存储中对数据进行操作，同时它也能为公司进行数据治理带来更多的便利性。 LakeHouse使用新的系统设计：直接在用于数据湖的低成本存储上实现与数据仓库中类似的数据结构和数据管理功能。 数据湖框架目前市面上流行的三大开源数据湖方案分别为：Delta Lake、Apache Iceberg和Apache Hudi。 Hudi基本介绍Hudi 概念 Hudi（Hadoop Upserts Deletes and Incrementals缩写）：用于管理分布式文件系统DFS上大型分析数据集存储。一言以蔽之，Hudi是一种针对分析型业务的、扫描优化的数据存储抽象，它能够使DFS数据集在分钟级的时延内支持变更，也支持下游系统对这个数据集的增量处理。 Hudi 功能 Hudi是在大数据存储上的一个数据集，可以将Change Logs通过upsert的方式合并进Hudi； Hudi对上可以暴露成一个普通Hive或Spark或Flink表，通过API或命令行可以获取到增量修改的信息，继续供下游消费； Hudi保管修改历史，可以做时间旅行或回退； Hudi内部有主键到文件级的索引，默认是记录到文件的布隆过滤器； Hudi 特性Apache Hudi使得用户能在Hadoop兼容的存储之上存储大量数据，同时它还提供两种原语，不仅可以批处理，还可以在数据湖上进行流处理。 Update&#x2F;Delete记录：Hudi使用细粒度的文件&#x2F;记录级别索引来支持Update&#x2F;Delete记录，同时还提供写操作的事务保证。查询会处理最后一个提交的快照，并基于此输出结果。 变更流：Hudi对获取数据变更提供了流的支持，可以从给定的时间点获取给定表中已updated&#x2F;inserted&#x2F;deleted的所有记录的增量流，并解锁新的查询类别。 Hudi 应用 Hudi对于Flink友好支持以后，可以使用Flink + Hudi构建实时湖仓一体架构，数据的时效性可以到分钟级，能很好的满足业务准实时数仓的需求。 通过湖仓一体、流批一体，准实时场景下做到了：数据同源、同计算引擎、同存储、同计算口径。 Hudi 发展 Apache Hudi由Uber开发并开源，该项目在2016年开始开发，并于2017年开源，2019年1月进入 Apache 孵化器，且2020年6月成为Apache顶级项目，目前最新版本：0.12.2版本。 Hudi一开始支持Spark进行数据摄入（批量Batch和流式Streaming），从0.7.0版本开始，逐渐与Flink整合，主要在于Flink SQL整合，还支持Flink SQL CDC。 Hudi 体验启动服务 Hudi是基于Hadoop的一个框架，首先要启动HDFS，Flink On Hudi，Flink，Flink SQL Client 启动HDFS集群，node1执行:1/export/server/hadoop/sbin/start-dfs.sh 单机版直接使用 start-dfs.sh 即可 Flink on Hudi(⭐️) 将以下两个jar包放入到&#x2F;export&#x2F;server&#x2F;flink&#x2F;lib下(如果有多台Flink机器都要放)。 Jar包 资源地址 hudi-flink1.14-bundle-0.12.1.jar &#x2F;export&#x2F;server&#x2F;hudi-0.12.1&#x2F;packaging&#x2F;hudi-flink-bundle&#x2F;target&#x2F; flink-sql-connector-hive-3.1.2_2.12-1.14.5.jar 启动Flink启动standalone集群服务,node1执行: 1/export/server/flink/bin/start-cluster.sh 启动Flink SQL Cli启动Flink SQL Cli命令行，node1执行: 1/export/server/flink/bin/sql-client.sh embedded shell 设置CheckPoint:1set execution.checkpointing.interval=30sec; 可以在启动FlinkSQL客户端后面加-i，表示启动之前先运行一下脚本 Flink SQL -i相关 sql-client.sh -i:指定文件启动flink-sql可以将通用的sql放在一个初始的sql文件中，启动flink的同时执行语句文件中可以写多个sql,用分号分割vim sql-client.sql 12-- 启动sql-client sql-client.sh -i sql-client.sql 插入数据 创建t1表，在Flink SQL Cli执行： 12345678910111213CREATE TABLE t1( uuid VARCHAR(20) PRIMARY KEY NOT ENFORCED, name VARCHAR(10), age INT, ts TIMESTAMP(3), `partition` VARCHAR(20))PARTITIONED BY (`partition`)WITH ( &#x27;connector&#x27; = &#x27;hudi&#x27;, -- 连接器指定hudi &#x27;path&#x27; = &#x27;hdfs://flinknode0:8020/hudi/t1&#x27;, -- 数据存储地址 &#x27;table.type&#x27; = &#x27;MERGE_ON_READ&#x27; -- 表类型，默认COPY_ON_WRITE,可选MERGE_ON_READ); 使用values插入数据，执行： 123456789INSERT INTO t1 VALUES (&#x27;id1&#x27;,&#x27;Danny&#x27;,23,TIMESTAMP &#x27;1970-01-01 00:00:01&#x27;,&#x27;par1&#x27;), (&#x27;id2&#x27;,&#x27;Stephen&#x27;,33,TIMESTAMP &#x27;1970-01-01 00:00:02&#x27;,&#x27;par1&#x27;), (&#x27;id3&#x27;,&#x27;Julian&#x27;,53,TIMESTAMP &#x27;1970-01-01 00:00:03&#x27;,&#x27;par2&#x27;), (&#x27;id4&#x27;,&#x27;Fabian&#x27;,31,TIMESTAMP &#x27;1970-01-01 00:00:04&#x27;,&#x27;par2&#x27;), (&#x27;id5&#x27;,&#x27;Sophia&#x27;,18,TIMESTAMP &#x27;1970-01-01 00:00:05&#x27;,&#x27;par3&#x27;), (&#x27;id6&#x27;,&#x27;Emma&#x27;,20,TIMESTAMP &#x27;1970-01-01 00:00:06&#x27;,&#x27;par3&#x27;), (&#x27;id7&#x27;,&#x27;Bob&#x27;,44,TIMESTAMP &#x27;1970-01-01 00:00:07&#x27;,&#x27;par4&#x27;), (&#x27;id8&#x27;,&#x27;Han&#x27;,56,TIMESTAMP &#x27;1970-01-01 00:00:08&#x27;,&#x27;par4&#x27;); 查看hdfs文件系统，hudi文件夹下生成名为t1的文件夹。地址： [http://node1:9870/explorer.html#/hudi](http://node1:9870/explorer.html \\l &#x2F;hudi ) (注意对应的域名解析为node1) 在Flink SQL Cli查看表内容： 1select * from t1; 更新数据 更新主键为id1的数据内容，执行： 12insert into t1 values (&#x27;id1&#x27;,&#x27;Danny&#x27;,27,TIMESTAMP &#x27;1970-01-01 00:00:01&#x27;,&#x27;par1&#x27;);select * from t1; 流式查询 流式查询（Streaming Query）需要设置read.streaming.enabled &#x3D; true。再设置read.start-commit，如果想消费所有数据，设置值为earliest。 使用参数如下： 参数名称 是否必填 默认值 备注 read.streaming.enabled false false 设置为true，开启stream query read.start-commit false the latest commit Instant time的格式为:’yyyyMMddHHmmss’ read.streaming_skip_compaction false false 是否不消费compaction commit，消费compaction commit可能会出现重复数据 clean.retain_commits false 10 cleaner 最多保留的历史 commits 数，大于此数量的历史 commits 会被清理掉，changelog 模式下，这个参数可以控制 changelog 的保留时间，例如 checkpoint 周期为 5 分钟一次，默认最少保留 50 分钟的时间。 注意：如果开启read.streaming.skip_compaction，但stream reader的速度比clean.retain_commits慢，就可能会造成没有读取数据就先进行了合并，合并后不再读取，从而造成数据丢失 在Flink Sql 执行 1234567891011121314151617181920212223242526272829CREATE TABLE t2( uuid VARCHAR(20) PRIMARY KEY NOT ENFORCED, name VARCHAR(10), age INT, ts TIMESTAMP(3), `partition` VARCHAR(20))PARTITIONED BY (`partition`)WITH ( &#x27;connector&#x27; = &#x27;hudi&#x27;, -- 连接器指定为hudi &#x27;path&#x27; = &#x27;hdfs://flinknode0:8020/hudi/t2&#x27;, -- 数据存储地址 &#x27;table.type&#x27; = &#x27;MERGE_ON_READ&#x27;, -- 表类型，默认COPY_ON_WRITE,可选MERGE_ON_READ &#x27;read.streaming.enabled&#x27; = &#x27;true&#x27;, -- 默认值false，设置为true，开启stream query &#x27;read.start-commit&#x27; = &#x27;20210316134557&#x27;, -- start-commit之前提交的数据不显示，默认值the latest commit，instant time的格式为：‘yyyyMMddHHmmss’ &#x27;read.streaming.check-interval&#x27; = &#x27;4&#x27; -- 检查间隔，默认60s);INSERT INTO t2 VALUES (&#x27;id1&#x27;,&#x27;Danny&#x27;,23,TIMESTAMP &#x27;1970-01-01 00:00:01&#x27;,&#x27;par1&#x27;), (&#x27;id2&#x27;,&#x27;Stephen&#x27;,33,TIMESTAMP &#x27;1970-01-01 00:00:02&#x27;,&#x27;par1&#x27;), (&#x27;id3&#x27;,&#x27;Julian&#x27;,53,TIMESTAMP &#x27;1970-01-01 00:00:03&#x27;,&#x27;par2&#x27;), (&#x27;id4&#x27;,&#x27;Fabian&#x27;,31,TIMESTAMP &#x27;1970-01-01 00:00:04&#x27;,&#x27;par2&#x27;), (&#x27;id5&#x27;,&#x27;Sophia&#x27;,18,TIMESTAMP &#x27;1970-01-01 00:00:05&#x27;,&#x27;par3&#x27;), (&#x27;id6&#x27;,&#x27;Emma&#x27;,20,TIMESTAMP &#x27;1970-01-01 00:00:06&#x27;,&#x27;par3&#x27;), (&#x27;id7&#x27;,&#x27;Bob&#x27;,44,TIMESTAMP &#x27;1970-01-01 00:00:07&#x27;,&#x27;par4&#x27;), (&#x27;id8&#x27;,&#x27;Han&#x27;,56,TIMESTAMP &#x27;1970-01-01 00:00:08&#x27;,&#x27;par4&#x27;); select * from t2; Apache Hudi 核心概念剖析 Hudi表的三个主要组件： 有序的时间轴元数据，类似于数据库事务日志； 分层布局的数据文件：实际写入表中的数据； 索引（多种实现方式）：映射包含指定记录的数据集。 时间轴(⭐️)Hudi把随着时间流逝，对表的一系列CRUD（增删改查）操作叫做Timeline，Timeline中某一次的操作，叫做Instant。Hudi的核心就是在所有的表中维护了一个包含在不同的即时时间对数据集操作（比如新增、修改或删除）的时间轴（Timeline）。 在Timeline上，每个commit被抽象为一个HoodieInstant，一个instant记录了一次提交 (commit) 的行为（action）、时间戳（time）、和状态（state）。 Instant Action: 指的是对Hudi表执行的操作类型，目前包括COMMITS、CLEANS、 DELTA_COMMIT、COMPACTION、ROLLBACK、SAVEPOINT这6种操作类型。 Commits：表示一批记录原子性的写入到一张表中。 Cleans:清除表中不再需要的旧版本文件。 Delta_commit:增量提交指的是将一批记录原子地写入MergeOnRead类型表，其中一些&#x2F;所有数据都可以写入增量日志。 Compaction：将行式文件转化为列式文件。 Rollback:Commits或者Delta_commit执行不成功时回滚数据，删除期间产生的任意文件。 Savepoint:将文件组标记为“saved”,cleans执行时不会删除对应的数据。 Instant Time：本次操作发生的时间，通常是时间戳（例如：20190117010349），它按照 动作开始时间的顺序单调递增； Instant State：表示在指定的时间点（Instant Time）对Hudi表执行操作（Instant Action）后，表所处的状态，目前包括REQUESTED（已调度但未初始化）、INFLIGHT（当前正在执行）、COMPLETED（操作执行完成）这3种状态 元数据表Hudi元数据表可以显著提高查询的读&#x2F;写性能，元数据表的主要目的是处理对“列表文件”操作的需求。以时间轴（Timeline）的形式将数据集上的各项操作元数据维护起来，以支持数据集的瞬态视图，这部分元数据存储于根目录下的元数据目录。 Commits：一个单独的commit包含对数据集上一批数据的一次写入操作的相关信息。Hudi用 单调递增的时间戳来标识commits，标定的是一次写入操作的开始。 Cleans：用于清除数据集中不再被查询所用到的旧文件的后台活动。 Compactions：用于协调Hudi内部的数据结构差异的后台活动。例如，将更新操作由基于行存的日志文件归集到列存数据上。 文件管理(⭐️)Hudi为了实现数据的CRUD（增删改查），需要能够唯一标识一条记录，Hudi将把数据集中的唯一字段(record key ) +数据所在分区(partitionPath) 联合起来当做数据的唯一键。其数据集的组织目录结构与Hive表示非常相似，一份数据集对应着一个根目录。数据集被打散为多个分区，分区字段以文件夹形式存在，该文件夹包含该分区的所有文件。在根目录下，每个分区都有唯一的分区路径，每个分区目录下有多个文件。 以HDFS存储来看，一个Hudi表的存储文件分为.hoodie文件与数据文件两类： .hoodie 文件：由于CRUD（增删改查）的零散性，每一次的操作都会生成一个文件，这些小文件越来越多后，会严重影响HDFS的性能，Hudi设计了一套文件合并机制。.hoodie文件夹中存放了对应的文件合并操作相关的日志文件。 par1、par2等相关的路径是实际的数据文件，按分区存储，par1、par2等即分区名。 .hoodie文件 Instant State操作的状态：发起(REQUESTED)，进行中(INFLIGHT)，还是已完成(COMPLETED) 数据文件 Hudi的base file (parquet文件) 在footer的meta记录了record key组成的BloomFilter，用于在file based index的实现中实现高效率的key contains检测。 Hudi的log（avro文件）是自己编码的，通过积攒数据buffer以LogBlock为单位写出，每个LogBlock 包含magic number、size、content、footer等信息，用于数据读、校验和过滤。 Index索引hudi支持以下索引选项，可以使用hoodie.index.type选择这些选项： Bloom Index（默认）：使用由记录键构建的Bloom过滤器，还可以选择使用记录键范围修改候选文件。 简单索引：将索引键从存储表中提取出来，与update&#x2F;delete 操作的新数据对应的键进行join。 HBase索引：管理外部 Apache HBase 表中的索引映射。 自带实现：可以扩展此公共API以实现自定义索引。 Bloom Index 和 简 单 索 引 都 有 全 局 选 项 ： hoodie.index.type&#x3D;GLOBAL_BLOOM hoodie.index.type&#x3D;GLOBAL_SIMPLE。HBase索引本质上是一个全局索引。 全局索引之间的区别： 全局索引：全局索引在表的所有分区中强制执行键的唯一性，即保证表中对于给定的记录键只存在一条记录。全局索引提供了更强的保证，但更新&#x2F;删除成本随着表的大小而增长，所以更适合小表。 非全局索引：仅在表的某一个分区内强制要求键保持唯一，非全局索引依靠写入器为同一个记录的 update&#x2F;delete 提供一致的分区路径，同时大幅提高了效率，更适用于大表。因为分区后索引查找操作可以更好，并且扩展更加方便。 存储类型数据计算模型 批式模型（Batch） 批式模型就是使用MapReduce、Hive、Spark等典型的批计算引擎，以小时任务或者天任务的形式来做数据计算。 流式模型（Stream） 流式模型，典型的就是使用Flink来进行实时的数据计算。 增量模型（Incremental） 针对批式和流式的优缺点，Uber提出了增量模型（Incremental Mode），相对批式来讲，更加实时；相对流式而言，更加经济。增量模型，简单来讲，是以mini batch的形式来跑准实时任务。 Hudi在增量模型中支持了两个最重要的特性： Upsert：这个主要是解决批式模型中，数据不能插入、更新的问题，有了这个特性，可以往 Hudi中写入增量数据，而不是每次进行完全的覆盖。（Hudi自身维护了key-&gt;file的映射，所以当upsert时很容易找到key对应的文件） Incremental Query：增量查询，减少计算的原始数据量。以Uber中司机和乘客的数据流Join 为例，每次抓取两条数据流中的增量数据进行批式的Join即可，相比流式数据而言，成本要降低几个数量级。 Hudi的表类型(⭐️)Hudi提供两类型表：写时复制（Copy on Write，COW）表和读时合并（Merge On Read，MOR）表。 Copy On Write Copy On Write简称COW。它是在数据写入的时候，复制一份原来的拷贝，在其基础上添加新数据仅使用列文件格式（例如parquet）存储数据。 更新update：在更新记录时，Hudi会先找到包含更新数据的文件，然后再使用更新值（最新的数据）重写该文件，包含其他记录的文件保持不变。当突然有大量写操作时会导致重写大量文件，从而导致极大的I&#x2F;O开销。 读取read：在读取数据时，通过读取最新的数据文件来获取最新的更新，此存储类型适用于少量写入和大量读取的场景。 数据副本在超过一定的个数限制后，将被删除（hoodie.cleaner.commits.retained ）参数配置，保留几个历史版本，不包含最后一个版本，默认10个）。这种类型的表，没有compact instant，因为写入时相当于已经compact了。 Merge On Read 更新Update：在更新记录时， 仅更新到增量文件（Avro）中，然后进行异步（或同步）的compaction， 最后创建列式文件（parquet）的新版本。此存储类型适合频繁写的工作负载，因为新记录是以追加的模式写入增量文件中。 读取Read：在读取数据集时，需要先将delta log增量文件与旧文件进行合并，然后生成列式文件成功后，再进行查询。 Merge On Read简称MOR，使用列式（例如parquet）+ 基于行（例如avro）的文件格式组合来存储数据。更新记录到增量文件（log文件）中，然后进行同步或异步压缩以生成列文件（parquet文件）的新版本。 hoodie.compact.inline：开启是否一个事务完成后执行压缩操作，默认不开启。 hoodie.compact.inline.max.delta.commits：设置提交多少次合并log文件到新的parquet文件，默认是5次。 hoodie.cleaner.commits.retained：保存有多少parquet文件，即控制FileSlice文件个数。 COW vs MOR 权衡 写时复制COW 读时合并MOR 写数据延迟 更高 更低 更新代价(I&#x2F;O) 更高(重写整个Parquet文件) 更低(追加到增量日志) 文件大小 更小(高更新代价(I&#x2F;O)) 更大(低更新代价) 写放大 更高 更低(取决于压缩策略) 适用场景 写少读多 写多读少 Hudi查询类型(⭐️)Hudi支持三种 不同的查询表的方 式：Snapshot Queries 、Incremental Queries 和Read Optimized Queries。 Snapshot Queries（快照查询） 查询某个增量提交操作中数据集的最新快照，先进行动态合并最新的基本文件(parquet)和增量文件(log)来提供近实时数据集（通常会存在几分钟的延迟）。即读取所有partiiton下每个FileGroup最新的FileSlice中的文件，Copy On Write表读parquet文件，Merge On Read表读parquet+log文件。 Incremental Queries（增量查询） 仅查询新写入数据集的文件，需要指定一个Commit&#x2F;Compaction的即时时间（位于Timeline上的某个Instant）作为条件，来查询此条件之后的新数据。这需要提供变更流来启用增量数据管道。 Read Optimized Queries（读优化查询） 直接查询基本文件（已存在的数据集的最新快照），其实就是列式文件（Parquet）。并保证与非Hudi列式数据集相比，具有相同的列式查询性能。 可查看给定的commit&#x2F;compact即时操作的表的最新快照。 通常读优化查询数据的最新程度取决于压缩策略。 表类型 支持的查询类型 写入时复制（Copy On Write） 快照查询（Snapshot Queries）+ 增量查询（Incremental Queries） 读取时合并（Merge On Read） 快照查询（Snapshot Queries）+ 增量查询（Incremental Queries）+ 读优化查询（Read Opitimized Queries） 写入类型在Hudi数据湖框架中支持三种方式写入数据：UPSERT（插入更新）、INSERT（插入）和BULK INSERT（批插入）。 UPSERT 插入这是默认操作。在该操作中，数据先通过index打标(INSERT&#x2F;UPDATE)，即通过查找索引，将输入记录标记为插入或更新。再运行启发式算法以确定如何最好地将这些记录放到存储上。 目前比较通用的启发式算法一般有模拟退火算法（SA）、遗传算法（GA）、蚁群算法（ACO）、人工神经网络（ANN）等。 INSERT 插入这种操作与 UPSERT 操作非常类似，只是跳过了查找索引这一步，使得它在性能上要比UPSERT 要快很多。如果只是需要 Hudi 的事务写&#x2F;增量拉取数据&#x2F;存储管理的能力，并且可以容忍重复数据，那么可以选择 INSERT 操作。 区别：跟Upsert相比，不去重，效率高。 BULK 插入bulk_insert可以减少数据序列化以及合并操作，于此同时，该数据写入方式会跳过数据去重，所以用户需要保证数据的唯一性。 bulk_insert在批量写入模式中是更加有效率的。默认情况下，批量执行模式按照分区路径对输入记录进行排序，并将这些记录写入Hudi，该方式可以避免频繁切换文件句柄导致的写性能下降。 可以通过hoodie.bulkinsert.sort.mode配置项来设置上述模式（NONE,GLOBAL_SORT , PARTITION_SORT），默认值为GLOBAL_SORT。 Insert 效率排名： NONE &gt; PARTITION_SORT &gt; GLOBAL_SORT upsert效率排名： GLOBAL_SORT&gt; PARTITION_SORT &gt; NONE Flink写入Hudibulk_insert1、用于快速导入快照数据到hudi 2、参数 参数名称 是否必须 默认值 参数说明 write.operation true upsert 设置为 bulk_insert 以开启bulk_insert功能 write.tasks false 4 bulk_insert 并行度, the number of files &gt;&#x3D; write.bucket_assign.tasks write.bulk_insert.shuffle_by_partition false true 写入前是否根据分区字段进行数据重分布。启用此选项将减少小文件的数量，但可能存在数据倾斜的风险 write.bulk_insert.sort_by_partition false true 写入前是否根据分区字段对数据进行排序。启用此选项将在写任务写多个分区时减少小文件的数量 write.sort.memory false 128 排序算子的可用托管内存。默认为 128 MB 3、示例 1234567with ( &#x27;connector&#x27; = &#x27;hudi&#x27;, &#x27;path&#x27; = &#x27;hdfs://node1:8020/test/stu_sink_hudi&#x27;, &#x27;table.type&#x27; = &#x27;MERGE_ON_READ&#x27;, &#x27;write.option&#x27; = &#x27;bulk_insert&#x27;, &#x27;write.precombine.field&#x27; = &#x27;age&#x27;); Index bootstrap1、该方式用于快照数据+增量数据的导入。如果快照数据已经通过bulk_insert导入到hudi，那么用户就可以近实时插入增量数据并且通过index bootstrap功能来确保数据不会重复。 2、参数 参数名称 是否必须 默认值 参数说明 index.bootstrap.enabled true false 当启用index bootstrap功能时，会将Hudi表中记录的索引一次性加载到Flink状态中 index.partition.regex false * 优化参数，设置正则表达式来过滤分区。 默认情况下，所有分区都被加载到Flink状态 3、使用方法 CREATE TABLE创建一条与Hudi表对应的语句。 注意这个table.type配置必须正确。 设置index.bootstrap.enabled &#x3D; true来启用index bootstrap功能 在flink-conf.yaml文件中设置Flink checkpoint的容错机制，设置配置项execution.checkpointing.tolerable-failed-checkpoints &#x3D; n（取决于Flink checkpoint执行时间） 等待直到第一个checkpoint成功，表明index bootstrap完成。 在index bootstrap完成后，用户可以退出并保存savepoint(或直接使用外部 checkpoint)。 重启任务，并且设置index.bootstrap.enable 为 false Changelog Mode1、Hudi可以保留消息的所有中间变化(I &#x2F; -U &#x2F; U &#x2F; D)，然后通过flink的状态计算消费，从而拥有一个接近实时的数据仓库ETL管道(增量计算)。 2、参数 参数名称 是否必须 默认值 参数说明 changelog.enabled false false 它在默认情况下是关闭的，为了拥有upsert语义，只有合并的消息被确保保留，中间的更改可以被合并。 设置为true以支持使用所有更改 Insert Mode1、将write.insert.deduplicate设置为false，则跳过重复数据删除 效果：每次刷新行为直接写入一个新的 parquet文件(MOR表也直接写入parquet文件)。 2、参数 参数名称 是否必须 默认值 参数说明 write.insert.deduplicate false true “插入模式”默认启用重复数据删除功能。 关闭此选项后，每次刷新行为直接写入一个新的 parquet文件 Hudi on Hive1、原理： 将Hudi表的数据映射为Hive外部表，基于该外部表，Hive可以方便的进行实时视图，读优化视图以及增量视图的查询。 2、配置添加jar包：将以下两个jar包放入到&#x2F;export&#x2F;server&#x2F;hive&#x2F;lib目录下（只放node1） Jar包 地址 hudi-hadoop-mr-bundle-0.12.1.jar &#x2F;export&#x2F;software&#x2F;hudi-0.12.1&#x2F;packaging&#x2F;hudi-hadoop-mr-bundle&#x2F;target hudi-hive-sync-bundle-0.12.1.jar &#x2F;export&#x2F;software&#x2F;hudi-0.12.1&#x2F;packaging&#x2F;hudi-hive-sync-bundle&#x2F;target ● 修改hive-site.xml配置文件，添加参数 123456789&lt;property&gt; &lt;name&gt;hive.default.aux.jars.path&lt;/name&gt; &lt;value&gt;file:///export/server/hive/lib/hudi-hadoop-mr-bundle-0.12.1.jar,file:///export/server/hive/lib/hudi-hive-sync-bundle-0.12.1.jar&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.aux.jars.path&lt;/name&gt; &lt;value&gt;file:///export/server/hive/lib/hudi-hadoop-mr-bundle-0.12.1.jar,file:///export/server/hive/lib/hudi-hive-sync-bundle-0.12.1.jar&lt;/value&gt; &lt;/property&gt; 3 案例123456789101112131415161718192021222324252627282930313233CREATE TABLE if not exists hudi_student_info ( `id` varchar(20), `name` varchar(20), `birth` varchar(20), `sex` varchar(10), `update_time` TIMESTAMP(3), PRIMARY KEY (`id`) NOT ENFORCED ) with( &#x27;connector&#x27;=&#x27;hudi&#x27;, &#x27;path&#x27;= &#x27;hdfs://192.168.88.161:8020/hudi/student_info&#x27;, -- 数据存储目录 &#x27;hoodie.datasource.write.recordkey.field&#x27;= &#x27;id&#x27;, -- 主键 &#x27;write.precombine.field&#x27;= &#x27;update_time&#x27;, -- 自动precombine的字段 &#x27;write.tasks&#x27;= &#x27;1&#x27;, &#x27;compaction.tasks&#x27;= &#x27;1&#x27;, &#x27;write.rate.limit&#x27;= &#x27;2000&#x27;, -- 限速 &#x27;table.type&#x27;= &#x27;MERGE_ON_READ&#x27;, -- 默认COPY_ON_WRITE,可选MERGE_ON_READ &#x27;compaction.async.enabled&#x27;= &#x27;true&#x27;, -- 是否开启异步压缩 &#x27;compaction.trigger.strategy&#x27;= &#x27;num_commits&#x27;, -- 按次数压缩 &#x27;compaction.delta_commits&#x27;= &#x27;1&#x27;, -- 默认为5 &#x27;changelog.enabled&#x27;= &#x27;true&#x27;, -- 开启changelog变更 &#x27;read.tasks&#x27; = &#x27;1&#x27;, &#x27;read.streaming.enabled&#x27;= &#x27;true&#x27;, -- 开启流读 &#x27;read.start-commit&#x27;=&#x27;earliest&#x27;, -- 开始读取的位点 &#x27;read.streaming.check-interval&#x27;= &#x27;3&#x27;, -- 检查间隔，默认60s &#x27;hive_sync.enable&#x27;= &#x27;true&#x27;, -- 开启自动同步hive &#x27;hive_sync.mode&#x27;= &#x27;hms&#x27;, -- 自动同步hive模式，默认jdbc模式 &#x27;hive_sync.metastore.uris&#x27;= &#x27;thrift://192.168.88.161:9083&#x27;, -- hive metastore地址 &#x27;hive_sync.table&#x27;= &#x27;hive_student_info&#x27;, -- hive 新建表名 &#x27;hive_sync.db&#x27;= &#x27;flink_demo&#x27;, -- hive 新建数据库名 &#x27;hive_sync.username&#x27;= &#x27;&#x27;, -- HMS 用户名 &#x27;hive_sync.password&#x27;= &#x27;&#x27;, -- HMS 密码 &#x27;hive_sync.support_timestamp&#x27;= &#x27;true&#x27;-- 兼容hive timestamp类型 ); 4 Hudi元数据字段含义1234567`_hoodie_commit_time` string, 提交时间`_hoodie_commit_seqno` string, 提交的序列号`_hoodie_record_key` string, 数据的主键`_hoodie_partition_path` string, 分区名（如果是分区表的时候）`_hoodie_file_name` string, 文件名`_hoodie_operation` string, 操作名注意： _hoodie_record_key + _hoodie_partition_path 共同构成唯一键 5 ro表和rt表 rt表支持快照查询和增量查询，查询rt表将会查询表基本列数据和增量日志数据的合并视图，立马可以查询到修改后的数据。 ro表则只查询表中基本列数据并不会去查询增量日志里的数据。 select * from t1;","categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"}],"author":"Johnson Liam"},{"title":"【FLink教育】FlinkCDC介绍&集成Hive","slug":"2023.09.15","date":"2023-09-14T16:00:00.000Z","updated":"2023-09-16T09:48:12.000Z","comments":true,"path":"2023/09/15/2023.09.15/","link":"","permalink":"http://example.com/2023/09/15/2023.09.15/","excerpt":"","text":"之前已经聊过了目前市面上常用的一些架构及技术选型。 传统数据入仓 - 离线方向 MySQL→Sqoop→HDFS→Hive 传统数据入仓架构 1.0，主要使用 DataX 或 Sqoop 全量同步到 HDFS，再围绕 Hive 做数仓。 此方案存在诸多缺陷：容易影响业务稳定性，因为每天都需要从业务表里查询数据；天级别的产出导致时效性差，延迟高；如果将调度间隔调成几分钟一次，则会对源库造成非常大的压力；扩展性差，业务规模扩大后极易出现性能瓶颈。 传统数仓2.0 - 增加实时方向(Canal、dataX实时采集增量数据到Kafka上再Sink到HDFS上，最后增量全量做合并，最终还是围绕Hive) 分为实时和离线两条链路，实时链路做增量同步，比如通过 Canal 同步到 Kafka 后再做实时回流； 全量同步一般只做一次，与每天的增量在 HDFS 上做定时合并，最后导入到 Hive 数仓里。 此方式只做一次全量同步，因此基本不影响业务稳定性，但是增量同步有定时回流，一般只能保持在小时和天级别，因此它的时效性也比较低。同时，全量与增量两条链路是割裂的，意味着链路多，需要维护的组件也多，系统的可维护性会比较差。 传统数仓集成方案3.0 MySQL&#x2F;PostgreSQL→Canal&#x2F;Debezium→Kafka→Flink&#x2F;Spark→Ck&#x2F;Hudi&#x2F;Doris 通过 Debezium、Canal 等工具采集 CDC 数据后，写入消息队列，再使用计算引擎做计算清洗，最终传输到下游存储，完成实时数仓、数据湖的构建 传统 CDC ETL 分析里引入了很多组件比如 Debezium、Canal，都需要部署和维护， Kafka 消息队列集群也需要维护。Debezium 的缺陷在于它虽然支持全量加增量，但它的单并发模型无法很好地应对海量数据场景。而 Canal 只能读增量，需要 DataX 与 Sqoop 配合才能读取全量，相当于需要两条链路，需要维护的组件也增加。因此，传统 CDC ETL 分析的痛点是单并发性能差，全量增量割裂，依赖的组件较多。 CDC方案比较 CDC 方案选择传统CDC 方案 传统 CDC ETL架构。通过 Debezium、Canal 等工具采集 CDC 数据后，写入消息队列，再使用计算引擎做计算清洗，最终传输到下游存储，完成实时数仓、数据湖的构建。 官方一直在思考是否可以使用 Flink CDC 去替换上图中虚线框内的采集组件和消息队列，从而简化分析链路，降低维护成本。同时更少的组件也意味着数据时效性能够进一步提高。答案是可以的，于是就有了基于 Flink CDC 的 ETL 分析流程。 FlinkCDC 方案 FlinkCDC 是一个纯SQL的方案，使用CDC实时读取业务数据库中的binlog日志，实现增量数据捕获。但是需要主要FlinkCDC并不能捕获到表的Schema信息发生变化的情况。表的Schema变化后，相对应的SQL也需要变更。 与此同时，用户也可以利用 Flink SQL 提供的丰富语法进行数据清洗、分析和聚合。此外，利用 Flink SQL 双流 JOIN、维表 JOIN、UDTF 语法可以非常容易地完成数据打宽，以及各种业务逻辑加工。 FlinkCDC 介绍FlinkCDC概述 Flink社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、PostgreSQL 等数据库直接读取全量数据和增量变更数据的 source 组件。目前也已开源，开源地址：https://github.com/ververica/flink-cdc-connectors Flink CDC 基于数据库日志的 Change Data Caputre 技术，实现了全量和增量的一体化读取能力，并借助 Flink 优秀的管道能力和丰富的上下游生态，支持捕获多种数据库的变更，并将这些变更实时同步到下游存储。 FlinkCDC原理※ Flink SQL CDC 内置了 Debezium 引擎，利用其抽取日志获取变更的能力，将changelog转换为 Flink SQL 认识的RowData数据。 主要是通过Debezium采集BinLog日志，得到类似于Json格式的字符串，JSON字符串里面有before，after，Source，op跟FlinkCDC维护的元数据信息RowKind包含的enum对象的属性刚好对应。 Flink CDC 技术的核心是支持将表中的全量数据和增量数据做实时一致性的同步与加工，让用户可以方便地获每张表的实时一致性快照。 123456789101112131415161718192021222324252627282930313233mysql&gt; show binlog events in &#x27;mysql-bin.000001&#x27;;+------------------+------+-------------+-----------+-------------+-----------------------------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+------+-------------+-----------+-------------+-----------------------------------------------------------+| mysql-bin.000001 | 4 | Format_desc | 195 | 106 | Server ver: 5.1.73-log, Binlog ver: 4 || mysql-bin.000001 | 106 | Query | 195 | 198 | use `hadoop`; delete from user where id=3 || mysql-bin.000001 | 198 | Intvar | 195 | 226 | INSERT_ID=4 || mysql-bin.000001 | 226 | Query | 195 | 332 | use `hadoop`; INSERT INTO user (id,name)VALUES (NULL,1) || mysql-bin.000001 | 332 | Query | 195 | 424 | use `hadoop`; delete from user where id=3 || mysql-bin.000001 | 424 | Intvar | 195 | 452 | INSERT_ID=5 || mysql-bin.000001 | 452 | Query | 195 | 560 | use `hadoop`; INSERT INTO user (id,name)VALUES (NULL,222) || mysql-bin.000001 | 560 | Query | 195 | 660 | use `hadoop`; DELETE FROM `user` WHERE (`id`=&#x27;1&#x27;) || mysql-bin.000001 | 660 | Intvar | 195 | 688 | INSERT_ID=6 || mysql-bin.000001 | 688 | Query | 195 | 795 | use `hadoop`; INSERT INTO `user` (`name`) VALUES (&#x27;555&#x27;) || mysql-bin.000001 | 795 | Intvar | 195 | 823 | INSERT_ID=7 || mysql-bin.000001 | 823 | Query | 195 | 930 | use `hadoop`; INSERT INTO `user` (`name`) VALUES (&#x27;555&#x27;) || mysql-bin.000001 | 930 | Intvar | 195 | 958 | INSERT_ID=8 || mysql-bin.000001 | 958 | Query | 195 | 1065 | use `hadoop`; INSERT INTO `user` (`name`) VALUES (&#x27;555&#x27;) || mysql-bin.000001 | 1065 | Intvar | 195 | 1093 | INSERT_ID=9 || mysql-bin.000001 | 1093 | Query | 195 | 1200 | use `hadoop`; INSERT INTO `user` (`name`) VALUES (&#x27;555&#x27;) || mysql-bin.000001 | 1200 | Query | 195 | 1300 | use `hadoop`; DELETE FROM `user` WHERE (`id`=&#x27;9&#x27;) || mysql-bin.000001 | 1300 | Query | 195 | 1400 | use `hadoop`; DELETE FROM `user` WHERE (`id`=&#x27;8&#x27;) || mysql-bin.000001 | 1400 | Query | 195 | 1500 | use `hadoop`; DELETE FROM `user` WHERE (`id`=&#x27;7&#x27;) || mysql-bin.000001 | 1500 | Query | 195 | 1600 | use `hadoop`; DELETE FROM `user` WHERE (`id`=&#x27;4&#x27;) || mysql-bin.000001 | 1600 | Query | 195 | 1700 | use `hadoop`; DELETE FROM `user` WHERE (`id`=&#x27;5&#x27;) || mysql-bin.000001 | 1700 | Query | 195 | 1800 | use `hadoop`; DELETE FROM `user` WHERE (`id`=&#x27;6&#x27;) || mysql-bin.000001 | 1800 | Intvar | 195 | 1828 | INSERT_ID=10 || mysql-bin.000001 | 1828 | Query | 195 | 1935 | use `hadoop`; INSERT INTO `user` (`name`) VALUES (&#x27;555&#x27;) || mysql-bin.000001 | 1935 | Intvar | 195 | 1963 | INSERT_ID=11 || mysql-bin.000001 | 1963 | Query | 195 | 2070 | use `hadoop`; INSERT INTO `user` (`name`) VALUES (&#x27;666&#x27;) || mysql-bin.000001 | 2070 | Intvar | 195 | 2098 | INSERT_ID=12 || mysql-bin.000001 | 2098 | Query | 195 | 2205 | use `hadoop`; INSERT INTO `user` (`name`) VALUES (&#x27;777&#x27;) |+------------------+------+-------------+-----------+-------------+-----------------------------------------------------------+ 官方解释 在Flink中RowData 代表了一行的数据，在 RowData 上面会有一个元数据的信息 RowKind，RowKind里面包括了插入(+I)、更新前(-U)、更新后(+U)、删除(-D)，这样和数据库里面的 binlog 概念十分类似。 通过 Debezium 采集的数据，也有一个类似的元数据 op 字段， op 字段的取值也有四种，分别是 c、u、d、r，各自对应 create、update、delete、read。对于代表更新操作的 u，其数据部分同时包含了前镜像 (before) 和后镜像 (after)。 FlinkCDC特性 支持数据库级别的快照，读取全量数据，2.0版本可以支持不加锁的方式读取 支持 binlog，捕获增量数据 支持Exactly-Once 支持 Flink DataStream API 支持 Flink Table&#x2F;SQL API，可使用 SQL DDL 来创建 CDC Source 表，并对表中的数据进行查询。 FlinkCDC部署及练习开始前准备了解与 Flink 版本的对应关系 不同的Flink版本支持的FlinkCDC版本也不一样，最好是2.0以后的版本，2.2都行。 Flink CDC 版本 Flink 版本 1.0.0 1.11.* 1.1.0 1.11.* 1.2.0 1.12.* 1.3.0 1.12.* 1.4.0 1.13.* 2.0.* 1.13.* 2.1.* 1.13.* 2.2.* 1.13.* , 1.14.* 2.3.* 1.13.*, 1.14.*, 1.15.*, 1.16.0 编译源码 一般来说，源码编译是不需要的，用户可以直接在 Flink CDC 官网下载官方编译好的二进制包或者在 pom.xml 文件中添加相关依赖即可。 以下几种情况需要进行源码编译： ➢ 用户对 Flink CDC 源码进行了修改 ➢ Flink CDC 某依赖项的版本与运行环境不一致 ➢ 官方未提供最新版本 Flink CDC 二进制安装包 比如，官方最新的 Flink CDC 二进制安装包是2.2版本的，而源代码已经到2.3版本了，如果想要使用2.3版本的 Flink CDC， 那么就需要自行编译了。 下面将介绍 Flink CDC 2.2 版本的编译。 下载源码在Linux上是有yum安装Git，非常简单，只需要一行命令 1yum -y install git 输入 git –version查看Git是否安装完成以及查看其版本号 1git --version 下载flink cdc源码 1git clone https://gitee.com/zoomake/flink-cdc-connectors-master.git 修改 pom.xml在 pom.xml 中找到这一项：flink.version。修改 flink 版本号为： 1&lt;flink.version&gt;1.14.5&lt;/flink.version&gt; 编译12cd /root/flink-cdc-connectorsmvn clean package -DskipTests 如果 maven 下载速度慢，可以在 pom.xml 文件加入这一段 1234567891011121314&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;tbds&lt;/id&gt; &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;/repository&gt;&lt;/repositories&gt; DataStream 方式的应用Mysq准备工作在 node1 下开启 binlog 日志，操作步骤如下: 登录mysql之后使用下面的命令查看是否开启binlog,代码如下: 1show variables like &#x27;%log_bin%&#x27;; 编辑配置文件: 123456vi /etc/my.cnf在[mysqld]下面加入如下代码:server_id=1log_bin = mysql-binbinlog_format = ROWexpire_logs_days = 30 重启mysql服务 1systemctl restart mysqld 进入mysql使用1)中的命令验证结果如图: 准备测试数据,执行如下代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748Drop database if exists test;CREATE DATABASE test DEFAULT CHARACTER SET = utf8 COLLATE = utf8_general_ci;Use test;-- 建表语句：-- 建表-- 学生表CREATE TABLE `Student`( `s_id` VARCHAR(20), `s_name` VARCHAR(20) NOT NULL DEFAULT &#x27;&#x27;, `s_birth` VARCHAR(20) NOT NULL DEFAULT &#x27;&#x27;, `s_sex` VARCHAR(10) NOT NULL DEFAULT &#x27;&#x27;, PRIMARY KEY(`s_id`));-- 成绩表CREATE TABLE `Score`( `s_id` VARCHAR(20), `c_id` VARCHAR(20), `s_score` INT(3), PRIMARY KEY(`s_id`,`c_id`));-- 插入学生表测试数据insert into Student values(&#x27;01&#x27; , &#x27;赵雷&#x27; , &#x27;1990-01-01&#x27; , &#x27;男&#x27;);insert into Student values(&#x27;02&#x27; , &#x27;钱电&#x27; , &#x27;1990-12-21&#x27; , &#x27;男&#x27;);insert into Student values(&#x27;03&#x27; , &#x27;孙风&#x27; , &#x27;1990-05-20&#x27; , &#x27;男&#x27;);insert into Student values(&#x27;04&#x27; , &#x27;李云&#x27; , &#x27;1990-08-06&#x27; , &#x27;男&#x27;);insert into Student values(&#x27;05&#x27; , &#x27;周梅&#x27; , &#x27;1991-12-01&#x27; , &#x27;女&#x27;);insert into Student values(&#x27;06&#x27; , &#x27;吴兰&#x27; , &#x27;1992-03-01&#x27; , &#x27;女&#x27;);insert into Student values(&#x27;07&#x27; , &#x27;郑竹&#x27; , &#x27;1989-07-01&#x27; , &#x27;女&#x27;);insert into Student values(&#x27;08&#x27; , &#x27;王菊&#x27; , &#x27;1990-01-20&#x27; , &#x27;女&#x27;);-- 成绩表测试数据insert into Score values(&#x27;01&#x27; , &#x27;01&#x27; , 80);insert into Score values(&#x27;01&#x27; , &#x27;02&#x27; , 90);insert into Score values(&#x27;01&#x27; , &#x27;03&#x27; , 99);insert into Score values(&#x27;02&#x27; , &#x27;01&#x27; , 70);insert into Score values(&#x27;02&#x27; , &#x27;02&#x27; , 60);insert into Score values(&#x27;02&#x27; , &#x27;03&#x27; , 80);insert into Score values(&#x27;03&#x27; , &#x27;01&#x27; , 80);insert into Score values(&#x27;03&#x27; , &#x27;02&#x27; , 80);insert into Score values(&#x27;03&#x27; , &#x27;03&#x27; , 80);insert into Score values(&#x27;04&#x27; , &#x27;01&#x27; , 50);insert into Score values(&#x27;04&#x27; , &#x27;02&#x27; , 30);insert into Score values(&#x27;04&#x27; , &#x27;03&#x27; , 20);insert into Score values(&#x27;05&#x27; , &#x27;01&#x27; , 76);insert into Score values(&#x27;05&#x27; , &#x27;02&#x27; , 87);insert into Score values(&#x27;06&#x27; , &#x27;01&#x27; , 31);insert into Score values(&#x27;06&#x27; , &#x27;03&#x27; , 34);insert into Score values(&#x27;07&#x27; , &#x27;02&#x27; , 89);insert into Score values(&#x27;07&#x27; , &#x27;03&#x27; , 98); 导入依赖12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;flink.version&gt;1.14.5&lt;/flink.version&gt; &lt;scala.version&gt;2.12&lt;/scala.version&gt; &lt;hadoop.version&gt;3.1.3&lt;/hadoop.version&gt; &lt;!--provided/compile--&gt; &lt;scopeFlag&gt;compile&lt;/scopeFlag&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;scopeFlag&#125;&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;scopeFlag&#125;&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;scopeFlag&#125;&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;scopeFlag&#125;&lt;/scope&gt; &lt;/dependency&gt; &lt;!--如果保存检查点到hdfs上，需要引入此依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- flink-cdc-mysql 连接器--&gt; &lt;dependency&gt; &lt;groupId&gt;com.ververica&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-mysql-cdc&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编写代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import com.ververica.cdc.connectors.mysql.source.MySqlSource;import com.ververica.cdc.connectors.mysql.table.StartupOptions; import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema; import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.restartstrategy.RestartStrategies; import org.apache.flink.runtime.state.hashmap.HashMapStateBackend;import org.apache.flink.runtime.state.storage.FileSystemCheckpointStorage;import org.apache.flink.streaming.api.CheckpointingMode;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.CheckpointConfig;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;public class FlinkCDC &#123; public static void main(String[] args) throws Exception &#123; //1.创建执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); //2.Flink-CDC 将读取 binlog 的位置信息以状态的方式保存在 CK,如果想要做到断点续传, 需要从 Checkpoint 或者 Savepoint 启动程序 //2.1 开启 Checkpoint,每隔 5 秒钟做一次 CK env.enableCheckpointing(5000L); //2.2 指定 CK 的一致性语义 env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); //2.3 设置任务取消时保留CK env.getCheckpointConfig().setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); //2.4 指定从 CK 自动重启策略 env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 2000L)); //2.5 设置状态后端 env.setStateBackend(new HashMapStateBackend()); env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(&quot;hdfs://flinknode0:8020/flink/checkpoints/FlinkCDC&quot;)); //2.6 设置访问 HDFS 的用户名 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;); //3.创建 Flink-MySQL-CDC 的 Source //initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest binlog. //latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the binlog which means only have the changes since th connector was started. //timestamp: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified timestamp.The consumer will traverse th binlog from the beginning and ignore change events whose timestamp is smaller than th specified timestamp. //specific-offset: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified offset. MySqlSource&lt;String&gt; mySqlSource = MySqlSource.&lt;String&gt;builder() .hostname(&quot;flinknode0&quot;) .port(3306) .username(&quot;root&quot;) .password(&quot;123456&quot;) .databaseList(&quot;test&quot;) .tableList(&quot;test.Student&quot;) //可选配置项,如果不指定该参数,则会读取上一个配置下的所有表的数据，注意：指定的时候需要使用&quot;db.table&quot;的方式 .startupOptions(StartupOptions.initial()) .deserializer(new JsonDebeziumDeserializationSchema()) .build(); //4.使用 CDC Source 从 MySQL 读取数据 DataStreamSource&lt;String&gt; mysqlDS = env.fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), &quot;MySQLSource&quot;); //5.打印数据 mysqlDS.print(); //6.执行任务 env.execute(); &#125;&#125; 观察程序捕获数据的变更在mysql的test数据库对Student表的数据，分别增删改，会观察到终端打印数据的实时变动: 增删改123456-- 新增数据insert into Student values(&#x27;09&#x27; , &#x27;董冬&#x27; , &#x27;1997-04-22&#x27; , &#x27;男&#x27;);-- Student表修改刚刚增加一行数据，在终端会看到迅速更新1条数据:UPDATE test.Student t SET t.s_birth = &#x27;1987-04-22&#x27; WHERE t.s_id LIKE &#x27;09&#x27;;-- Student表删除最后一行数据,在终端会看到迅速更新1条数据:DELETE FROM test.Student WHERE s_id LIKE &#x27;09&#x27;; FlinkSQL 方式的应用向3台服务器(测试节点)的Flink的lib目标下添加jar包(参见flink的 flink-lib的jar包目录) 将涉及Flink CDC的相关jar包（flink-sql-connector-mysql-cdc-2.2.1.jar、commons-cli-1.4）放到Flink的lib目录下. 启动 Flink-Cluster 1/export/server/flink/bin/start-cluster.sh 启动HDFS 因为 flink 集群配置的 checkpoint 存储地址在 hdfs 上，所以需要启动HDFS 1start-dfs.sh 启动 FlinkSQL-Client 12cd /export/server/flink/bin/sql-client.sh 设置表格模式（table mode），在内存中实体化结果，并将结果用规则的分页表格可视化展示出来。执行如下命令启用： 1SET sql-client.execution.result-mode = tableau; 在FlinkSQL-Client,执行创建表 mysql_cdc_to_test_Student，代码： 1234567891011121314151617CREATE TABLE if not exists mysql_cdc_to_test_Student ( s_id STRING, s_name STRING, s_birth STRING, s_sex STRING, PRIMARY KEY (`s_id`) NOT ENFORCED) WITH ( &#x27;connector&#x27;= &#x27;mysql-cdc&#x27;, &#x27;hostname&#x27;= &#x27;192.168.88.161&#x27;, &#x27;port&#x27;= &#x27;3306&#x27;, &#x27;username&#x27;= &#x27;root&#x27;, &#x27;password&#x27;=&#x27;123456&#x27;, &#x27;server-time-zone&#x27;= &#x27;Asia/Shanghai&#x27;, &#x27;scan.startup.mode&#x27;=&#x27;initial&#x27;, &#x27;database-name&#x27;= &#x27;test&#x27;, &#x27;table-name&#x27;= &#x27;Student&#x27;); 查询数据，实时同步mysql的对应表的数据： 1select * from mysql_cdc_to_test_Student; 在mysql的test数据库对Student表的数据，分别增删改，会观察到FlinkSQL-Client数据的实时变动: 观察控制台捕获数据的变更123456-- Student表增加一行数据,在FlinkSQL-Client会看到迅速更新1条数据：insert into Student values(&#x27;09&#x27; , &#x27;董冬&#x27; , &#x27;1997-04-22&#x27; , &#x27;男&#x27;);-- Student表修改刚刚修改一行数据，在FlinkSQL-Client会看到迅速更新2条数据:(+U/-U)UPDATE test.Student t SET t.s_birth = &#x27;1987-04-22&#x27; WHERE t.s_id LIKE &#x27;09&#x27;;-- Student表删除最后一行数据,在FlinkSQL-Client会看到迅速更新1条数据:DELETE FROM test.Student WHERE s_id LIKE &#x27;09&#x27;; 查询学习 01 课程的学生及考试情况。 先创建mysql_cdc_to_test_Score表如下： Flink CDC MySQL Connector 可通过参数 scan.startup.mode 配置启动模式。启动模式有两种：initial 和 latest-offset initial: 在首次启动时，对数据库的表执行初始快照，快照数据读取完成后继续读取 binlog 数据。这个模式可以得到历史到现在的所有数据。initial 是默认的启动模式。 latest-offset: 首次启动时不执行快照，只读取 binlog 的最新数据。 使用场景: 如果需要读取全量的数据，包括历史数据和 binlog 数据则选用 initial 模式。 如果只需要最新的 binlog 数据，则选用 latest-offset。 12345678910111213141516CREATE TABLE if not exists mysql_cdc_to_test_Score ( `s_id` STRING, `c_id` STRING, `s_score` INT, PRIMARY KEY (`s_id`) NOT ENFORCED) WITH ( &#x27;connector&#x27;= &#x27;mysql-cdc&#x27;, &#x27;hostname&#x27;= &#x27;192.168.88.161&#x27;, &#x27;port&#x27;= &#x27;3306&#x27;, &#x27;username&#x27;= &#x27;root&#x27;, &#x27;password&#x27;=&#x27;123456&#x27;, &#x27;server-time-zone&#x27;= &#x27;Asia/Shanghai&#x27;, &#x27;scan.startup.mode&#x27;=&#x27;initial&#x27;, &#x27;database-name&#x27;= &#x27;test&#x27;, &#x27;table-name&#x27;= &#x27;Score&#x27;); 计算结果（Flink SQL）: 查询学习 01 课程的学生及考试情况。 123456-- 尝试自行修改 Mysql test 数据库中的 Score 表或 Student 表的数据，会在 FlinkSQL-Client 观察到查询结果的动态变化。SELECT t.*,s.s_name,s.s_sexFROM mysql_cdc_to_test_Score AS tINNER JOIN mysql_cdc_to_test_Student AS s ON t.s_id = s.s_idWhere t.c_id = &#x27;01&#x27;; Flink CDC MySQL Connector 常用参数参数名及含义 参数名 必填 默认值 类型 参数描述 connector 是 无 String 指定connector，这里填 mysql-cdc hostname 是 无 String MySql server 的主机名或者 IP 地址 username 是 无 String 连接 MySQL 数据库的用户名 password 是 无 String 连接 MySQL 数据库的密码 database-name 是 无 String 需要监控的数据库名,支持正则表达式 table-name 是 无 String 需要监控的表名,支持正则表达式 port 是 3306 Integer MySQL 服务的端口号 server-id 否 无 Integer 当开启scan.incremental.snapshot.enabled时，建议指定server-id;server-id 可以是单个值，如5400; 也可以提供数值范围，如5400-5408 scan.incremental.snapshot.enabled 否 true Boolean 增量快照是读取表快照的新机制；和旧的快照读相比有以下优点：1. 并行读取 2. 支持checkpoint 3. 不需要锁表；当需要并行读取时，server-id需要设置数值范围，如5400-5408 scan.incremental.snapshot.chunk.size 否 8096 Integer 当读取表的快照时，表快照捕获的表的块大小(行数) scan.snapshot.fetch.size 否 1024 Integer 每次读表时最多拉取的记录数 scan.startup.mode 否 initial String MySQL CDC 启动模式，有效值：initial 和 latest-offset connect.timeout 否 30s Duration connector 连接 MySQL 服务的最长等待超时时间 connect.max-retries 否 3 Integer connector 创建 MySQL 连接的重试次数 connection.pool.size 否 20 Integer 连接池的大小 Flink CDC 2.0 详解Flink CDC 1.X 痛点 MySQL CDC 是 Flink CDC 中使用最多也是最重要的 Connector，本文下述章节描述 Flink CDC Connector 均为 MySQL CDC Connector。 总结一下 FlinkCDC1.X痛点: 一致性通过加锁来保证 不支持水平拓展, 只支持单并发 全量阶段不支持断点续传(没有CheckPoint机制) 全量 + 增量读取的过程需要保证所有数据的一致性，因此需要通过全局锁保证，但是加锁容易对在线业务造成影响，且 DBA 一般不给锁权限。 不支持水平扩展，因为 Flink CDC 底层是基于 Debezium，其架构是单节点，所以 Flink CDC 的数据源只支持单并发。在全量阶段读取阶段，如果表非常大 (亿级别)，读取时间在小时甚至天 级别，用户无法通过增加资源去提升作业速度。 全量读取阶段不支持 checkpoint：CDC 读取分为两个阶段，全量读取和增量读取，目前全量读取阶段是不支持 checkpoint 的，因此会存在一个问题：当我们同步全量数据时，假设需要 5 个小时，当同步了 4 小时的时候作业失败，这时候就需要重新开始，再读取 5 个小时。 Debezium 锁分析 由于Flink CDC 底层封装了 Debezium, 想要深入了解FlinkCDC的痛点, 就必须知道Debezium的锁的机制. Debezium 同步一张表分为两个阶段： 全量阶段：查询当前表中所有记录； 增量阶段：从 binlog 消费变更数据。 以全局锁为例，首先是获取一个锁，然后再去开启可重复读的事务。这里加锁范围是读取binlog 的当前位点和当前表的 schema。这样做的目的是保证 binlog 的起始位置和读取到的当前schema 是可以对应上的，因为表的 schema 是会改变的，比如删除列或者增加列。在读取这两个信息后，SnapshotReader 会在可重复读事务里读取全量数据，在全量数据读取完成后，会启动binlogReader从读取的 binlog 起始位置开始增量读取，从而保证全量数据 + 增量数据的无缝衔接。 表锁是全局锁的退化版，因为全局锁的权限会比较高，因此在某些场景，用户可能没有全局锁的权限，但是有表锁的权限。不过表锁的加锁时间会更长，因为表锁有个特征：锁提前释放了可重复读的事务默认会提交，所以锁需要等到全量数据读完后才能释放。 Flink CDC 1.x 可以不加锁，能够满足大部分场景，但牺牲了一定的数据准确性。Flink CDC 1.x 默认加全局锁，虽然能保证数据一致性，但存在上述数据库无响应(hang住)故障的风险。 Flink CDC 2.X 设计目标 (以 MySQL 为例)通过上面的分析，可以知道 2.0 的设计方案核心要解决上述的三个问题，即支持无锁读取、水平扩展、checkpoint。 左边是 Chunk 的切分算法描述，Chunk 的切分算法其实和很多数据库的分库分表原理类似，通过表的主键对表中的数据进行分片。假设每个 Chunk 的步长为 10，按照这个规则进行切分，只需要把这些 Chunk 的区间做成左开右闭或者左闭右开的区间，保证衔接后的区间能够等于表的主键区间即可。 右边是每个 Chunk 的无锁读算法描述，该算法的核心思想是在划分了 Chunk 后，对于每个 Chunk 的全量读取和增量读取，在不用锁的条件下完成一致性的合并。 Flink CDC 2.X设计实现 对于大部分用户来讲，其实无需过于关注如何无锁算法和分片的细节，了解整体的流程就好。 在对于有主键的表做初始化模式，整体的流程主要分为 5 个阶段： Chunk 切分； Chunk 分配； （实现并行读取数据&amp;CheckPoint） Chunk 读取； （实现无锁读取） Chunk 汇报； Chunk 分配。 整体流程可以概括为：首先通过主键对表进行 Snapshot Chunk 划分（第一步：Chunk 切分），再将 Snapshot Chunk 分发给多个 SourceReader（第二步：Chunk 分配），每个 Snapshot Chunk 读取时通过算法实现无锁条件下的一致性读（第三步：Chunk 读取）， SourceReader 读取时支持 chunk 粒度的 checkpoint， 在 Snapshot Chunk 读取完成之后，有一个汇报的流程，即 SourceReader 需要将 Snapshot Chunk 完成信息汇报给 SourceEnumerator（第四步：Chunk 汇报），在所有 Snapshot Chunk 读取完成后， 下发一个 binlog chunk 进行增量部分的 binlog 读取（第五步：Chunk 分配），这便是 Flink CDC 2.0 的整体流程。 官方测试效果:用 TPC-DS 数据集中的 customer 表进行了测试，Flink 版本是 1.13.1，customer 表的数据量是 6500 万条，Source 并发为 8，全量读取阶段：Flink CDC 2.0 用时 13 分钟；Flink CDC 1.4 用时 89 分钟；读取性能提升 6.8 倍 使用JDBC注意事项 ● 在mysql-cdc 2.x中默认开启了scan.incremental.snapshot.enabled， 如果表没有主键，则会导致增量快照读（ incremental snapshot reading）失败，则需要将scan.incremental.snapshot.enabled设置为false ● 快照数据块分割采用的算法是：chunk reading algorithm ，块切分采用固定的步长，由参数scan.incremental.snapshot.chunk.size确定，默认值是：8096（行数） 针对自增的数字，则按主键从小到大进行切换 针对其它主键，则按SELECT MAX(STR_ID) AS chunk_high FROM (SELECT * FROM TestTable WHERE STR_ID &gt; ‘uuid-001’ limit 25) 获取切换的范围 每个chunk reader执行Offset Signal Algorithm以获得快照块的最终一致输出 ● 如果需要并行运行，每个并行Reader应该有一个唯一的服务器id，所以’ server-id ‘必须是’ 5400-6400 ‘这样的范围，并且范围必须大于并行度 (并行度通过 SET ‘parallelism.default’ &#x3D; 8; 设置）。 Flink集成HiveFlink集成Hive Flink 与 Hive 的集成主要体现在以下两个方面: 持久化元数据 Flink利用Hive的MetaStore作为持久化的Catalog，可通过Hive Catalog将不同会话中的 Flink元数据存储到Hive MetaStore 中。 读写 Hive 的表 Flink与Hive的集成，如同使用SparkSQL或者Impala操作Hive中的数据一样，我们可以使用Flink直接读写Hive中的表。 支持的Hive版本对于不同的Hive版本，可能在功能方面有所差异，这些差异取决于你使用的Hive版本，而不取决于Flink，一些版本的功能差异如下： Hive 内置函数在使用 Hive-1.2.0 及更高版本时支持。 列约束，也就是 PRIMARY KEY 和 NOT NULL，在使用 Hive-3.1.0 及更高版本时支持。 更改表的统计信息，在使用 Hive-1.2.0 及更高版本时支持。 DATE列统计信息，在使用 Hive-1.2.0 及更高版时支持。 使用 Hive-2.0.x 版本时不支持写入 ORC 表。 集成Hive的方式 要与 Hive 集成，您需要在 Flink 下的/lib/目录中添加一些额外的依赖包， 以便通过 TableAPI 或 SQL Client 与 Hive 进行交互。 或者，您可以将这些依赖项放在专用文件夹中，并分别使用 Table API 程序或 SQL Client 的-C或-l选项将它们添加到 classpath 中。 Apache Hive 是基于 Hadoop 之上构建的, 首先您需要 Hadoop 的依赖，进行如下配置： 12vim /etc/profileexport HADOOP_CLASSPATH=`hadoop classpath` 使用 Flink 提供的 Hive jar： Flink1.14.5集成Hive只需要添加如下三个jar包，以Hive3.12为例，分别为： 123flink-sql-connector-hive-3.1.2_2.12-1.14.5.0.jarflink-connector-hive_2.12-1.14.5.jar (2.12为scala版本) hive-exec-3.1.2.jar （存在于Hive安装路径下的lib文件夹） Hive Catalog Hive Catalog的主要作用是使用Hive MetaStore去管理Flink的元数据。Hive Catalog可以将元数据进行持久化，这样后续的操作就可以反复使用这些表的元数据，而不用每次使用时都要重新注册。 配置Hive Catalog为了避免每次打开客户端后，手动创建Hive Catalog,可以将配置写到文件中，每次启动时直接指定配置文件即可。 配置sql-conf.sql 创建sql-conf.sql文件，该文件是Flink SQL Cli启动时使用的配置文件，将该文件放到Flink安装目录flink&#x2F;conf&#x2F;下，具体的配置如下，主要是配置catalog： 123456CREATE CATALOG myhive WITH (&#x27;type&#x27;=&#x27;hive&#x27;,&#x27;hive-conf-dir&#x27;=&#x27;/export/server/hive/conf&#x27;,&#x27;hive-version&#x27;=&#x27;3.1.2&#x27;,&#x27;hadoop-conf-dir&#x27;=&#x27;/export/server/hadoop/etc/hadoop/&#x27; );USE CATALOG myhive; 使用Hive CatalogHive Catalog可以处理两种类型的表：一种是Hive兼容的表，另一种是普通表(generic table)。 Hive兼容表是以兼容Hive的方式来存储的，所以，对于Hive兼容表而言，既可以使用Flink去操作该表，又可以使用Hive去操作该表。 普通表是对Flink而言的，当使用Hive Catalog创建一张普通表，仅仅是使用Hive MetaStore将其元数据进行了持久化，所以可以通过Hive查看这些表的元数据信息(通过DESCRIBE FORMATTED命令)，但是不能通过Hive去处理这些表，因为语法不兼容。 Hive Dialect Flink 目前支持两种 SQL 方言: default 和 hive。 从 1.11.0 开始，在使用Hive方言时，Flink允许用户用Hive语法来编写SQL语句。通过提供与Hive 语法的兼容性，旨在改善与 Hive 的互操作性，并减少用户需要在 Flink 和 Hive 之间切换来执行不同语句的情况。 Flink读写Hive写入Hive表 Flink支持以批处理(Batch)和流处理(Streaming)的方式写入Hive表。 当以批处理的方式写入Hive表时，只有当写入作业结束时，才可以看到写入的数据。 批处理模式写入 支持append追加数据和overwrite覆盖数据 123#使用批处理模式set execution.type = batch;set execution.runtime-mode = batch; 流处理模式写入 流式写入Hive表，不支持Insert overwrite方式 12345#使用流处理模式set execution.type = streaming;set execution.runtime-mode = streaming;#开启checkpoint set execution.checkpointing.interval=30sec; 流处理写入涉及的相关参数： partition.time-extractor.timestamp-pattern 默认值：(none) 解释：分区时间抽取器，与 DDL 中的分区字段保持一致,如果是按天分区，则可以是$dt，如果是按年(year)月(month)日(day)时(hour)进行分区，则该属性值为：​$year-$month-​$day ​$hour:00:00，如果是按天时进行分区，则该属性值为：​$day $hour:00:00。 sink.partition-commit.trigger 默认值：process-time 解释：分区触发器类型，可选 process-time 或partition-time。 process-time：不需要时间提取器和水位线，当当前时间大于分区创建时间 +sink.partition-commit.delay 中定义的时间，提交分区； partition-time：需要 Source 表中定义 watermark，当 watermark &gt; 提取到的分区时间 +sink.partition-commit.delay 中定义的时间，提交分区; sink.partition-commit.delay 默认值：0S 解释：分区提交的延时时间，如果是按天分区，则该属性的值为：1d，如果是按小时分区，则该属性值为1h。默认值是0s,即一旦分区中有数据，它将立即提交。注意:该分区可能被提交多次。 sink.partition-commit.policy.kind 默认值：(none) 解释：提交分区的策略，用于通知下游的应用该分区已经完成了写入，也就是说该分区的数据可以被访问读取。可选的值如下：可以同时配置上面的两个值，比如metastore，success-file。 metastore：添加分区的元数据信息，仅Hive表支持该值配置。 success-file：在表的存储路径下添加一个_SUCCESS文件。 读取Hive表Flink支持以批处理(Batch)和流处理(Streaming)的方式读取Hive中的表。批处理的方式与Hive的本身查询类似，即只在提交查询的时刻查询一次Hive表。流处理的方式将会持续地监控Hive表，并且会增量地提取新的数据。默认情况下，Flink是以批处理的方式读取Hive表。 关于流式读取Hive表，Flink既支持分区表又支持非分区表。对于分区表而言，Flink将会监控新产生的分区数据，并以增量的方式读取这些数据。对于非分区表，Flink会监控Hive表存储路径文件夹里面的新文件，并以增量的方式读取新的数据。 Flink读取Hive表可以配置一下参数： streaming-source.enable 默认值：false 解释：是否开启流式读取 Hive 表，默认不开启。 streaming-source.partition.include 默认值：all 解释：配置读取Hive的分区，包括两种方式：all和latest。all意味着读取所有分区的数据，latest表示只读取最新的分区数据。值得注意的是，latest方式只能用于开启了流式读取Hive表，并用于维表JOIN的场景。 streaming-source.monitor-interval 默认值：None 解释：持续监控Hive表分区或者文件的时间间隔。值得注意的是，当以流的方式读取Hive表时，该参数的默认值是1m，即1分钟。当temporal join时，默认的值是60m，即1小时。另外，该参数配置不宜过短 ，最短是1 个小时，因为目前的实现是每个 task 都会查询metastore，高频的查可能会对metastore 产生过大的压力。 streaming-source.partition-order 默认值：partition-name 解释：streaming source的分区顺序。分区表默认的是partition-name，表示使用默认分区名称顺序加载最新分区，也是推荐使用的方式。除此之外还有两种方式，分别为：create-time和partition-time。其中create-time表示使用分区文件创建时间顺序。partition-time表示使用分区时间顺序。指的注意的是，对于非分区表，该参数默认值为：create-time。 partition.time-extractor.kind 默认值：default 分区时间提取器类型。用于从分区中提取时间，支持default和自定义。如果使用default，则需要通过参数partition.time-extractor.timestamp-pattern配置时间戳提取的正则表达式。对于自定义，应该配置提取器类 streaming-source.consume-start-offset 默认值：None 解释：流式读取Hive表的起始偏移量。 Hive维表JoinFlink支持的是processing-time的temporal join，Flink既支持非分区表的temporal join，又支持分区表的temporal join。对于分区表而言，Flink会监听Hive表的最新分区数据。值得注意的是，Flink尚不支持 event-time temporal join。 Temporal Join最新分区 如果Hive分区表的每个分区都包含全量的数据，那么每个分区将做为一个时态表的版本数据，即将最新的分区数据作为一个全量维表数据。该功能仅支持Flink的streaming模式。 1234#注意：使用 Hive 最新分区作为Tempmoral table之前，需要设置必要的参数&#x27;streaming-source.enable&#x27; = &#x27;true&#x27;,&#x27;streaming-source.partition.include&#x27; = &#x27;latest&#x27;#还需要设置streaming-source.monitor-interval的值，即数据更新的时间间隔。 Temporal Join最新表 对于Hive的非分区表或者全量的分区表，当使用temporal join时，整个Hive表会被缓存到Slot内存中，然后根据流中的数据对应的key与其进行匹配。 对于有界表：streaming-source.enable = false lookup.join.cache.ttl 默认值：60min 解释：表示缓存时间。由于 Hive 维表会把维表所有数据缓存在 TM 的内存中，当维表数据量很大时，很容易造成 OOM。当然TTL的时间也不能太短，因为会频繁地加载数据，从而影响性能。当使用此种方式时，Hive表必须是有界表，即非Streaming Source的时态表，换句话说，该表的属性streaming-source.enable &#x3D; false。 对于无界表：streaming-source.enable = true 需要设置streaming-source.monitor-interval的值，即数据更新的时间间隔。 对于分区表，还需要设置streaming-source.partition.include为 all。","categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"}],"author":"Johnson Liam"},{"title":"【FLink教育】Flink技术选型","slug":"2023.09.14","date":"2023-09-13T16:00:00.000Z","updated":"2023-09-15T13:01:19.000Z","comments":true,"path":"2023/09/14/2023.09.14/","link":"","permalink":"http://example.com/2023/09/14/2023.09.14/","excerpt":"","text":"传统方案传统数据集成方案的痛点 大数据技术的应用可以从海量的用户行为数据中进行挖掘分析，根据分析结果优化平台的服务质量，最终满足用户的需求。大数据分析平台就是将大数据技术应用于教育培训领域，为企业经营提供数据支撑： 建立集团数据仓库，统一集团数据中心，把分散的业务数据进行预先处理和存储。 根据业务分析需要，从海量的用户行为数据中进行挖掘分析，定制多维的数据集合，形成数据集市，供各个场景主题使用。 前端业务数据展示选择和控制，选取合适的前端数据统计、分析结果展示工具。 - 上图为传统数据入仓架构 1.0，主要使用 DataX 或 Sqoop 全量同步到 HDFS，再围绕 Hive 做数仓。 此方案存在诸多缺陷：容易影响业务稳定性，因为每天都需要从业务表里查询数据；天级别的产出导致时效性差，延迟高；如果将调度间隔调成几分钟一次，则会对源库造成非常大的压力；扩展性差，业务规模扩大后极易出现性能瓶颈。 上图为传统数据入仓 2.0 架构。分为实时和离线两条链路，实时链路做增量同步，比如通过 Canal 同步到 Kafka 后再做实时回流；全量同步一般只做一次，与每天的增量在 HDFS 上做定时合并，最后导入到 Hive 数仓里。 此方式只做一次全量同步，因此基本不影响业务稳定性，但是增量同步有定时回流，一般只能保持在小时和天级别，因此它的时效性也比较低。同时，全量与增量两条链路是割裂的，意味着链路多，需要维护的组件也多，系统的可维护性会比较差。 上图为传统 CDC ETL 分析架构。通过 Debezium、Canal 等工具采集 CDC 数据后，写入消息队列，再使用计算引擎做计算清洗，最终传输到下游存储，完成实时数仓、数据湖的构建。 传统 CDC ETL 分析里引入了很多组件比如 Debezium、Canal，都需要部署和维护， Kafka 消息队列集群也需要维护。Debezium 的缺陷在于它虽然支持全量加增量，但它的单并发模型无法很好地应对海量数据场景。而 Canal 只能读增量，需要 DataX 与 Sqoop 配合才能读取全量，相当于需要两条链路，需要维护的组件也增加。因此，传统 CDC ETL 分析的痛点是单并发性能差，全量增量割裂，依赖的组件较多。 优享学教育大数据平台1.0版本技术架构简要分析 架构 : Debezium 1.8 + Pulsar 2.7.0 + Clickhouse 21.9.4 数据处理流程分析： 数据源 Mysql的数据使用Debezium工具单机同步至Pulsar。 七陌及诸葛智能的数据采用Http的方式同步至Pulsar。 数据处理 中间无处理，数据最终在Clickhouse中进行处理。 数据存储 Clickhouse直接消费Pulsar的数据，写入到Clickhouse中，Clickhouse存储原始数据。 现有架构存在的问题主要如下： 对于Mysql数据源采用的是Debezium工具，该工具仅能单机部署，只能单并发读取Binlog日志，且全量同步不支持断点续传，传输能力较差，每次服务器重启会对Mysql数据进行全量同步，相当耗时，且生态扩展性不好。 Pulsar作为消息传输系统不建议持久化存储数据，因此数据的持久化存储到了Clickhouse中，因此导致Clickhouse存储的是大量明细数据，会导致Clickhouse变得臃肿。 Pulsar与Clickhouse之间缺少数据的处理过程，如果遇到复杂的业务逻辑，比如数据的拉宽、清洗、转换，该架构是无法胜任的。 Clickhouse单表查询性能强劲，但join性能和并发相对较差，而1.0架构的查询Join操作也是在Clickhouse中进行。Clickhouse运维成本高，扩展性较差，各个节点一致性要求高，语法非标准SQL增加使用成本。 缺乏数仓分层的概念和支撑，如果后期随着业务的发展，指标的计算可能会比较复杂，因此引入数仓分层是非常有必要的。 优享学教育大数据平台2.0版本逻辑架构 初步设想是将MySQL数据通过Debezium进入到Kafka。但是存储在Kafka的数据有失效时间，不会存太久的历史数据，这样就必须把维度表持久化到HBase中。并且该架构单并发性能差，依赖组件多，链路长，维护工作量庞大，对系统资源的消耗极大。 我们整体的目标规划是替代kafka，把Hudi作为中间存储，将数仓建设在Hudi之上，并以Flink 作为流批一体计算引擎。 最终，我们决定直接通过Flink CDC千表入湖，替换Debezium、kafka组件采集数据，采用Flink sql实时处理，在Hudi中做数仓分层，替代在kafka中做分层处理。并且在该项目中，我们采用Dinky将多表的source进行了合并，实现千表入湖，source的合并也可以避免多个任务通过Flink CDC接MySQL表以及Binlog，对MySQL库的性能造成影响。 最终我们采用的架构2.0如下图： 架构 : Mysql 5.7 + Flink CDC 2.2 + Hadoop 3.3.0 + Hive 3.1.2 + Flink 1.14.5 + Hudi 0.11.1 + Doris 1.1.0 + Dinky 0.6.6 + Presto 0.261 + Hue 4.1.0 数据源 Mysql业务数据库 数据采集 使用Flink CDC 2.2作为同步工具,将Mysql数据多并发实时采集传输至存储端 数据存储 使用Hudi存储数据，使用Doris存储DWD层与DWS数据做查询分析 数据计算 实时计算:使用Flink&#x2F;Flink SQL进行实时数据处理 使用Persto灵活用于离线数据分析 数据分析 使用Doris灵活用于自定义数据分析 大数据平台应用 实时场景:实时报表,业务监控,动态展示大屏 这样做的好处有： Kafka不再担任实时数据仓库存储的中间存储介质，而Hudi存储在HDFS上，可以存储海量数据集； 实时数据仓库中间层可以使用 OLAP 分析引擎查询中间结果数据； 真正意义上的准实时，数据 T+1 延迟的问题得到解决； 读时 Schema 不再需要严格定义 Schema 类型，支持 schema evolution； 支持主键索引，数据查询效率数倍增加，并且支持 ACID 语义，保证数据不重复不丢失； Hudi 具有 Timeline 的功能，可以更多存储数据中间的状态数据，数据完备性更强。 优享学教育大数据平台2.0版本数据流转数据流转如下图 业务数据主要放在业务数据库Mysql两个数据库bxg库和crm库中。 Flink CDC通过读取Mysql数据库的Binlog日志(Mysql的Binlog日志完整记录了数据库中的变更，可以把Binlog文件当作流的数据源)，实时捕获数据变更,将原始数据通过Dinky千表入湖,同步到Hudi的ODS层，并使用Hudi on Hive 会将ODS层元数据信息自动同步到Hive中，便于管理。 使用Flink SQL将Hudi的ODS层数据拉宽写入到Hudi的DWD层 ，DWD层通过join和轻度聚合进入DWS层，DWS层可直接用于BI报表。 使用Flink SQL将Hudi的DWD和DWS层数据同步到Doris的DWD和DWS层。 最终动态显示大屏连接到Doris的DWS层指标数据，将指标动态显示到大屏上。 未来展望由于在Hive中查询较慢，故在当前的架构中，我们在Doris中也保存了一份数据便于业务查询。但是这导致Doris和Hudi没有很好的融合，数据冗余存储，所以目前相当于还是两套架构。 在后续Doris即将发布的1.2版本支持数据湖联邦查询，即可以通过外表的方式联邦分析位于Hudi中的数据，在避免数据拷贝的前提下，查询性能大幅提升。我们可以升级为如下架构，Metabase和Hue直接接入Doris进行可视化展示。可以很好的解决了我们现有架构的一些问题。 项目技术选型流式处理平台采用Flink CDC作为同步工具 为什么选用Flink CDC 2.2? 2.0架构选用了Flink CDC 2.2作为同步工具，相比较 Debezium，Flink CDC底层封装了Debezium，Flink CDC 2.2解决了Debezium的痛点(详见后续CDC讲解): 并发读取，全量数据的读取性能可以水平扩展。 全程无锁，不对线上业务产生锁的风险。 断点续传，支持全量阶段的 Checkpoint。 对比全量+增量同步的能力: Debezium仅支持单机部署，Flink CDC 2.2支持分布式架构，多并发能力大大提升作业速度。支持断点续传，无需再为每次服务器重启的全量同步耽误大量时间担忧。 在数据转换 &#x2F; 数据清洗能力上: 在Flink CDC上操作相当简单，可以通过Flink SQL去操作数据，Debezium等则需要通过脚本或者模板去做，所以用户的使用门槛会比较高。 在生态方面，对下游的一些数据库或者数据源的支持: Flink CDC下游有丰富的Connector，也支持各种自定义Connector。其架构分布式架构不单纯体现在数据读取能力的水平扩展上，更重要的是在大数据场景下分布式系统接入能力。例如Flink CDC的数据入湖或者入仓的时候，下游通常是分布式的系统，如Hive、HDFS、Iceberg、Hudi等，那么从对接入分布式系统能力上看，Flink CDC的架构能够很好地接入此类系统。 配合Dinky实现千表入湖和千表入仓 在Flink CDC进行多表入Hudi和Doris，使用了Dinky作为实时计算平台，可实现千表入仓入湖。Dinky定义了CDCSOURCE整库同步的语法，该语法和CDAS作用相似，可以直接自动构建一个整库入仓入湖的实时任务，并且对Source进行了合并，不会产生额外的Mysql及网络压力，支持对任意Sink的同步，如Kafka、Doris、Hudi、Jdbc 。 分布式计算平台分布式计算采用Flink SQL 为什么采用Flink SQL? Flink 具备统一的框架处理有界和无界两种数据流的能力 部署灵活，Flink 底层支持多种资源调度器，包括Yarn、Kubernetes 等。Flink 自身带的Standalone的调度器，在部署上也十分灵活。 极高的可伸缩性，可伸缩性对于分布式系统十分重要，阿里巴巴双11大屏采用Flink 处理海量数据，使用过程中测得Flink 峰值可达17亿条&#x2F;秒。 极致的流式处理性能。Flink 相对于Storm 最大的特点是将状态语义完全抽象到框架中，支持本地状态读取，避免了大量网络IO，可以极大提升状态存取的性能。 Flink 是目前开源社区中唯一一套集高吞吐、低延迟、高性能于一身的分布式流式数据处理框架。 基于轻量级分布式快照(Snapshot&#x2F;Checkpoints)的容错机制 支持SavePoint保存点，Flink 通过SavePoints 技术将任务执行的快照保存在存储介质上，当任务重启的时候，可以从事先保存的 SavePoints 恢复原有的计算状态，使得任务继续按照停机之前的状态运行。 Flink SQL开发大大降低了开发难度,易于开发和维护 海量数据存储使用Hudi 存储原始数据,使用Doris存储实时宽表和聚合数据。 为什么选用Hudi? 数据湖是一个集中式数据存储库，用来存储大量的原始数据，使用平面架构来存储数据。湖仓一体（LakeHouse）是新出现的一种数据架构，它同时吸收了数据仓库和数据湖的优势。直接在用于数据湖的低成本存储上实现与数据仓库中类似的数据结构和数据管理功能。 目前市面上流行的三大开源数据湖方案分别为：Delta Lake、Apache Iceberg和Apache Hudi。与其它两种方案相比，Hudi（Hadoop Update Delete Incremantal）最大的特点是： Update&#x2F;Delete记录，Hudi使用细粒度的文件&#x2F;记录级别索引来支持Update&#x2F;Delete记录，同时还提供写操作的事务保证。查询会处理最后一个提交的快照，并基于此输出结果。 变更流，Hudi对获取数据变更提供了一流的支持：可以从给定的时间点获取给定表中已Updated&#x2F;Inserted&#x2F;Deleted的所有记录的增量流，并解锁新的查询姿势（类别）。 为什么选用Doris 相比较于Clickhouse，Doris优势: SQL的标准兼容好，使用成本低，是一个强一致性元数据的系统，导数功能较为完备。 大宽表和多表查询速度都很快，具备CBO（基于代价）优化器，可以很好应对复杂查询，支持高并发查询。 弹性伸缩能力要好，支持在线扩缩容，运维成本低。 相较于Doris，Clickhouse则需要做较多工作： ZooKeeper存在性能瓶颈导致集群规模不能特别大。 基本无法做到弹性伸缩，纯手工扩缩容工作量巨大且容易出错。 故障节点的容忍度较低，出现一个节点故障会引发某些操作失败。 导数需要外部保证数据不重不丢，导数失败需要删了重导。 元数据需要自己保证各个节点一致性，偶发性的不一致情况较多。 分布式表和本地表有两套表结构，较多用户难以理解。 多表Join SQL需要改写和优化，方言较多几乎是不兼容其他引擎的SQL。 总体上说，新框架2.0，解决了1.0框架的痛点问题，且实时性更佳,更好的满足业务需求。 在该架构方案的基础上可以继续实现用户画像、智能推荐、数据挖掘等等，非常容易扩展且不需要修改整体架构，因此具有高度的灵活和可扩展性。 框架软件版本 软件 版本 Mysql 5.7 Java 1.8.0_241 Hadoop 3.3.0 Zookeeper 3.4.6 Hive 3.1.2 Flink 1.14.5 Hudi 0.11.1 Doris 1.1.0 Dinky 0.6.6 Flink CDC 2.2.0 Presto 0.261 Hue 4.1.0 非功能描述框架版本选型 Apache：运维麻烦，组件间兼容性需要自己调研。（一般大厂使用，技术实力雄厚，有专业的运维人员） CDH：国内使用最多的版本，但CM不开源，但其实对中、小公司使用来说没有影响（建议使用） HDP：开源，可以进行二次开发，但是没有CDH稳定，国内使用较少 资源类型 资源大小 描述 CPU 32核 CPU是计算机的大脑，是衡量服务器性能的首要指标 内存 128G Spark计算非常的耗内存，因此使用128G内存的服务器，如果有钱可以提供更大的内存服务器 硬盘 10T 数据存储的地方，因此数据量越大，硬盘的容量必然更高 技术亮点 使用Flink CDC实现业务数据库Mysql的实时采集,支持多并发读取数据源, 大幅提高同步读取的速度。 使用Flink CDC全量读取阶段全程无锁并支持checkpoint,避免锁库风险，同时支持数据断点续传。针对大数据量的全量同步，避免失败造成全量重新同步。 使用 Flink CDC 去替换传统采集组件和消息队列，简化分析链路，降低维护成本,同时更少的组件也意味着数据时效性能够进一步提高。 Flink CDC社区22年4月总结用户用了 Flink CDC 后，遇到的第一个痛点就是需要将 MySQL 的 DDL 手工映射成 Flink 的 DDL。手工映射表结构是比较繁琐的，尤其是当表和字段数非常多的时候,手工映射也容易出错。本项目中解决了该问题,实现了自动帮助用户自动去映射表结构。 本项目解决了Flink CDC整库入湖的挑战。用户主要使用SQL，这就需要为每个表的数据同步链路定义一个 INSERT INTO 语句。假如用户的 MySQL 实例中甚至有上千张的业务表，用户就要写上千个 INSERT INTO 语句。每一个INSERT INTO 任务都会创建至少一个数据库连接，读取一次 Binlog 数据。千表入湖的话就需要上千个连接，上千次的 Binlog 重复读取。这就会对 MySQL 和网络造成很大的压力。本项目通过使用Dinky与Flink CDC结合解决了整库同步,千表入湖、千表入仓对Mysql造成压力的问题。 基于Hudi数据湖,使用Hudi on Hive实现湖仓一体，流批一体，基于Hudi on Hive可实现湖上建仓，实现离线数仓开发。 使用Flink SQL对数据进行ETL,大大降低了实时数仓的开发难度。 使用Doris作为存储平台，同时 Doris拥有强大的查询性能","categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"},{"name":"Interview","slug":"Interview","permalink":"http://example.com/tags/Interview/"}],"author":"Johnson Liam"},{"title":"ClickHouse全面解析","slug":"2023.09.13","date":"2023-09-12T16:00:00.000Z","updated":"2023-09-14T13:58:04.000Z","comments":true,"path":"2023/09/13/2023.09.13/","link":"","permalink":"http://example.com/2023/09/13/2023.09.13/","excerpt":"","text":"https://www.mubu.com/doc/Ud_30MJRFa","categories":[],"tags":[],"author":"Johnson Liam"},{"title":"【Flink】FlinkSQL| 状态编程| 自定义函数","slug":"2023.09.12","date":"2023-09-11T16:00:00.000Z","updated":"2023-09-14T03:00:53.000Z","comments":true,"path":"2023/09/12/2023.09.12/","link":"","permalink":"http://example.com/2023/09/12/2023.09.12/","excerpt":"","text":"Flink 中的状态编程 在 Flink 中，算子任务可以分为无状态和有状态两种情况。 在传统的事务型处理架构中，这种额外的状态数据是保存在数据库中的。而对于实时流处理来说，这样做需要频繁读写外部数据库，如果数据规模非常大肯定就达不到性能要求了。所以 Flink 的解决方案是，将状态直接保存在内存中来保证性能，并通过分布式扩展来提高吞吐量。 有状态算子的一般处理流程： 算子任务接收到上游发来的数据； 获取当前状态； 根据业务逻辑进行计算，更新状态； 得到计算结果，输出发送到下游任务。 状态分类按照由 Flink 管理还是用户自行管理，状态可以分为原始状态 ( Raw State ) 和托管状态 (Managed State)。 原始状态：即用户自定义的 State。Flink 在做快照的时候，把整个 State 当做一个整体，需要开发者自己管理，使用 byte 数组来读写状态内容。 托管状态：是由 Flink 框架管理的 State，如 ValueState、ListState 等，其序列化和反序列化由 Flink 框架提供支持，无需用户感知、干预。通常在 DataStream 上的状态，推荐使用托管状态，一般情况下，在实现自定义算子时，才会使用到原始状态。 算子状态（Operator State）：Operator State可以用在所有算子上，每个算子子任务或者说每个算子实例共享一个状态，流入这个算子子任务的数据可以访问和更新这个状态。 按键分区状态（Keyed State）：Keyed State是KeyedStream上的状态。假如输入流按照id为Key进行了keyBy分组，形成一个KeyedStream，数据流中所有id为1的数据共享一个状态，可以访问和更新这个状态，以此类推，每个Key对应一个自己的状态。 无论是Keyed State还是Operator State，Flink的状态都是基于本地的，即每个算子子任务维护着这个算子子任务对应的状态存储，算子子任务之间的状态不能相互访问。 分区状态State主要有三种实现，分别为ValueState、MapState和AppendingState，AppendingState又可以细分为ListState、ReducingState和AggregatingState。 分区状态划分 值状态（ValueState）：状态中只保存一个“值”（value）。 123456789101112131415// ValueState&lt;T&gt;本身是一个接口，源码中定义如下：// 这里的 T 是泛型，表示状态的数据内容可以是任何具体的数据类型。如果想要保存一个长整型值作为状态，那么类型就是 ValueState&lt;Long&gt;。// 可以在代码中读写值状态，实现对于状态的访问和更新。// T value()：获取当前状态的值；// update(T value)：对状态进行更新，传入的参数 value 就是要覆写的状态值。public interface ValueState&lt;T&gt; extends State &#123; T value() throws IOException; void update(T value) throws IOException;&#125;// 在具体使用时，为了让运行时上下文清楚到底是哪个状态，还需要创建一个“状态描述器”（StateDescriptor）来提供状态的基本信息。例如源码中，ValueState 的状态描述器构造方法如下：public ValueStateDescriptor(String name, Class&lt;T&gt; typeClass) &#123; super(name, typeClass, null);&#125; 列表状态（ListState）：将需要保存的数据，以列表（List）的形式组织起来。在 ListState接口中同样有一个类型参数T，表示列表中数据的类型。 1234567// ListState 提供了一系列的方法来操作状态，使用方式与一般的List 非常相似。// Iterable&lt;T&gt; get()：获取当前的列表状态，返回的是一个可迭代类型 Iterable；// update(List&lt;T&gt; values)：传入一个列表values，直接对状态进行覆盖；// add(T value)：在状态列表中添加一个元素 value；// addAll(List&lt;T&gt; values)：向列表中添加多个元素，以列表 values 形式传入。// 类似地，ListState 的状态描述器就叫作 ListStateDescriptor，用法跟 ValueStateDescriptor完全一致。 映射状态（MapState）：把一些键值对（key-value）作为状态整体保存起来，可以认为就是一组 key-value 映射的列表。对应的 MapState&lt;UK, UV&gt;接口中，就会有 UK、UV 两个泛型，分别表示保存的 key 和 value 的类型。 1234567891011// MapState 提供了操作映射状态的方法，与 Map 的使用非常类似。// UV get(UK key)：传入一个 key 作为参数，查询对应的 value 值；// put(UK key, UV value)：传入一个键值对，更新 key 对应的 value 值；// putAll(Map&lt;UK, UV&gt; map)：将传入的映射 map 中所有的键值对，全部添加到映射状态中；// remove(UK key)：将指定 key 对应的键值对删除；// boolean contains(UK key)：判断是否存在指定的 key，返回一个 boolean 值。// 另外，MapState 也提供了获取整个映射相关信息的方法：// Iterable&lt;Map.Entry&lt;UK, UV&gt;&gt; entries()：获取映射状态中所有的键值对；// Iterable&lt;UK&gt; keys()：获取映射状态中所有的键（key），返回一个可迭代 Iterable 类型；// Iterable&lt;UV&gt; values()：获取映射状态中所有的值（value），返回一个可迭代 Iterable类型；// boolean isEmpty()：判断映射是否为空，返回一个 boolean 值。 归约状态（ReducingState）：类似于值状态（Value），不过需要对添加进来的所有数据进行归约，将归约聚合之后的值作为状态保存下来。ReducintState这个接口调用的方法类似于 ListState，只不过它保存的只是一个聚合值，所以调用.add()方法时，不是在状态列表里添加元素，而是直接把新数据和之前的状态进行归约，并用得到的结果更新状态。 12// 归约逻辑的定义，是在归约状态描述器（ReducingStateDescriptor）中，通过传入一个归约函数（ReduceFunction）来实现的。这里的归约函数，就是之前介绍 reduce 聚合算子时讲到的 ReduceFunction，所以状态类型跟输入的数据类型是一样的。public ReducingStateDescriptor(String name, ReduceFunction&lt;T&gt; reduceFunction, Class&lt;T&gt; typeClass) &#123;...&#125; 聚合状态（AggregatingState）：与归约状态非常类似，聚合状态也是一个值，用来保存添加进来的所有数据的聚合结果。与 ReducingState 不同的是，它的聚合逻辑是由在描述器中传入一个更加一般化的聚合函数（AggregateFunction）来定义的；里面通过一个累加器（Accumulator）来表示状态，所以聚合的状态类型可以跟添加进来的数据类型完全不同，使用更加灵活。 1// AggregatingState 接口调用方法也与ReducingState 相同，调用.add()方法添加元素时，会直接使用指定的AggregateFunction 进行聚合并更新状态。 状态生存时间在某些场景下 Flink 用户状态一直在无限增长，一些用例需要能够自动清理旧的状态。例如，作业中定义了超长的时间窗口，或者在动态表上应用了无限范围的 GROUP BY 语句。此外，目前开发人员需要自己完成 TTL 的临时实现，例如使用可能不节省存储空间的计时器服务。 123456789// 要使用 State TTL 功能，首先要定义一个 StateTtlConfig 对象。StateTtlConfig 对象可以通过构造器模式来创建，典型地用法是传入一个 Time 对象作为 TTL 时间，然后可以设置时间处理语义(TtlTimeCharacteristic)、更新类型(UpdateType)以及状态可见性(StateVisibility)。当创建完 StateTtlConfig 对象，可以在状态描述符中启用 State TTL 功能。StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(10)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build();ValueStateDescriptor&lt;String&gt; stateDescriptor = new ValueStateDescriptor&lt;&gt;(&quot;my state&quot;, String.class);stateDescriptor.enableTimeToLive(ttlConfig); 时间处理语义 12345678// TtlTimeCharacteristic 表示 State TTL 功能可以使用的时间处理语义：public enum TtlTimeCharacteristic &#123; ProcessingTime&#125;// 截止到目前当前版本，只支持 ProcessingTime 时间处理语义。// 可以通过如下方法显示设置：setTtlTimeCharacteristic(StateTtlConfig.TtlTimeCharacteristic.ProcessingTime) 更新类型 12345678910111213// UpdateType 表示状态时间戳(上次访问时间戳)的更新时机：public enum UpdateType &#123; Disabled, OnCreateAndWrite, OnReadAndWrite&#125;// 如果设置为 Disabled，则表示禁用 TTL 功能，状态不会过期；// 如果设置为 OnCreateAndWrite，那么表示在状态创建或者每次写入时都会更新时间戳；// 如果设置为 OnReadAndWrite，那么除了在状态创建和每次写入时更新时间戳外，读取状态也会更新状态的时间戳。// 如果不配置默认为 OnCreateAndWrite。// 可以通过如下方法显示设置：setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) 状态可见性 1234567891011// StateVisibility 表示状态可见性，在读取状态时是否返回过期值：public enum StateVisibility &#123; ReturnExpiredIfNotCleanedUp, NeverReturnExpired&#125;// 如果设置为 ReturnExpiredIfNotCleanedUp，那么当状态值已经过期，但还未被真正清理掉，就会返回给调用方；// 如果设置为 NeverReturnExpired，那么一旦状态值过期了，就永远不会返回给调用方，只会返回空状态。// 可以通过如下方法显示设置：setStateVisibility(StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp)setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) 过期清理策略 全量快照清理策略 12345678// 全量快照清理策略，这种策略可以在生成全量快照(Snapshot/Checkpoint)时清理过期状态，这样可以大大减小快照存储，但需要注意的是本地状态中过期数据并不会被清理。唯有当作业重启并从上一个快照恢复后，本地状态才会实际减小。如果要在 DataStream 中使用该过期请策略，请参考如下所示代码：import org.apache.flink.api.common.state.StateTtlConfig;import org.apache.flink.api.common.time.Time;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupFullSnapshot() .build(); 这种过期清理策略对开启了增量检查点的 RocksDB 状态后端无效。 增量清理策略 1234567// 对 Heap StateBackend 的增量清理策略。这种策略下存储后端会为所有状态条目维护一个惰性全局迭代器。每次触发增量清理时，迭代器都会向前迭代删除已遍历的过期数据。如果要在 DataStream 中使用该过期请策略，请参考如下所示代码：import org.apache.flink.api.common.state.StateTtlConfig;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupIncrementally(5, false) .build();// 该策略有两个参数：第一个参数表示每次触发清理时需要检查的状态条目数，总是在状态访问时触发。第二个参数定义了在每次处理记录时是否额外触发清理。堆状态后端的默认后台清理每次触发检查 5 个条目，处理记录时不会额外进行过期数据清理。 如果该状态没有被访问或者没有记录需要处理，那么过期状态会一直存在。 增量清理所花费的时间会增加记录处理延迟。 目前仅堆状态后端实现了增量清理。为 RocksDB 状态后端设置增量清理不会有任何效果。 如果堆状态后端与同步快照一起使用，全局迭代器在迭代时保留所有 Key 的副本，因为它的特定实现不支持并发修改。启用此功能将增加内存消耗。异步快照没有这个问题。 对于现有作业，可以随时在 StateTtlConfig 中启用或者停用此清理策略。 RocksDB 压缩清理策略 12345678// 如果使用 RocksDB StateBackend，则会调用 Flink 指定的压缩过滤器进行后台清理。RocksDB 周期性运行异步压缩来合并状态更新并减少存储。Flink 压缩过滤器使用 TTL 检查状态条目的过期时间戳并删除过期状态值。如果要在 DataStream 中使用该过期请策略，请参考如下所示代码：import org.apache.flink.api.common.state.StateTtlConfig;StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupInRocksdbCompactFilter(1000) .build();// RocksDB 压缩过滤器在每次处理一定状态条目后，查询当前的时间戳并检查是否过期。频繁地更新时间戳可以提高清理速度，但同样也会降低压缩性能。RocksDB 状态后端的默认每处理 1000 个条目就查询当前时间戳。 算子状态算子状态（Operator State）就是一个算子并行实例上定义的状态，作用范围被限定为当前算子任务，与Key无关，不同Key的数据只要分发到同一个并行子任务，就会访问到同一个算子状态。 使用场景算子状态一般用在 Source 或 Sink 等与外部系统连接的算子上，或者完全没有 key 定义的场景。比如 Flink 的 Kafka 连接器中，就用到了算子状态。在给 Source 算子设置并行度后，Kafka 消费者的每一个并行实例，都会为对应的主题（topic）分区维护一个偏移量， 作为算子状态保存起来。这在保证 Flink 应用“精确一次”（exactly-once）状态一致性时非常有用。 状态类型算子状态也支持不同的结构类型，主要有三种：ListState、UnionListState 和 BroadcastState。 列表状态（ListState）：与 Keyed State 中的 ListState 一样，将状态表示为一组数据的列表。与 Keyed State 中的列表状态的区别是：在算子状态的上下文中，不会按键（key）分别处理状态，所以每一个并行子任务上只会保留一个“列表”（list），也就是当前并行子任务上所有状态项的集合。列表中的状态项就是可以重新分配的最细粒度，彼此之间完全独立。 当算子并行度进行缩放调整时，算子的列表状态中的所有元素项会被统一收集起来，相当于把多个分区的列表合并成了一个“大列表”，然后再均匀地分配给所有并行任务。这种“均匀分配”的具体方法就是“轮询”（round-robin），与之前介绍的 rebanlance 数据传输方式类似，是通过逐一“发牌”的方式将状态项平均分配的。这种方式也叫作“平均分割重组”（even-splitredistribution）。 算子状态中不会存在“键组”（key group）这样的结构，所以为了方便重组分配，就把它直接定义成了“列表”（list）。这也就解释了，为什么算子状态中没有最简单的值状态（ValueState）。 总结：ListState的快照存储数据，在系统重启后，list数据的重分配模式为： round-robin； 轮询平均分配 联合列表状态（UnionListState）：与 ListState 类似，联合列表状态也会将状态表示为一个列表。它与常规列表状态的区别在于：算子并行度进行缩放调整时对于状态的分配方式不同。 UnionListState 的重点就在于“联合”（union）。在并行度调整时，常规列表状态是轮询分配状态项，而联合列表状态的算子则会直接广播状态的完整列表。这样，并行度缩放之后的并行子任务就获取到了联合后完整的“大列表”，可以自行选择要使用的状态项和要丢弃的状态项。这种分配也叫作“联合重组”（union redistribution）。如果列表中状态项数量太多，为资源和效率考虑一般不建议使用联合重组的方式。 总结：unionListState的快照存储数据，在系统重启后，list数据的重分配模式为： 广播模式； 在每个subtask上都拥有一份完整的数据 广播状态（BroadcastState）：有时希望算子并行子任务都保持同一份“全局”状态，用来做统一的配置和规则设定。这时所有分区的所有数据都会访问到同一个状态，状态就像被“广播”到所有分区一样，这种特殊的算子状态，就叫作广播状态（BroadcastState）。 因为广播状态在每个并行子任务上的实例都一样，所以在并行度调整的时候就比较简单， 只要复制一份到新的并行任务就可以实现扩展；而对于并行度缩小的情况，可以将多余的并行子任务连同状态直接砍掉——因为状态都是复制出来的，并不会丢失。 在底层，广播状态是以类似映射结构（map）的键值对（key-value）来保存的，必须基于一个“广播流”（BroadcastStream）来创建。 Flink SQL常用语法 主要分为DDL和DML语句 DDLDDL: Create 子句 目前 Flink SQL 支持下列 CREATE 语句 ● CREATE TABLE ● CREATE DATABASE ● CREATE VIEW ● CREATE FUNCTION 建表语句DMLDML: With 子句DML: WHERE 子句DML: DISTINCT 子句DML: 窗口聚合DML: Group 聚合DML: Joins 语法Flink 也支持了非常多的数据 Join 方式，主要包括以下三种： 动态表（流）与动态表（流）的 Join 动态表（流）与外部维表（比如 Redis）的 Join 动态表字段的列转行（一种特殊的 Join） 细分 Flink SQL 支持的 Join： Regular Join：流与流的 Join，包括 Inner Equal Join、Outer Equal Join 实时 Regular Join 可以不是 等值 join。等值 join 和 非等值 join 区别在于，等值 join 数据 shuffle 策略是 Hash，会按照 Join on 中的等值条件作为 id 发往对应的下游；非等值 join 数据 shuffle 策略是 Global，所有数据发往一个并发，按照非等值条件进行关联 Join 的流程是左流新来一条数据之后，会和右流中符合条件的所有数据做 Join，然后输出。 流的上游是无限的数据，所以要做到关联的话，Flink 会将两条流的所有数据都存储在 State 中，所以 Flink 任务的 State 会无限增大，因此你需要为 State 配置合适的 TTL，以防止 State 过大 Interval Join：流与流的 Join，两条流一段时间区间内的 Join 实时 Interval Join 可以不是 等值 join。等值 join 和 非等值 join 区别在于，等值 join 数据 shuffle 策略是 Hash，会按照 Join on 中的等值条件作为 id 发往对应的下游；非等值 join 数据 shuffle 策略是 Global，所有数据发往一个并发，然后将满足条件的数据进行关联输出 当左Join时如果左流中有数据没有被Join到，当这些数据之后（时间语义上的之后）的数据被Join到后，这些数据就过期了输出NUll Temporal Join：流与流的 Join，包括事件时间，处理时间的 Temporal Join，类似于离线中的快照 Join Lookup Join：流与外部维表的 Join Array Expansion：表字段的列转行，类似于 Hive 的 explode 数据炸开的列转行 Table Function：自定义函数的表字段的列转行，支持 Inner Join 和 Left Outer Join DML: 集合操作DML: TopNFlink SQL 自定义函数UDF函数归类Flink 中的函数有两个维度的归类标准。 一个归类标准是：系统（内置）函数和 Catalog 函数。系统函数没有命名空间，只能通过其名称来进行引用。Catalog 函数属于 Catalog 和数据库，因此它们拥有 Catalog 和数据库的命名空间。用户可以通过全&#x2F;部分限定名（catalog.db.func 或 db.func）或者函数来对 Catalog 函数进行引用。 另一个归类标准是：临时函数和持久化函数。临时函数由用户创建，它仅在会话的生命周期（也就是一个 Flink 任务的一次运行生命周期内）内有效。持久化函数不是由系统提供的，是存储在 Catalog 中，它在不同会话的生命周期内都有效。 这两个维度归类标准组合下，Flink SQL 总共提供了 4 种函数： 临时性系统内置函数 系统内置函数 临时性 Catalog 函数（例如：Create Temporary Function） Catalog 函数（例如：Create Function） 请注意，在用户使用函数时，系统函数始终优先于 Catalog 函数解析，临时函数始终优先于持久化函数解析。 自定义函数当前 Flink 提供了一下几种 UDF 能力： 标量函数（Scalar functions 或 UDAF）：输入一条输出一条，将标量值转换成一个新标量值，对标 Hive 中的 UDF； 表值函数（Table functions 或 UDTF）：输入一条条输出多条，对标 Hive 中的 UDTF； 聚合函数（Aggregate functions 或 UDAF）：输入多条输出一条，对标 Hive 中的 UDAF； 表值聚合函数（Table aggregate functions 或 UDTAF）：仅仅支持 Table API，不支持 SQL API，其可以将多行转为多行； 异步表值函数（Async table functions）：这是一种特殊的 UDF，支持异步查询外部数据系统，用在前文介绍到的 lookup join 中作为查询外部系统的函数。 标量函数 继承ScalarFunction类，实现eval()方法 自定义标量函数1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.itheima.flink.udf;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.Table;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;import org.apache.flink.table.functions.ScalarFunction;public class MyScalarFunc &#123; public static void main(String[] args) throws Exception &#123; // 1、构建动态表运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // 2、注册UDF tEnv.createTemporaryFunction(&quot;mySum&quot;, new MyFunc()); // 3、调用UDF处理数据 Table result = tEnv.sqlQuery(&quot;select mySum(1, 2, 3, 45)&quot;); // 4、输出结果并执行 tEnv.toDataStream(result).print(); env.execute(); &#125; /** * 自定义标量函数进行累加操作 */ public static class MyFunc extends ScalarFunction &#123; public Integer eval(Integer... a) &#123; Integer sum = 0; for (Integer i : a) &#123; sum += i; &#125; return sum; &#125; &#125;&#125; 表值函数 TableFuntion 可以有0个、一个、多个输入参数，他的返回值可以是任意行，每行可以有多列数据。 实现自定义TableFunction需要继承TableFunction类，实现eval方法。 TableFunction是一个泛型类，需要指定返回值类型 不同于标量函数，eval方法没有返回值，使用collect方法来收集对象。 自定义表值函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.DataTypes;import org.apache.flink.table.api.Schema;import org.apache.flink.table.api.Table;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;import org.apache.flink.table.functions.TableFunction;import org.apache.flink.types.Row;import static org.apache.flink.api.common.typeinfo.Types.*;public class MyTableFunc &#123; public static void main(String[] args) throws Exception &#123; // 1、构建动态表运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // 2、注册函数 tEnv.createTemporaryFunction(&quot;mySplit&quot;, new MyFunc(&quot;u&quot;)); // 3、调用UDF处理数据 // 构建dataStream final SingleOutputStreamOperator&lt;Row&gt; ordersDataStream = env.fromElements( Row.of(2, &quot;Euro&quot;), Row.of(1, &quot;US Dollar&quot;), Row.of(50, &quot;Yen&quot;), Row.of(3, &quot;Euro&quot;)).returns(ROW_NAMED(new String[]&#123;&quot;amount&quot;, &quot;currency&quot;&#125;, INT, STRING)); // 通过已有的dataStream构建动态表 Table ordersTable = tEnv.fromDataStream(ordersDataStream, Schema.newBuilder() .column(&quot;amount&quot;, DataTypes.INT()) .column(&quot;currency&quot;, DataTypes.STRING()) .columnByExpression(&quot;proctime&quot;, &quot;PROCTIME()&quot;) .build()); tEnv.createTemporaryView(&quot;ordersTable&quot;, ordersTable); Table result = tEnv.sqlQuery(&quot;SELECT o.currency, T.word, T.length FROM ordersTable as o, LATERAL TABLE(mySplit(currency)) as T(word, length)&quot;); // 4、输出结果 tEnv.toDataStream(result, Row.class).print(&quot;分词查询&quot;); env.execute(); &#125; /** * 自定义表值函数对字符串进行切分并返回切分后的单词与单词长度 */ public static class MyFunc extends TableFunction&lt;Tuple2&lt;String, Integer&gt;&gt; &#123; private String sep = &quot;,&quot;; public MyFunc(String sep) &#123; this.sep = sep; &#125; public void eval(String str) &#123; for (String s : str.split(sep)) &#123; collect(Tuple2.of(s, s.length())); &#125; &#125; &#125;&#125; 聚合函数Flink 的AggregateFunction是一个基于中间计算结果状态进行增量计算的函数。由于是迭代计算方式，所以，在窗口处理过程中，不用缓存整个窗口的数据，所以效率执行比较高。 该函数会将给定的聚合函数应用于每个窗口和键。 对每个元素调用聚合函数，以递增方式聚合值，并将每个键和窗口的状态保持在一个累加器中。 AggregateFunction需要重写的方法有： createAccumulator：创建一个新的累加器，开始一个新的聚合。累加器是正在运行的聚合的状态。 add：将给定的输入添加到给定的累加器，并返回新的累加器值。 getResult：从累加器获取聚合结果。 merge：合并两个累加器，返回合并后的累加器的状态。 自定义聚合函数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.itheima.flink.udf;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;public class MyAggregateFunc &#123; public static void main(String[] args) throws Exception &#123; // 1、构建动态表运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // 2、构建测试数据 DataStreamSource&lt;Tuple3&lt;String, String, Long&gt;&gt; tuple3 = env.fromElements(ENGLISH); // 3、处理数据 SingleOutputStreamOperator&lt;Double&gt; aggregate = tuple3.keyBy(t -&gt; t.f0) .countWindow(2) .aggregate(new MyAggFunc()); aggregate.print(); env.execute(); &#125; public static final Tuple3[] ENGLISH = new Tuple3[] &#123; Tuple3.of(&quot;class1&quot;, &quot;张三&quot;, 100L), Tuple3.of(&quot;class1&quot;, &quot;李四&quot;, 40L), Tuple3.of(&quot;class1&quot;, &quot;王五&quot;, 60L), Tuple3.of(&quot;class2&quot;, &quot;赵六&quot;, 20L), Tuple3.of(&quot;class2&quot;, &quot;小七&quot;, 30L), Tuple3.of(&quot;class2&quot;, &quot;小八&quot;, 50L), &#125;; /** * 自定义聚合函数求每个班级的平均分 */ public static class MyAggFunc implements AggregateFunction&lt;Tuple3&lt;String, String, Long&gt;, Tuple2&lt;Long, Long&gt;, Double&gt; &#123; /** * 初始化累加器 * @return 最初的累加器 */ @Override public Tuple2&lt;Long, Long&gt; createAccumulator() &#123; return Tuple2.of(0L, 0L); &#125; /** * 将新输入的数据与累加器进行计算 * @param value The value to add * @param accumulator The accumulator to add the value to * @return 新的累加器 */ @Override public Tuple2&lt;Long, Long&gt; add(Tuple3&lt;String, String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator) &#123; return Tuple2.of(accumulator.f0 + 1, accumulator.f1 + value.f2); &#125; /** * 用总成绩除以总人数得到平均分 * @param accumulator The accumulator of the aggregation * @return 平均分 */ @Override public Double getResult(Tuple2&lt;Long, Long&gt; accumulator) &#123; return (double) (accumulator.f1) / accumulator.f0; &#125; /** * 合并累加器 * @param a An accumulator to merge * @param b Another accumulator to merge * @return 最终累加器 */ @Override public Tuple2&lt;Long, Long&gt; merge(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b) &#123; return Tuple2.of(a.f0 + b.f0, a.f1 + b.f1); &#125; &#125;&#125; 表值聚合函数表聚合，多对多，多行输入多行输出，用户定义的表聚合函数（User-Defined Table Aggregate Functions，UDTAF），可以把一个表中数据，聚合为具有多行和多列的结果表 用户定义表聚合函数，是通过继承 TableAggregateFunction 抽象类来实现的，TableAggregateFunction 要求必须实现的方法： createAccumulator() accumulate() emitValue() merge() 表值聚合函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package com.itheima.flink.udf;import lombok.Data;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.DataTypes;import org.apache.flink.table.api.Schema;import org.apache.flink.table.api.Table;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;import org.apache.flink.table.functions.TableAggregateFunction;import org.apache.flink.types.Row;import org.apache.flink.util.Collector;import java.io.Serializable;import java.util.Arrays;import static org.apache.flink.api.common.typeinfo.Types.*;import static org.apache.flink.table.api.Expressions.$;import static org.apache.flink.table.api.Expressions.call;public class MyAggTableFunc &#123; public static void main(String[] args) throws Exception &#123; // 1、构建动态表运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //2.构建TableEnv StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); final SingleOutputStreamOperator&lt;Row&gt; ordersDataStream = env.fromElements( Row.of(1, &quot;Latte&quot;, 6), Row.of(1,&quot;Milk&quot;,3), Row.of(1,&quot;Breve&quot;,5), Row.of(1,&quot;Mocha&quot;,8), Row.of(1,&quot;Tea&quot;,4)).returns(ROW_NAMED(new String[]&#123;&quot;id&quot;, &quot;name&quot;, &quot;price&quot;&#125;, INT, STRING, INT)); Table table = tableEnv.fromDataStream(ordersDataStream, Schema.newBuilder() .column(&quot;id&quot;, DataTypes.INT()) .column(&quot;name&quot;, DataTypes.STRING()) .column(&quot;price&quot;, DataTypes.INT()) .build()); // 3、注册UDF// tEnv.createTemporaryFunction(&quot;top2&quot;, new MyFunc());//// Arrays.stream(tEnv.listFunctions()).forEach(System.out::println);//// // 4、调用UDF处理数据// Table result = table.groupBy($(&quot;id&quot;))// .flatAggregate(call(&quot;top2&quot;, $(&quot;price&quot;)).as(&quot;v&quot;, &quot;rank&quot;))// .select($(&quot;id&quot;), $(&quot;v&quot;), $(&quot;rank&quot;));//// // 5、输出结果// tEnv.toChangelogStream(result).print();//// env.execute(); tableEnv.createTemporaryFunction(&quot;top2&quot;, new MyFunc()); Table result = table.groupBy($(&quot;id&quot;)) .flatAggregate(call(&quot;top2&quot;, $(&quot;price&quot;)).as(&quot;v&quot;, &quot;rank&quot;)) .select($(&quot;id&quot;), $(&quot;v&quot;), $(&quot;rank&quot;)); tableEnv.toChangelogStream(result).print(&quot;result&quot;); env.execute(&quot;FlinkSqlUDFTableAggregateFunction&quot;); &#125; @Data public static class Top2Accum implements Serializable &#123; public Integer first; public Integer second; &#125; public static class MyFunc extends TableAggregateFunction&lt;Tuple2&lt;Integer, Integer&gt;, Top2Accum&gt; &#123; /** * @return 初始化累加器 */ @Override public Top2Accum createAccumulator() &#123; Top2Accum top2Accum = new Top2Accum(); top2Accum.setFirst(Integer.MIN_VALUE); top2Accum.setSecond(Integer.MIN_VALUE); return top2Accum; &#125; /** * 比较新值与中间结果更新前两名 * @param acc * @param v */ public void accumulate(Top2Accum acc, Integer v) &#123; if (v &gt; acc.getFirst()) &#123; acc.setSecond(acc.getFirst()); acc.setFirst(v); &#125; else if (v &gt; acc.getSecond()) &#123; acc.setSecond(v); &#125; &#125; /** * 将所有中间结果进行遍历合并计算 * @param acc * @param iterable */ public void merge(Top2Accum acc, java.lang.Iterable&lt;Top2Accum&gt; iterable) &#123; for (Top2Accum acc2 : iterable) &#123; accumulate(acc, acc2.getFirst()); accumulate(acc, acc2.getSecond()); &#125; &#125; public void emitValue(Top2Accum acc, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; out) &#123; if (acc.getFirst() &gt; Integer.MIN_VALUE) &#123; out.collect(Tuple2.of(acc.getFirst(), 1)); &#125; if (acc.getSecond() &gt; Integer.MIN_VALUE) &#123; out.collect(Tuple2.of(acc.getSecond(), 2)); &#125; &#125; &#125;&#125;","categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"author":"Johnson Liam"},{"title":"【Flink】Flink水印机制与快照机制","slug":"2023.09.11","date":"2023-09-10T16:00:00.000Z","updated":"2023-09-15T13:01:43.000Z","comments":true,"path":"2023/09/11/2023.09.11/","link":"","permalink":"http://example.com/2023/09/11/2023.09.11/","excerpt":"","text":"Flink 中的水印操作问题引入流处理中的乱序问题当 flink 以 EventTime 模式处理流数据时，它会根据数据里的时间戳来处理基于时间的算子。 但是由于网络、分布式等原因，会导致数据乱序的情况。 watermark解决乱序问题不以事件时间作为触发计算的条件，而是根据Watermark判断是否触发。 当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark&#x3D;EventTime)的计算结果如下： Watermark&#x3D;EventTime Watermark &#x3D; EventTime -5s如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark &#x3D; EventTime -5s， 如下： 基于SQL的水印实现场景： 使用Socket模拟接收数据 设置WaterMark，设置的逻辑：在第一条数据进来时，设置WaterMark为0，指定第一条数据的时间戳后，获取该时间戳与当前 WaterMark的最大值，并将最大值设置为下一条数据的WaterMark，以此类推 1234567891011121314151617181920-- 创建映射表CREATE TABLE MyTable (item STRING,ts TIMESTAMP(3), -- TIMESTAMP 类型的时间戳WATERMARK FOR ts AS ts - INTERVAL &#x27;0&#x27; SECOND) WITH (&#x27;connector&#x27; = &#x27;socket&#x27;,&#x27;hostname&#x27; = &#x27;node1&#x27;,&#x27;port&#x27; = &#x27;9999&#x27;,&#x27;format&#x27; = &#x27;csv&#x27;);-- 设置滚动窗口进行聚合计算SELECTTUMBLE_START(ts, INTERVAL &#x27;5&#x27; SECOND) AS window_start,TUMBLE_END(ts, INTERVAL &#x27;5&#x27; SECOND) AS window_end,TUMBLE_ROWTIME(ts, INTERVAL &#x27;5&#x27; SECOND) as window_rowtime,item,count(item) as total_itemFROM MyTableGROUP BY TUMBLE(ts, INTERVAL &#x27;5&#x27; SECOND), item; 数据有序的场景测试数据： 123456hello,2022-03-25 16:39:45hello,2022-03-25 16:39:46hello,2022-03-25 16:39:47hello,2022-03-25 16:39:48hello,2022-03-25 16:39:49hello,2022-03-25 16:39:50 数据无序的场景测试数据： 12345678910111213hello,2022-03-25 16:39:45hello,2022-03-25 16:39:46hello,2022-03-25 16:39:47hello,2022-03-25 16:39:48hello,2022-03-25 16:39:49hello,2022-03-25 16:39:50hello,2022-03-25 16:39:47hello,2022-03-25 16:39:46hello,2022-03-25 16:39:51hello,2022-03-25 16:39:52hello,2022-03-25 16:39:53hello,2022-03-25 16:39:54hello,2022-03-25 16:39:55 设置迟到时间： 1234567891011121314151617181920drop table MyTable;-- 允许Flink处理延迟以5秒内的迟到数据，修改最大乱序时间CREATE TABLE MyTable (item STRING,ts TIMESTAMP(3), -- TIMESTAMP 类型的时间戳WATERMARK FOR ts AS ts - INTERVAL &#x27;5&#x27; SECOND) WITH (&#x27;connector&#x27; = &#x27;socket&#x27;,&#x27;hostname&#x27; = &#x27;node1&#x27;,&#x27;port&#x27; = &#x27;9999&#x27;,&#x27;format&#x27; = &#x27;csv&#x27;);SELECTTUMBLE_START(ts, INTERVAL &#x27;5&#x27; SECOND) AS window_start,TUMBLE_END(ts, INTERVAL &#x27;5&#x27; SECOND) AS window_end,TUMBLE_ROWTIME(ts, INTERVAL &#x27;5&#x27; SECOND) as window_rowtime,item,count(item) as total_itemFROM MyTableGROUP BY TUMBLE(ts, INTERVAL &#x27;5&#x27; SECOND), item; 基于DataStream的水印水印策略设置WatermarkStrategy 可以在 Flink 应用程序中的两处使用： 第一种是直接在数据源上使用 第二种是直接在非数据源的操作之后使用 第一种方式相比会更好，因为数据源可以利用 watermark 生成逻辑中有关分片&#x2F;分区（shards&#x2F;partitions&#x2F;splits）的信息。使用这种方式，数据源通常可以更精准地跟踪 watermark，整体 watermark 生成将更精确。直接在源上指定 WatermarkStrategy意味着你必须使用特定数据源接口，例如与kafka链接，使用kafka Connerctor。 仅当无法直接在数据源上设置策略时，才应该使用第二种方式（在任意转换操作之后设置 WatermarkStrategy） 直接在数据源中使用（比如kafka） 123456789final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();KafkaSource&lt;String&gt; source = KafkaSource.&lt;String&gt;builder() .setBootstrapServers(brokers) .setTopics(&quot;input-topic&quot;) .setGroupId(&quot;my-group&quot;) .setStartingOffsets(OffsetsInitializer.earliest()).setValueOnlyDeserializer(new SimpleStringSchema()).build();env.fromSource(source, WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(20)), &quot;Kafka Source&quot;); 直接在非数据源的操作之后使用 123456789101112131415final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;MyEvent&gt; stream = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter(), typeInfo);DataStream&lt;MyEvent&gt; withTimestampsAndWatermarks = stream .filter( event -&gt; event.severity() == WARNING ) .assignTimestampsAndWatermarks(&lt;watermark strategy&gt;);withTimestampsAndWatermarks .keyBy( (event) -&gt; event.getGroup() ) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .reduce( (a, b) -&gt; a.add(b) ) .addSink(...); 使用去获取流并生成带有时间戳的元素和 watermark 的新流时，如果原始流已经具有时间戳或 watermark，则新指定的时间戳分配器将覆盖原有的时间戳和 watermark。WatermarkStrategy。 水印策略案例单调递增生成水印周期性 watermark 生成方式的一个最简单特例就是你给定的数据源中数据的时间戳升序出现。在这种情况下，当前时间戳就可以充当 watermark，因为后续到达数据的时间戳不会比当前的小。 1WatermarkStrategy.forMonotonousTimestamps(); 这个也就是相当于上述的延迟策略去掉了延迟时间，以event中的时间戳充当了水印。 在程序中可以这样使用： 123DataStream dataStream = ...... ;dataStream.assignTimestampsAndWatermarks(WatermarkStrategy.forMonotonousTimestamps());// 它的底层实现是AscendingTimestampsWatermarks，其实它就是BoundedOutOfOrdernessWatermarks类的一个子类，没有了延迟时间，可以通过具体的源码查看实现. 案例演示： 对有序的数据流添加水印，底层调用的是固定延迟生成水印，只是传递的水印等待时间是0，意味着不考虑乱序问题 使用单点递增水印，解决的是数据有序的场景 需求：从socket接受数据，进行转换，然后应用窗口，每隔5s生成一个窗口（非系统时间驱动窗口计算，数据中携带的事件时间），使用水印时间触发窗口计算 eventTime一定是一个毫秒值的时间戳，否则无法参与计算 固定延迟生成水印通过静态方法forBoundedOutOfOrderness提供,入参接收一个Duration类型的时间间隔，也就是我们可以接受的最大的延迟时间。使用这种延迟策略的时候需要我们对数据的延迟时间有一个大概的预估判断。 1WatermarkStrategy.forBoundedOutOfOrderness(Duration maxOutOfOrderness) 我们实现一个延迟3秒的固定延迟水印： 12DataStream dataStream = ...... ;dataStream.assignTimestampsAndWatermarks(WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(3))); 他的底层是使用的WatermarkGenerator接口的一个实现类BoundedOutOfOrdernessWatermarks。重写实现了两个方法： onEvent ：每个元素都会调用这个方法，如果我们想依赖每个元素生成一个水印，然后发射到下游。 onPeriodicEmit : 如果数据量比较大的时候，我们每条数据都生成一个水印的话，会影响性能，所以这里还有一个周期性生成水印的方法。这个水印的生成周期可以这样设置： 1env.getConfig().setAutoWatermarkInterval(5000L); 固定延迟水印策略1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.itheima.flink.watermark;import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import java.time.Duration;public class WaterMarkStrategyDemo &#123; public static void main(String[] args) throws Exception &#123; // 1、构建流处理运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2、从网络端口中获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3、提取时间戳并设置水印策略 streamSource.assignTimestampsAndWatermarks( // 指定单调递增的水印策略 // 底层是调用的AscendingTimestampsWatermarks // AscendingTimestampsWatermarks又继承自BoundedOutOfOrdernessWatermarks // 本质上单调递增的水印策略就是固定延迟为0的水印策略 // WatermarkStrategy.forMonotonousTimestamps() // BoundedOutOfOrdernessWatermarks实现WatermarkGenerator&lt;T&gt;接口 WatermarkStrategy.&lt;String&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5)) .withTimestampAssigner(new SerializableTimestampAssigner&lt;String&gt;() &#123; @Override public long extractTimestamp(String element, long recordTimestamp) &#123; return Long.parseLong(element.split(&quot;,&quot;)[1]) * 1000; &#125; &#125;) ).keyBy(str -&gt; str.split(&quot;,&quot;)[0]) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .process(new ProcessWindowFunction&lt;String, String, String, TimeWindow&gt;() &#123; @Override public void process(String s, ProcessWindowFunction&lt;String, String, String, TimeWindow&gt;.Context context, Iterable&lt;String&gt; elements, Collector&lt;String&gt; out) throws Exception &#123; elements.forEach(System.out::println); &#125; &#125;); env.execute(); &#125;&#125; 多并行度设置水印两个基本原则： 一对多：进行广播 多对一：取最小值 多并行度水印策略123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.itheima.flink.watermark;import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import java.time.Duration;public class ParallelWaterMarkStrategy &#123; public static void main(String[] args) throws Exception &#123; // 1、构建流处理运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(2); // 2、从网络端口中获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3、提取时间戳并设置水印策略 streamSource .map(new MapFunction&lt;String, Tuple2&lt;String, String&gt;&gt;() &#123; @Override public Tuple2&lt;String, String&gt; map(String value) throws Exception &#123; return Tuple2.of(value.split(&quot;,&quot;)[0], value.split(&quot;,&quot;)[1]); &#125; &#125;) .assignTimestampsAndWatermarks( WatermarkStrategy.&lt;Tuple2&lt;String, String&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5)) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, String&gt;&gt;() &#123; @Override public long extractTimestamp(Tuple2&lt;String, String&gt; element, long recordTimestamp) &#123; return Long.parseLong(element.f1) * 1000; &#125; &#125;) ).keyBy(t -&gt; t.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .process(new ProcessWindowFunction&lt;Tuple2&lt;String, String&gt;, Tuple2&lt;String, String&gt;, String, TimeWindow&gt;() &#123; @Override public void process(String s, ProcessWindowFunction&lt;Tuple2&lt;String, String&gt;, Tuple2&lt;String, String&gt;, String, TimeWindow&gt;.Context context, Iterable&lt;Tuple2&lt;String, String&gt;&gt; elements, Collector&lt;Tuple2&lt;String, String&gt;&gt; out) throws Exception &#123; System.out.println(&quot;当前窗口为：&quot; + context.window().toString()); elements.forEach(out::collect); &#125; &#125;).print(); env.execute(); &#125;&#125; 处理空闲数据案例在某些情况下，由于数据产生的比较少，导致一段时间内没有数据产生，进而就没有水印的生成，导致下游依赖水印的一些操作就会出现问题，比如某一个算子的上游有多个算子，这种情况下，水印是取其上游两个算子的较小值，如果上游某一个算子因为缺少数据迟迟没有生成水印，就会出现eventtime倾斜问题，导致下游没法触发计算。 所以filnk通过WatermarkStrategy.withIdleness()方法允许用户在配置的时间内（即超时时间内）没有记录到达时将一个流标记为空闲。这样就意味着下游的数据不需要等待水印的到来。 当下次有水印生成并发射到下游的时候，这个数据流重新变成活跃状态。 通过下面的代码来实现对于空闲数据流的处理 123WatermarkStrategy .&lt;Tuple2&lt;Long, String&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withIdleness(Duration.ofMinutes(1)); 设置最大空闲时间水印策略12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.itheima.flink.watermark;import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.typeinfo.Types;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import java.time.Duration;public class WithIdlenessWaterMarkStrategy &#123; public static void main(String[] args) throws Exception &#123; // 1、构建流处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(10); // 2、从网络端口中获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3、设置水印策略 SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; watermarks = streamSource .map(line -&gt; Tuple2.of(line.split(&quot;,&quot;)[0], line.split(&quot;,&quot;)[1])) .returns(Types.TUPLE(Types.STRING, Types.STRING)) .assignTimestampsAndWatermarks( WatermarkStrategy.&lt;Tuple2&lt;String, String&gt;&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5)) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple2&lt;String, String&gt;&gt;() &#123; @Override public long extractTimestamp(Tuple2&lt;String, String&gt; element, long recordTimestamp) &#123; return Long.parseLong(element.f1) * 1000; &#125; &#125;) .withIdleness(Duration.ofSeconds(30)) ); // 4、窗口计算输出结果 watermarks.keyBy(t -&gt; t.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .process(new ProcessWindowFunction&lt;Tuple2&lt;String, String&gt;, Tuple2&lt;String, String&gt;, String, TimeWindow&gt;() &#123; @Override public void process(String s, ProcessWindowFunction&lt;Tuple2&lt;String, String&gt;, Tuple2&lt;String, String&gt;, String, TimeWindow&gt;.Context context, Iterable&lt;Tuple2&lt;String, String&gt;&gt; elements, Collector&lt;Tuple2&lt;String, String&gt;&gt; out) throws Exception &#123; System.out.println(&quot;当前窗口信息：&quot; + context.window().toString()); elements.forEach(out::collect); &#125; &#125;).print(); // 5、提交作业 env.execute(); &#125;&#125; 长期延迟数据处理水印机制(水位线、watermark)机制可以帮助我们在短期延迟下，允许乱序数据的到来。这个机制很好的处理了那些因为网络等情况短期延迟的数据，让窗口等它们一会儿。 水印机制无法长期的等待下去，因为水印机制简单说就是让窗口一直等在那里，等达到水印时间才会触发计算和关闭窗口。这个等待不能一直等，因为会一直缓着数据不计算。一般水印也就是几秒钟最多几分钟而已。 这个场景的解决方式就是：延迟数据处理机制(allowedLateness方法) 水印： 乱序数据处理（时间很短的延迟） 延迟处理：长期延迟数据的处理机制 主要的办法是给定一个允许延迟的时间，在该时间范围内仍可以接受处理延迟数据： 设置允许延迟的时间是通过allowedLateness(lateness: Time)设置 保存延迟数据则是通过sideOutputLateData(outputTag: OutputTag[T])保存 获取延迟数据是通过DataStream.getSideOutput(tag: OutputTag[X])获取 自定义水印策略1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.itheima.flink.watermark;import org.apache.flink.api.common.eventtime.*;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import java.time.Duration;import java.util.Random;import static org.apache.flink.util.Preconditions.checkArgument;import static org.apache.flink.util.Preconditions.checkNotNull;public class CustomWaterMarkStrategy &#123; public static void main(String[] args) throws Exception &#123; // 1、构建流处理运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2、从网络端口中获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3、提取时间戳并设置水印策略 streamSource.assignTimestampsAndWatermarks(new WatermarkStrategy&lt;String&gt;() &#123; @Override public WatermarkGenerator&lt;String&gt; createWatermarkGenerator(WatermarkGeneratorSupplier.Context context) &#123; return new CustomWaterMarkGenerator(); &#125; &#125;.withTimestampAssigner(new SerializableTimestampAssigner&lt;String&gt;() &#123; @Override public long extractTimestamp(String element, long recordTimestamp) &#123; return Long.parseLong(element.split(&quot;,&quot;)[1]) * 1000; &#125; &#125;)).keyBy(str -&gt; str.split(&quot;,&quot;)[0]) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .process(new ProcessWindowFunction&lt;String, String, String, TimeWindow&gt;() &#123; @Override public void process(String s, ProcessWindowFunction&lt;String, String, String, TimeWindow&gt;.Context context, Iterable&lt;String&gt; elements, Collector&lt;String&gt; out) throws Exception &#123; elements.forEach(System.out::println); &#125; &#125;); env.execute(); &#125; private static class CustomWaterMarkGenerator implements WatermarkGenerator&lt;String&gt; &#123; /** The maximum timestamp encountered so far. */ private long maxTimestamp = 0L; /** * 来一个元素调用一次 * @param event 输入的元素 * @param eventTimestamp * @param output */ @Override public void onEvent(String event, long eventTimestamp, WatermarkOutput output) &#123; this.maxTimestamp = Math.max(eventTimestamp, maxTimestamp); &#125; @Override public void onPeriodicEmit(WatermarkOutput output) &#123; output.emitWatermark(new Watermark(maxTimestamp + new Random().nextInt(10) * 1000 - 1)); &#125; &#125;&#125; Flink 中的快照机制Checkpoint 检查点为了使 Flink 的状态具有良好的容错性，Flink 提供了检查点机制 (CheckPoints) 。通过检查点机制，Flink 定期在数据流上生成 checkpoint barrier ，当某个算子收到 barrier 时，即会基于当前状态生成一份快照，然后再将该 barrier 传递到下游算子，下游算子接收到该 barrier 后，也基于当前状态生成一份快照，依次传递直至到最后的 Sink 算子上。当出现异常后，Flink 就可以根据最近的一次的快照数据将所有算子恢复到先前的状态。 Checkpoint实现过程Flink 的数据可以粗略分为以下三类： 第一种是元信息，相当于一个 Flink 作业运行起来所需要的最小信息集合，包括比如 Checkpoint 地址、Job Manager、Dispatcher、Resource Manager 等等，这些信息的容错是由 Kubernetes&#x2F;Zookeeper 等系统的高可用性来保障的，不在我们讨论的容错范围内。 Flink 作业运行起来以后，会从数据源读取数据写到 Sink 里，中间流过的数据称为处理的中间数据 Inflight Data (第二类)。 对于有状态的算子比如聚合算子，处理完输入数据会产生算子状态数据 (第三类)。 Flink 会周期性地对所有算子的状态数据做快照，上传到持久稳定的海量存储中 (Durable Bulk Store)，这个过程就是做 Checkpoint。Flink 作业发生错误时，会回滚到过去的一个快照检查点 Checkpoint 恢复。 Checkpointing 的流程分为以下几步： 第一步： 第二步： 第三步： 第四步： Checkpoint参数配置在flink-conf.yaml中配置： 1234567891011121314151617181920212223#开启checkpoint 每5000ms 一次execution.checkpointing.interval: 5000#设置有且仅有一次模式 目前支持 EXACTLY_ONCE、AT_LEAST_ONCE execution.checkpointing.mode: EXACTLY_ONCE#设置checkpoint的存储方式state.backend: filesystem#设置checkpoint的存储位置state.checkpoints.dir: hdfs://node1:8020/checkpoints#设置savepoint的存储位置state.savepoints.dir: hdfs://node1:8020/checkpoints#设置checkpoint的超时时间 即一次checkpoint必须在该时间内完成 不然就丢弃execution.checkpointing.timeout: 2500#设置两次checkpoint之间的最小时间间隔execution.checkpointing.min-pause: 500#设置并发checkpoint的数目execution.checkpointing.max-concurrent-checkpoints: 1#开启checkpoints的外部持久化这里设置了清除job时保留checkpoint，默认值时保留一个 假如要保留3个state.checkpoints.num-retained: 3#默认情况下，checkpoint不是持久化的，只用于从故障中恢复作业。当程序被取消时，它们会被删除。但是你可以配置checkpoint被周期性持久化到外部，类似于savepoints。这些外部的checkpoints将它们的元数据输出到外#部持久化存储并且当作业失败时不会自动清除。这样，如果你的工作失败了，你就会有一个checkpoint来恢复。#ExternalizedCheckpointCleanup模式配置当你取消作业时外部checkpoint会产生什么行为:#RETAIN_ON_CANCELLATION: 当作业被取消时，保留外部的checkpoint。注意，在此情况下，您必须手动清理checkpoint状态。#DELETE_ON_CANCELLATION: 当作业被取消时，删除外部化的checkpoint。只有当作业失败时，检查点状态才可用。execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION State 状态后端Flink 提供了不同的状态后端，用于指定状态的存储方式和位置。 默认情况下，flink的状态会保存在taskmanager的内存中，⽽checkpoint会保存在jobManager的内存中。 Flink 提供了三种可用的状态后端用于在不同情况下进行状态的保存： MemoryStateBackend FsStateBackend RocksDBStateBackend MemoryStateBackendMemoryStateBackend内部将状态（state）数据作为对象保存在java堆内存中（taskManager），通过checkpoint机制，MemoryStateBackend将状态（state）进⾏快照并保存Jobmanager（master）的堆内存中。 使用 MemoryStateBackend 时的注意点： 默认情况下，每一个状态的大小限制为 5 MB。可以通过 MemoryStateBackend 的构造函数增加这个大小。 状态大小受到 akka 帧大小的限制，所以无论怎么调整状态大小配置，都不能大于 akka 的帧大小。 状态的总大小不能超过 JobManager 的内存。 何时使用 MemoryStateBackend： 本地开发或调试时建议使用 MemoryStateBackend，因为这种场景的状态大小的是有限的。 MemoryStateBackend 最适合小状态的应用场景。例如 Kafka Consumer，或者一次仅一记录的函数 （Map, FlatMap，或 Filter）。 全局配置 flink-conf.yaml 123state.backend: hashmap# 可选，当不指定 checkpoint 路径时，默认自动使用 JobManagerCheckpointStoragestate.checkpoint-storage: jobmanager FsStateBackend该持久化存储主要将快照数据保存到文件系统中，目前支持的文件系统主要是 HDFS和本地文件。 FsStateBackend适用的场景： 具有大状态，长窗口，大键 &#x2F; 值状态的作业。 所有高可用性设置。 分布式文件持久化，每次读写都会产生网络IO，整体性能不佳 全局配置 flink-conf.yaml： 1234state.backend: hashmap state.checkpoints.dir: file:///checkpoint-dir/ # 默认为FileSystemCheckpointStorage state.checkpoint-storage: filesystem RocksDBStateBackendRocksDB 是一种嵌入式的本地数据库 RocksDBStateBackend 将处理中的数据使用 RocksDB 存储在本地磁盘上。在 checkpoint 时，整个 RocksDB 数据库会被存储到配置的文件系统中，或者在超大状态作业时可以将****增量****的数据存储到配置的文件系统中。同时 Flink 会将极少的元数据存储在 JobManager 的内存中，或者在 Zookeeper 中（对于高可用的情况）。 何时使用 RocksDBStateBackend： RocksDBStateBackend 最适合用于处理大状态，长窗口，或大键值状态的有状态处理任务。 RocksDBStateBackend 非常适合用于高可用方案。 RocksDBStateBackend 是目前唯一支持增量 checkpoint 的后端。增量 checkpoint 非常适用于超大状态的场景。 全局配置 flink-conf.yaml： 123456state.backend: rocksdbstate.checkpoints.dir: file:///checkpoint-dir/# Optional, Flink will automatically default to FileSystemCheckpointStorage# when a checkpoint directory is specified.state.checkpoint-storage: filesystem 任务重启策略重启策略类型Flink支持的重启策略类型如下： none, off, disable：无重启策略，作业遇到问题直接失败，不会重启。 fixeddelay, fixed-delay：固定延迟重启策略，作业失败后，延迟一定时间重启。但是有最大重启次数限制，超过这个限制后作业失败，不再重启。 failurerate, failure-rate：失败率重启策略，作业失败后，延迟一定时间重启。但是有最大失败率限制。如果一定时间内作业失败次数超过配置值，则标记为真的失败，不再重启。 exponentialdelay, exponential-delay：作业失败后重启延迟时间随着失败次数指数递增。没有最大重启次数限制，无限尝试重启作业。 注意：如果启用了checkpoint并且没有显式配置重启策略，会默认使用fixeddelay策略，最大重试次数为Integer.MAX_VALUE。 全局配置全局配置影响Flink提交的所有作业的。修改全局配置需要编辑flink-conf.yaml文件。 12345678910111213141516171819202122232425262728293031no restartrestart-strategy: nonefixeddelayrestart-strategy: fixed-delay# 尝试重启次数restart-strategy.fixed-delay.attempts: 10# 两次连续重启的间隔时间restart-strategy.fixed-delay.delay: 20 sfailureraterestart-strategy: failure-rate# 两次连续重启的间隔时间restart-strategy.failure-rate.delay: 10 s# 计算失败率的统计时间跨度restart-strategy.failure-rate.failure-rate-interval: 2 min# 计算失败率的统计时间内的最大失败次数restart-strategy.failure-rate.max-failures-per-interval: 10exponentialdelayrestart-strategy: exponential-delay# 初次失败后重启时间间隔（初始值）restart-strategy.exponential-delay.initial-backoff: 1 s# 以后每次失败，重启时间间隔为上一次重启时间间隔乘以这个值restart-strategy.exponential-delay.backoff-multiplier: 2# 每次重启间隔时间的最大抖动值（加或减去该配置项范围内的一个随机数），防止大量作业在同一时刻重启restart-strategy.exponential-delay.jitter-factor: 0.1# 最大重启时间间隔，超过这个最大值后，重启时间间隔不再增大restart-strategy.exponential-delay.max-backoff: 1 min# 多长时间作业运行无失败后，重启间隔时间会重置为初始值（第一个配置项的值）restart-strategy.exponential-delay.reset-backoff-threshold: 1 h 端到端一致性端到端一致性端到端的保障指的是在整个数据处理管道上结果都是正确的。在每个组件都提供自身的保障情况下，整个处理管道上端到端的保障会受制于保障最弱的那个组件。 内部：Checkpoints机制，在发生故障的时候能够恢复各个环节的数据。 Source：保证数据读取之后仍存在，可设置数据读取的偏移量，当发生故障的时候重置偏移量到故障之前的位置。 Sink：从故障恢复时，数据不会重复写入外部系统，需要支持幂等写或事务写。 两阶段提交实现Sink一致性 幂等写：一个操作可以重复执行多次，但只导致一次结果的更改 过程中的不一致 可能导致下游消费者出现脏读 事务写：需要外部数据支持事务写入，可以通过两阶段提交实现事务写 1、Sink算子在一批数据处理过程中，先通过预提交事务开始对外输出数据 2、等这批数据处理完成（即完成checkpoint）后，向checkpoint coordinator上报自己完成checkpoint信息 3、checkpoint coordinator收到所有算子任务完成ck的信息后，再向所有算子任务广播本次ck的完成信息 4、两阶段事务提交sink算子收到协调器的回调信息时，执行事务commit操作 2pc提交主要实现beginTransaction-开启事务、preCommit准备提交、commit正式提交、abort丢弃四个方法 设置checkpoint和重启策略(DataStreamAPI)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.itheima.flink.checkpoint;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.restartstrategy.RestartStrategies;import org.apache.flink.api.common.time.Time;import org.apache.flink.runtime.state.hashmap.HashMapStateBackend;import org.apache.flink.streaming.api.CheckpointingMode;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.CheckpointConfig;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;public class CheckpointDemo &#123; public static void main(String[] args) throws Exception &#123; // 1、构建流处理运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度 env.setParallelism(1); // 开启checkpoint env.enableCheckpointing(5000); // 设置ck超时时间 env.getCheckpointConfig().setCheckpointTimeout(2000); // 设置ck的一致性语义 env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); // 设置两次ck的间隔 env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500); // 设置ck的并行度 env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // 设置FsStateBackend状态后端 env.setStateBackend(new HashMapStateBackend()); env.getCheckpointConfig().setCheckpointStorage(&quot;hdfs://node1:8020/flink-checkpoints&quot;); // 设置作业取消时，ck信息的保留策略 env.getCheckpointConfig().setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); //指定重启策略 env.setRestartStrategy(RestartStrategies.fixedDelayRestart( // 最大的重启次数 3, // 两次重启之间的时间间隔 Time.seconds(5))); // 2、从网络端口中获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3、处理数据 streamSource.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; if (value.contains(&quot;laowang&quot;)) &#123; System.out.println(1/0); &#125; String[] strings = value.split(&quot; &quot;); for (String str : strings) &#123; out.collect(str); &#125; &#125; &#125;).print(); //如果执行报hdfs权限相关错误,可以执行 hadoop fs -chmod -R 777 / System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);//设置用户名 // 4、提交作业 env.execute(); &#125;&#125;","categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"author":"Johnson Liam"},{"title":"【Sql】SQL经典10题","slug":"2023.09.10","date":"2023-09-09T16:00:00.000Z","updated":"2023-09-17T09:50:11.000Z","comments":true,"path":"2023/09/10/2023.09.10/","link":"","permalink":"http://example.com/2023/09/10/2023.09.10/","excerpt":"","text":"准备工作12345show databases ;create database if not exists db;use db;-- 一些语句会走 MapReduce，所以慢。 可以开启本地化执行的优化。set hive.exec.mode.local.auto=true;-- (默认为false) 访问量统计准备数据+需求分析12345678910111213141516171819202122232425CREATE TABLE db.test1 ( userId string, visitDate string, visitCount INT ) ROW format delimited FIELDS TERMINATED BY &quot;\\t&quot;;INSERT overwrite TABLE db.test1VALUES ( &#x27;u01&#x27;, &#x27;2017/1/21&#x27;, 5 ), ( &#x27;u02&#x27;, &#x27;2017/1/23&#x27;, 6 ), ( &#x27;u03&#x27;, &#x27;2017/1/22&#x27;, 8 ), ( &#x27;u04&#x27;, &#x27;2017/1/20&#x27;, 3 ), ( &#x27;u01&#x27;, &#x27;2017/1/23&#x27;, 6 ), ( &#x27;u01&#x27;, &#x27;2017/2/21&#x27;, 8 ), ( &#x27;u02&#x27;, &#x27;2017/1/23&#x27;, 6 ), ( &#x27;u01&#x27;, &#x27;2017/2/22&#x27;, 4 );-- 要求使用 SQL 统计出每个用户的累计访问次数，如下表所示：|用户id | 月份 | 小计 | 累积----------------------------| u01 | 2017-01 | 11 | 11 || u01 | 2017-02 | 12 | 23 || u02 | 2017-01 | 12 | 12 || u03 | 2017-01 | 8 | 8 || u04 | 2017-01 | 3 | 3 | 答案1234567891011121314151617with t1 as ( select userId, date_format(regexp_replace(visitDate, &quot;/&quot;, &quot;-&quot;), &#x27;yyyy-MM&#x27;) vm, visitCountfrom tb ),t2 as (select userId, vm, sum(vm) as monthSum from t1 group by userId, vm)select userId &#x27;用户id&#x27;, vm &#x27;月份&#x27;, monthSum &#x27;小计&#x27;, sum(monthSum) over(partition by userId order by monthSum desc) &#x27;累计&#x27;from t2; 电商场景TopN统计准备数据+需求分析1234567891011121314151617181920212223242526272829303132-- 需求有50W个京东店铺，每个顾客访客访问任何一个店铺的任何一个商品时都会产生一条访问日志， -- 访问日志存储的表名为Visit，访客的用户id为user_id，被访问的店铺名称为shop，数据如下：CREATE TABLE db.test2 ( user_id string, shop string ) ROW format delimited FIELDS TERMINATED BY &#x27;\\t&#x27;;INSERT INTO TABLE db.test2 VALUES( &#x27;u1&#x27;, &#x27;a&#x27; ),( &#x27;u2&#x27;, &#x27;b&#x27; ),( &#x27;u1&#x27;, &#x27;b&#x27; ),( &#x27;u1&#x27;, &#x27;a&#x27; ),( &#x27;u3&#x27;, &#x27;c&#x27; ),( &#x27;u4&#x27;, &#x27;b&#x27; ),( &#x27;u1&#x27;, &#x27;a&#x27; ),( &#x27;u2&#x27;, &#x27;c&#x27; ),( &#x27;u5&#x27;, &#x27;b&#x27; ),( &#x27;u4&#x27;, &#x27;b&#x27; ),( &#x27;u6&#x27;, &#x27;c&#x27; ),( &#x27;u2&#x27;, &#x27;c&#x27; ),( &#x27;u1&#x27;, &#x27;b&#x27; ),( &#x27;u2&#x27;, &#x27;a&#x27; ),( &#x27;u2&#x27;, &#x27;a&#x27; ),( &#x27;u3&#x27;, &#x27;a&#x27; ),( &#x27;u5&#x27;, &#x27;a&#x27; ),( &#x27;u5&#x27;, &#x27;a&#x27; ),( &#x27;u5&#x27;, &#x27;a&#x27; );--（1）每个店铺的UV（访客数）-- UV和PV-- PV是访问当前网站所有的次数-- UV是访问当前网站的客户数(需要去重)--(2)每个店铺访问次数top3的访客信息。输出店铺名称、访客id、访问次数 答案123456789101112131415161718192021-- 2.1select shop, count(distinct user_id) as cntfrom test group by shop -- 2.2with t1 as (select shop, user_id, count(user_id) as cntfrom test group by shop, user_id),t2 as (select shop, user_id, cnt, row_number over(partition by shop order by cnt desc) rnfrom t1 )select * from t2 where rn &lt;= 3; 订单量统计准备数据+需求分析12345678910111213141516171819202122-- 需求已知一个表STG.ORDER，有如下字段:Date，Order_id，User_id，amount。 -- 数据样例:2017-01-01,10029028,1000003251,33.57。 CREATE TABLE db.test3 ( dt string, order_id string, user_id string, amount DECIMAL ( 10, 2 ) )ROW format delimited FIELDS TERMINATED BY &#x27;\\t&#x27;;INSERT overwrite TABLE db.test3 VALUES (&#x27;2017-01-01&#x27;,&#x27;10029028&#x27;,&#x27;1000003251&#x27;,33.57), (&#x27;2017-01-01&#x27;,&#x27;10029029&#x27;,&#x27;1000003251&#x27;,33.57), (&#x27;2017-01-01&#x27;,&#x27;100290288&#x27;,&#x27;1000003252&#x27;,33.57), (&#x27;2017-02-02&#x27;,&#x27;10029088&#x27;,&#x27;1000003251&#x27;,33.57), (&#x27;2017-02-02&#x27;,&#x27;100290281&#x27;,&#x27;1000003251&#x27;,33.57), (&#x27;2017-02-02&#x27;,&#x27;100290282&#x27;,&#x27;1000003253&#x27;,33.57), (&#x27;2017-11-02&#x27;,&#x27;10290282&#x27;,&#x27;100003253&#x27;,234), (&#x27;2018-11-02&#x27;,&#x27;10290284&#x27;,&#x27;100003243&#x27;,234);-- 请给出sql进行统计: -- (1)给出 2017年每个月的订单数、用户数、总成交金额。-- (2)给出2017年11月的新客数(指在11月才有第一笔订单) 答案1234567891011121314151617181920212223242526272829-- 3.1with t1 as (select date_format(dt, &#x27;yyyy-MM&#x27;) as month order_id, user_id, amountfrom test)select month, count(order_id), count(user_id), sum(amount)from t1 group by month having substr(month, 1, 4) = &#x27;2017&#x27;-- 3.2 求新客户数(求最小消费的时间为2017-11)with t1 as (select date_format(dt, &#x27;yyyy-MM&#x27;) as month order_id, user_id, amount),t2 as (select user_id, min(month) as minMonth from t1 group by user_id) select count(user_id) from t2 where minMonth = &#x27;2017-11&#x27; 大数据排序准备数据+需求分析123456789101112131415161718192021222324252627282930-- 需求有一个5000万的用户文件(user_id，name，age)，一个2亿记录的用户看电影的记录文件(user_id，url)，CREATE TABLE db.test4user (user_id string,name string,age int);CREATE TABLE db.test4log (user_id string,url string);INSERT INTO TABLE db.test4user VALUES(&#x27;001&#x27;,&#x27;u1&#x27;,10),(&#x27;002&#x27;,&#x27;u2&#x27;,15),(&#x27;003&#x27;,&#x27;u3&#x27;,15),(&#x27;004&#x27;,&#x27;u4&#x27;,20),(&#x27;005&#x27;,&#x27;u5&#x27;,25),(&#x27;006&#x27;,&#x27;u6&#x27;,35),(&#x27;007&#x27;,&#x27;u7&#x27;,40),(&#x27;008&#x27;,&#x27;u8&#x27;,45),(&#x27;009&#x27;,&#x27;u9&#x27;,50),(&#x27;0010&#x27;,&#x27;u10&#x27;,65);INSERT INTO TABLE db.test4log VALUES(&#x27;001&#x27;,&#x27;url1&#x27;),(&#x27;002&#x27;,&#x27;url1&#x27;),(&#x27;003&#x27;,&#x27;url2&#x27;),(&#x27;004&#x27;,&#x27;url3&#x27;),(&#x27;005&#x27;,&#x27;url3&#x27;),(&#x27;006&#x27;,&#x27;url1&#x27;),(&#x27;007&#x27;,&#x27;url5&#x27;),(&#x27;008&#x27;,&#x27;url7&#x27;),(&#x27;009&#x27;,&#x27;url5&#x27;),(&#x27;0010&#x27;,&#x27;url1&#x27;);-- 根据年龄段观看电影的次数进行排序？ 答案123456789101112with t1 as (select user_id, name, concat_ws(&#x27;-&#x27;, floor(age/10) * 10, floor(age/10) * 10 + 10) as ageRange ,from user)select ageRange, count(1) as cntfrom t1 left join log on t1.user_id = log.user_idgroup by ageRange order by cnt desc 活跃用户统计准备数据+需求分析1234567891011121314151617181920212223-- 需求有日志如下，请写出代码求得所有用户和活跃用户的总数及平均年龄。-- （活跃用户指连续两天都有访问记录的用户）CREATE TABLE test5(dt string,user_id string,age int)ROW format delimited fields terminated BY &#x27;,&#x27;;INSERT overwrite TABLE db.test5 VALUES (&#x27;2019-02-11&#x27;,&#x27;test_1&#x27;,23),(&#x27;2019-02-11&#x27;,&#x27;test_2&#x27;,19),(&#x27;2019-02-11&#x27;,&#x27;test_3&#x27;,39),(&#x27;2019-02-11&#x27;,&#x27;test_1&#x27;,23),(&#x27;2019-02-11&#x27;,&#x27;test_3&#x27;,39),(&#x27;2019-02-11&#x27;,&#x27;test_1&#x27;,23),(&#x27;2019-02-12&#x27;,&#x27;test_2&#x27;,19),(&#x27;2019-02-13&#x27;,&#x27;test_1&#x27;,23),(&#x27;2019-02-15&#x27;,&#x27;test_2&#x27;,19),(&#x27;2019-02-16&#x27;,&#x27;test_2&#x27;,19);-- 有日志如下，请写出代码求得所有用户和活跃用户的总数及平均年龄。（活跃用户指连续两天都有访问记录的用户）-- type 总数 平均年龄-- &#x27;所有用户&#x27; 3 27-- &#x27;活跃用户&#x27; 1 19 答案1 电商购买金额统计准备数据+需求分析12345678910111213-- 表ordertable字段:(购买用户：userid，金额：money，购买时间：paymenttime(格式：2017-10-01)，订单id：orderidCREATE TABLE db.test6 ( userid string, money decimal(10,2), paymenttime string, orderid string);INSERT INTO TABLE db.test6 VALUES(&#x27;001&#x27;,100,&#x27;2017-10-01&#x27;,&#x27;123&#x27;),(&#x27;001&#x27;,200,&#x27;2017-10-02&#x27;,&#x27;124&#x27;),(&#x27;002&#x27;,500,&#x27;2017-10-01&#x27;,&#x27;125&#x27;),(&#x27;001&#x27;,100,&#x27;2017-11-01&#x27;,&#x27;126&#x27;);-- 请用sql写出所有用户中在今年10月份第一次购买商品的金额 答案电商分组TopN准备数据+需求分析123456789101112131415161718192021222324-- 需求有一个账号表如下，请写出SQL语句CREATE TABLE db.test10( `dist_id` string COMMENT &#x27;区组id&#x27;, `account` string COMMENT &#x27;账号&#x27;, `gold` int COMMENT &#x27;金币&#x27;);INSERT INTO TABLE db.test10 VALUES (&#x27;1&#x27;,&#x27;77&#x27;,18), (&#x27;1&#x27;,&#x27;88&#x27;,106), (&#x27;1&#x27;,&#x27;99&#x27;,10), (&#x27;1&#x27;,&#x27;12&#x27;,13), (&#x27;1&#x27;,&#x27;13&#x27;,14), (&#x27;1&#x27;,&#x27;14&#x27;,25), (&#x27;1&#x27;,&#x27;15&#x27;,36), (&#x27;1&#x27;,&#x27;16&#x27;,12), (&#x27;1&#x27;,&#x27;17&#x27;,158), (&#x27;2&#x27;,&#x27;18&#x27;,12), (&#x27;2&#x27;,&#x27;19&#x27;,44), (&#x27;2&#x27;,&#x27;10&#x27;,66), (&#x27;2&#x27;,&#x27;45&#x27;,80), (&#x27;2&#x27;,&#x27;78&#x27;,98);-- 查询各自区组的money排名前十的账号（分组取前10） -- dist_id string &#x27;区组id&#x27;, account string &#x27;账号&#x27;, gold int &#x27;金币&#x27; 答案教育领域SQL准备数据+需求分析1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859-- 现有图书管理数据库的三个数据模型如下： 图书（数据表名：BOOK）-- 读者（数据表名：READER）-- 借阅记录（数据表名：BORROW LOG）CREATE TABLE db.book(book_id string, `SORT` string, book_name string, writer string, OUTPUT string, price decimal(10,2));INSERT INTO TABLE db.book VALUES(&#x27;001&#x27;,&#x27;TP391&#x27;,&#x27;信息处理&#x27;,&#x27;author1&#x27;,&#x27;机械工业出版社&#x27;,&#x27;20&#x27;),(&#x27;002&#x27;,&#x27;TP392&#x27;,&#x27;数据库&#x27;,&#x27;author12&#x27;,&#x27;科学出版社&#x27;,&#x27;15&#x27;),(&#x27;003&#x27;,&#x27;TP393&#x27;,&#x27;计算机网络&#x27;,&#x27;author3&#x27;,&#x27;机械工业出版社&#x27;,&#x27;29&#x27;),(&#x27;004&#x27;,&#x27;TP399&#x27;,&#x27;微机原理&#x27;,&#x27;author4&#x27;,&#x27;科学出版社&#x27;,&#x27;39&#x27;),(&#x27;005&#x27;,&#x27;C931&#x27;,&#x27;管理信息系统&#x27;,&#x27;author5&#x27;,&#x27;机械工业出版社&#x27;,&#x27;40&#x27;),(&#x27;006&#x27;,&#x27;C932&#x27;,&#x27;运筹学&#x27;,&#x27;author6&#x27;,&#x27;科学出版社&#x27;,&#x27;55&#x27;);CREATE TABLE db.reader (reader_id string, company string, name string, sex string, grade string, addr string);INSERT INTO TABLE db.reader VALUES(&#x27;0001&#x27;,&#x27;阿里巴巴&#x27;,&#x27;jack&#x27;,&#x27;男&#x27;,&#x27;vp&#x27;,&#x27;addr1&#x27;),(&#x27;0002&#x27;,&#x27;百度&#x27;,&#x27;robin&#x27;,&#x27;男&#x27;,&#x27;vp&#x27;,&#x27;addr2&#x27;),(&#x27;0003&#x27;,&#x27;腾讯&#x27;,&#x27;tony&#x27;,&#x27;男&#x27;,&#x27;vp&#x27;,&#x27;addr3&#x27;),(&#x27;0004&#x27;,&#x27;京东&#x27;,&#x27;jasper&#x27;,&#x27;男&#x27;,&#x27;cfo&#x27;,&#x27;addr4&#x27;),(&#x27;0005&#x27;,&#x27;网易&#x27;,&#x27;zhangsan&#x27;,&#x27;女&#x27;,&#x27;ceo&#x27;,&#x27;addr5&#x27;),(&#x27;0006&#x27;,&#x27;搜狐&#x27;,&#x27;lisi&#x27;,&#x27;女&#x27;,&#x27;ceo&#x27;,&#x27;addr6&#x27;);CREATE TABLE db.borrow_log(reader_id string, book_id string, borrow_date string);INSERT INTO TABLE db.borrow_log VALUES (&#x27;0001&#x27;,&#x27;002&#x27;,&#x27;2019-10-14&#x27;),(&#x27;0002&#x27;,&#x27;001&#x27;,&#x27;2019-10-13&#x27;),(&#x27;0003&#x27;,&#x27;005&#x27;,&#x27;2019-09-14&#x27;),(&#x27;0004&#x27;,&#x27;006&#x27;,&#x27;2019-08-15&#x27;),(&#x27;0005&#x27;,&#x27;003&#x27;,&#x27;2019-10-10&#x27;),(&#x27;0006&#x27;,&#x27;004&#x27;,&#x27;2019-17-13&#x27;);-- （1）创建图书管理库的图书、读者和借阅三个基本表的表结构。请写出建表语句。-- （2）找出姓李的读者姓名（NAME）和所在单位（COMPANY）。-- （3）查找“高等教育出版社”的所有图书名称（BOOK_NAME）及单价（PRICE），结果按单价降序排序。-- （4）查找价格介于10元和20元之间的图书种类(SORT）出版单位（OUTPUT）和单价（PRICE），-- -- 结果按出版单位 （OUTPUT）和单价（PRICE）升序排序。 -- （5）查找所有借了书的读者的姓名（NAME）及所在单位（COMPANY）。-- （6）求”科学出版社”图书的最高单价、最低单价、平均单价。-- （7）找出当前至少借阅了2本图书（大于等于2本）的读者姓名及其所在单位。-- （8）考虑到数据安全的需要，需定时将“借阅记录”中数据进行备份，-- -- 请使用一条SQL语句，在备份用户bak下创建 与“借阅记录”表结构完全一致的数据表BORROW_LOG_BAK.-- -- 井且将“借阅记录”中现有数据全部复制到 BORROW_L0G_ BAK中。--（9）现在需要将原Oracle数据库中数据迁移至Hive仓库，请写出“图书”在Hive中的建表语句（Hive实现，提示：列 分隔符|；-- -- 数据表数据需要外部导入：分区分别以month＿part、day＿part 命名） 答案服务器SQL统计准备数据+需求分析12345678910111213141516171819202122232425262728293031323334-- 有一个线上服务器访问日志格式如下（用sql答题）| 时间 | 接口 | ip地址 |-------------------------------------------------| 2016/11/9 14:22 | /api/user/login | 110.23.5.33 || 2016/11/9 14:23 | /api/user/detail | 57.3.2.16 || 2016/11/9 15:59 | /api/user/login | 200.6.5.166 || ... | ... | ... |CREATE TABLE db.test8(`date` string, interface string, ip string);INSERT INTO TABLE db.test8 VALUES(&#x27;2016-11-09 11:22:05&#x27;,&#x27;/api/user/login&#x27;,&#x27;110.23.5.23&#x27;),(&#x27;2016-11-09 11:23:10&#x27;,&#x27;/api/user/detail&#x27;,&#x27;57.3.2.16&#x27;),(&#x27;2016-11-09 23:59:40&#x27;,&#x27;/api/user/login&#x27;,&#x27;200.6.5.166&#x27;),(&#x27;2016-11-09 11:14:23&#x27;,&#x27;/api/user/login&#x27;,&#x27;136.79.47.70&#x27;),(&#x27;2016-11-09 11:15:23&#x27;,&#x27;/api/user/detail&#x27;,&#x27;94.144.143.141&#x27;),(&#x27;2016-11-09 11:16:23&#x27;,&#x27;/api/user/login&#x27;,&#x27;197.161.8.206&#x27;),(&#x27;2016-11-09 12:14:23&#x27;,&#x27;/api/user/detail&#x27;,&#x27;240.227.107.145&#x27;),(&#x27;2016-11-09 13:14:23&#x27;,&#x27;/api/user/login&#x27;,&#x27;79.130.122.205&#x27;),(&#x27;2016-11-09 14:14:23&#x27;,&#x27;/api/user/detail&#x27;,&#x27;65.228.251.189&#x27;),(&#x27;2016-11-09 14:15:23&#x27;,&#x27;/api/user/detail&#x27;,&#x27;245.23.122.44&#x27;),(&#x27;2016-11-09 14:17:23&#x27;,&#x27;/api/user/detail&#x27;,&#x27;22.74.142.137&#x27;),(&#x27;2016-11-09 14:19:23&#x27;,&#x27;/api/user/detail&#x27;,&#x27;54.93.212.87&#x27;),(&#x27;2016-11-09 14:20:23&#x27;,&#x27;/api/user/detail&#x27;,&#x27;218.15.167.248&#x27;),(&#x27;2016-11-09 14:24:23&#x27;,&#x27;/api/user/detail&#x27;,&#x27;20.117.19.75&#x27;),(&#x27;2016-11-09 15:14:23&#x27;,&#x27;/api/user/login&#x27;,&#x27;183.162.66.97&#x27;),(&#x27;2016-11-09 16:14:23&#x27;,&#x27;/api/user/login&#x27;,&#x27;108.181.245.147&#x27;),(&#x27;2016-11-09 14:17:23&#x27;,&#x27;/api/user/login&#x27;,&#x27;22.74.142.137&#x27;),(&#x27;2016-11-09 14:19:23&#x27;,&#x27;/api/user/login&#x27;,&#x27;22.74.142.137&#x27;); -- 求11月9号下午14点（14-15点），访问/api/user/login接口的top10的ip地址 答案充值日志SQL统计准备数据+需求分析123456789101112131415161718192021222324252627282930313233-- 需求有一个充值日志表credit_log，-- 字段如下： `dist_id` int &#x27;区组id&#x27;, `account` string &#x27;账号&#x27;, `money` int &#x27;充值金额&#x27;,-- ` create_time` string &#x27;订单时间&#x27; 请写出SQL语句，查询充值日志表2019年01月02号每个区组下充值额最大的账号，-- 要求结果： 区组id，账号，金额，充值时间CREATE TABLE db.test9( dist_id string COMMENT &#x27;区组id&#x27;, account string COMMENT &#x27;账号&#x27;, `money` decimal(10,2) COMMENT &#x27;充值金额&#x27;, create_time string COMMENT &#x27;订单时间&#x27;);INSERT INTO TABLE db.test9 VALUES (&#x27;1&#x27;,&#x27;11&#x27;,100006,&#x27;2019-01-02 13:00:01&#x27;), (&#x27;1&#x27;,&#x27;22&#x27;,110000,&#x27;2019-01-02 13:00:02&#x27;), (&#x27;1&#x27;,&#x27;33&#x27;,102000,&#x27;2019-01-02 13:00:03&#x27;), (&#x27;1&#x27;,&#x27;44&#x27;,100300,&#x27;2019-01-02 13:00:04&#x27;), (&#x27;1&#x27;,&#x27;55&#x27;,100040,&#x27;2019-01-02 13:00:05&#x27;), (&#x27;1&#x27;,&#x27;66&#x27;,100005,&#x27;2019-01-02 13:00:06&#x27;), (&#x27;1&#x27;,&#x27;77&#x27;,180000,&#x27;2019-01-03 13:00:07&#x27;), (&#x27;1&#x27;,&#x27;88&#x27;,106000,&#x27;2019-01-02 13:00:08&#x27;), (&#x27;1&#x27;,&#x27;99&#x27;,100400,&#x27;2019-01-02 13:00:09&#x27;), (&#x27;1&#x27;,&#x27;12&#x27;,100030,&#x27;2019-01-02 13:00:10&#x27;), (&#x27;1&#x27;,&#x27;13&#x27;,100003,&#x27;2019-01-02 13:00:20&#x27;), (&#x27;1&#x27;,&#x27;14&#x27;,100020,&#x27;2019-01-02 13:00:30&#x27;), (&#x27;1&#x27;,&#x27;15&#x27;,100500,&#x27;2019-01-02 13:00:40&#x27;), (&#x27;1&#x27;,&#x27;16&#x27;,106000,&#x27;2019-01-02 13:00:50&#x27;), (&#x27;1&#x27;,&#x27;17&#x27;,100800,&#x27;2019-01-02 13:00:59&#x27;), (&#x27;2&#x27;,&#x27;18&#x27;,100800,&#x27;2019-01-02 13:00:11&#x27;), (&#x27;2&#x27;,&#x27;19&#x27;,100030,&#x27;2019-01-02 13:00:12&#x27;), (&#x27;2&#x27;,&#x27;10&#x27;,100000,&#x27;2019-01-02 13:00:13&#x27;), (&#x27;2&#x27;,&#x27;45&#x27;,100010,&#x27;2019-01-02 13:00:14&#x27;), (&#x27;2&#x27;,&#x27;78&#x27;,100070,&#x27;2019-01-02 13:00:15&#x27;); -- --请写出SQL语句，查询充值日志表2019年01月02号每个区组下充值额最大的账号，要求结果：--区组id，账号，金额，充值时间 答案","categories":[],"tags":[{"name":"HQL","slug":"HQL","permalink":"http://example.com/tags/HQL/"}],"author":"Johnson Liam"},{"title":"【数据治理】华为数据治理案例","slug":"2023.09.09","date":"2023-09-08T16:00:00.000Z","updated":"2023-09-15T13:01:27.000Z","comments":true,"path":"2023/09/09/2023.09.09/","link":"","permalink":"http://example.com/2023/09/09/2023.09.09/","excerpt":"","text":"数据治理思考数据问题: 数据管理责任不清晰,造成数据问题无人决策解决; 数据多源头,造成数据不一致,不可信; 数据大量搬家造成IT重复投资; 数据无定义造成难于理解、难于使用; 各部门发布报告,统计口径不一致,困扰业务决策; 数据形态多样化,数据量迅猛增长,数据处理逻辑复杂,投资大; 华为在数字化转型过程中,解决了上述问题,因为华为认识到只有建立了完整的数据治理体系,保证数据内容的质量,才能够真正有效地挖掘企业内部的数据价值,对外提高竞争力。 数据治理模块域数据治理主要专注于如下模块域: 数据集成 数据集成用来完成数据入湖动作,不是简单的数据搬家,而是按照一定的方法论 进行数据备份。数据入湖的前提条件是满足6项数据标准,包括:明确数据 Owner、发布数据标准、定义数据密级、明确数据源、数据质量评估、元数据注 册。此标准由数据代表在入湖前完成梳理并在数据治理平台上进行资产注册。 数据标准 数据标准管理着重建立统一的数据语言,L1到L5数据层级业务对象的定义是数据 标准的载体,并对应发布包括L1到L5数据层级的数据标准。各业务对象对应物理 实现的IT系统需发布相应的数据字典并进行数据源认证。而对于梳理出来,但没 有落IT系统的业务对象,需在后继的开发中进行数字化落地。 数据开发 数据开发是编排、调度和运维的中心,数据开发是一个提供分析、设计、实施、 部署及维护一站式数据解决方案,完成数据加工、转换和质量提升等。数据开发 屏蔽了各种数据存储的差异,一站式满足从数据集成、数据清洗&#x2F;转换、数据质量 监控等全流程的数据处理,是数据治理实施的主战场。 数据质量 数据质量管理的目标在于保证数据满足使用的要求。数据标准是衡量数据质量最基本的基准。数据质量要求各业务部门对相应数据领域的数据质量全权负责,按业务需求设计数据质量标准,制定数据质量管控目标,并遵循企业数据治理要求进行数据质量度量,制定符合各自业务情况的数据质量政策及数据质量相关的改进计划,持续进行数据质量管控。 数据资产 数据资产包括业务资产、技术资产、指标资产等。数据资产管理是数据治理的重要支撑手段,核心是构建企业级的元数据管理中心、建立数据资产目录、建立数据搜索引擎、实现数据血缘和数据全景可视。其中元数据包括业务元数据、技术元数据和操作元数据,要求将企业所有概念数据模型、逻辑数据模型以及物理数据模型系统化地管理起来,同时建设企业数据地图及数据血缘关系,为数据调用、数据服务、数据运营及运维提供强有力的信息支撑。 数据服务 数据服务通过在整个企业范围统一数据服务设计和实现的规范并进行数据服务生 命周期管理,集约管理数据服务并减少数据调用和集成的开发成本。 数据安全 由于企业使用的数据资源,既有来自于内部业务系统,所有权属于企业的数据, 同时也有来自外部的数据,必须将数据安全纳入数据治理的范畴,对所有企业数 据要求依据数据安全等级定义进行数据安全定级,在数据产生、传输、存储和使 用的过程中进行必要的数据安全访问控制,同时对数据相应的CRUD活动均需产生 日志以完成安全审计。 主数据 主数据管理是数据标准落地和提升数据质量的重要手段,是企业级数据治理的重 要范畴,其目标在于保证在企业范围内重要业务实体数据的一致(定义和实际物理 数据的一致)。主数据管理首先进行企业主数据的识别,然后对已识别主数据按照 主数据规范要求进行数据治理和IT改造,以支撑企业业务流和工具链的打通和串联。 管理中心 数据治理的开展离不开组织、流程和政策的建设,管理中心也管理着数据治理过程中公共核心的统一数据源、数据驾驶舱等,满足不同角色的用户拥有个性化的工作台。 数据治理实践 华为数据治理的规范流程建设,完成了从数据产生、数据整合、数据分析与数据消费 全价值流的规则制定。 华为数据治理组织实践,建立实体化的数据管理组织,虚线向 公司数据管理部汇报,同时组建了跨领域数据联合作战团队。 华为已建立统一的数据 分类管理框架,指导各领域进行分类管理。 华为信息架构框架,通过政策发文明确信 息架构的定义和构成要素,在公司层面建立统一的架构方法。基于ISO8000标准,华 为建立了数据质量管理框架和运作机制,每年例行开展两次公司级数据质量度量,从 “设计“与”执行”两个方面度量数据质量,由公司数据Owner定期发布公司数据质 量报告,牵引各业务领域持续改进数据质量。 数据治理框架架构 人员分配 在战略层面，由数据治理Sponsor和各部门负责人组成的数据治理领导组制定数据治理的战略方向，以构建数据文化和氛围为纲，整体负责数据治理工作的开展、政策的推广和执行，并作为数据治理问题的最终决策组织解决争议，监控和监督数据治理工作的绩效，并确保数据治理工作预算支持。数据治理委员会和各领域数据治理工作组是数据治理战略在运作层面具体的实施团队。其中： 数据治理委员会：由数据治理负责人、数据治理专家和数据架构专家团组成，面向企业进行数据治理工作的统筹并提供工作指导，在整个企业范围定期沟通数据治理工作，形成数据质量精细化管控文化。根据数据治理领导组的愿景和长期目标，建立和管理数据治理流程、阶段目标和计划，设计和维护数据治理方法、总则、工具和平台，协助各数据领域工作组实施数据治理工作，对整体数据治理工作进行度量和汇报，并对跨领域的数据治理问题和争议进行解决和决策。 各领域数据治理工作组：在各领域数据范围内进行数据治理的工作，依据数据治理委员会制定的数据治理方法和总则，制定本领域数据治理目标和工作计划，负责领域数据资产的看护，维护更新相应数据标准和及相关元数据，设计本领域数据度量和规则，监控收集数据质量问题并持续改进提升，主动升级数据相关问题。最终完成领域内数据资产的看护，并支撑数据治理目标的达成。领域数据治理工作组由数据Owner、数据代表、数据管家、数据专员和数据架构师组成。其中： 数据Owner (Data Owners)：数据Owner是领域数据治理工作的责任人。 制定本领域数据治理的目标，工作计划和执行优先级。 建立数据治理责任机制，将本领域的数据治理工作分解到工作组成员,并跟进 建立和推动领域数据文化和氛围。 数据代表 (Data Representatives)数据代表是领域数据治理工作的专家带头人。 深刻理解数据工作的目标、方法、规则、工具，并通过识别关键业务流程和IT系统，对本领域数据治理的路标和工作计划进行细化并排序，最终管理执行。 作为本领域数据治理专家，管理并解决问题和争议，必要时提交数据Owner进行裁决。 对业务环节数据的完整性、及时性、准确性、一致性、唯一性、有效性负责，确保行为即记录，记录即数据，并依据数据质量规格对本领域数据进行度量和报告。 落实本领域信息架构的建设和遵从，负责本领域数据资产的看护，维护相应数据标准和数据目录并更新发布。 承接上下游数据需求，并主动根据领域内应用场景和业务需求识别数据需求，对需求的实现进行推动和管理。 依据相关规定定义本领域数据安全等级，并进行数据授权管理。 数据管家 (Data Stewards)：数据管家是领域数据治理工作的协助者。 确保领域治理工作的流程和内容规范，符合数据治理要求。 协助数据代表进行问题跟踪和解决。 梳理、维护并更新领域数据元数据（业务对象、数据标准、数据模型）。 推广和维护数据治理工具和平台在本领域的应用。 数据专员 (Data Specialists)：数据专员是领域数据治理工作的专家团队。基于本领域数据治理的工作计划，利用数据专项技能，支撑数据代表完成数据Owner分配的各类数据治理工作。 数据架构师 (Data Architects)：数据架构师是领域数据治理工作在IT层面的代表。开发和维护本领域的数据系统或子系统，确保数据在系统中得以记录，数据标准、数据质量规则、数据安全、主&#x2F;参考数据管理、数据服务在系统中得以实施。","categories":[],"tags":[{"name":"Interview","slug":"Interview","permalink":"http://example.com/tags/Interview/"}],"author":"Johnson Liam"},{"title":"【Flink】FlinkSQL及Flink四大基石","slug":"2023.09.08","date":"2023-09-07T16:00:00.000Z","updated":"2023-09-14T03:02:31.000Z","comments":true,"path":"2023/09/08/2023.09.08/","link":"","permalink":"http://example.com/2023/09/08/2023.09.08/","excerpt":"","text":"Flink SQL基本介绍实操FlinkSQL启动!!1234# 先启动集群/export/server/flink-1.15.2/bin/start-cluster.sh# 再启动客户端/export/server/flink-1.15.2/bin/sql-client.sh 设置输出模式设置输出模式：我一般用的是tableau模式. 123456# 表格模式（table mode）在内存中实体化结果，并将结果用规则的分页表格可视化展示出来。执行如下命令启用：SET sql-client.execution.result-mode=table;# 变更日志模式（changelog mode）不会实体化和可视化结果，而是由插入（+）和撤销（-）组成的持续查询产生结果流：SET sql-client.execution.result-mode=changelog;# Tableau模式（tableau mode）更接近传统的数据库，会将执行的结果以制表的形式直接打在屏幕之上：SET sql-client.execution.result-mode=tableau; 全心全意写SQL执行 SQL 查询： 123SELECT &#x27;Hello World&#x27;;SELECT name, COUNT(*) AS cnt FROM (VALUES (&#x27;Bob&#x27;), (&#x27;Alice&#x27;), (&#x27;Greg&#x27;), (&#x27;Bob&#x27;)) AS NameTable(name) GROUP BY name; SQL 查询案例 案例场景：计算每一种商品（sku_id 唯一标识）的售出个数、总销售额、平均销售额、最低价、最高价 数据准备：数据源为商品的销售流水（sku_id：商品，price：销售价格），然后写入到 Kafka 的指定 topic（sku_id：商品，count_result：售出个数、sum_result：总销售额、avg_result：平均销售额、min_result：最低价、max_result：最高价）当中 SQL Client中演示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//1.创建一个数据源（输入）表，这里的数据源是 flink 自带的一个随机 mock 数据的数据源。CREATE TABLE source_table ( sku_id STRING, price BIGINT) WITH ( &#x27;connector&#x27; = &#x27;datagen&#x27;, &#x27;rows-per-second&#x27; = &#x27;1&#x27;, &#x27;fields.sku_id.length&#x27; = &#x27;1&#x27;, &#x27;fields.price.min&#x27; = &#x27;1&#x27;, &#x27;fields.price.max&#x27; = &#x27;1000000&#x27;);//2.创建一个数据汇（输出）表，输出到 kafka 中CREATE TABLE sink_table ( sku_id STRING, count_result BIGINT, sum_result BIGINT, avg_result DOUBLE, min_result BIGINT, max_result BIGINT, PRIMARY KEY (`sku_id`) NOT ENFORCED ) WITH ( &#x27;connector&#x27; = &#x27;upsert-kafka&#x27;, &#x27;topic&#x27; = &#x27;test&#x27;, &#x27;properties.bootstrap.servers&#x27; = &#x27;node1.itcast.cn:9092&#x27;, &#x27;key.format&#x27; = &#x27;json&#x27;, &#x27;value.format&#x27; = &#x27;json&#x27; );CREATE TABLE sink_table ( sku_id STRING, count_result BIGINT, sum_result BIGINT, avg_result DOUBLE, min_result BIGINT, max_result BIGINT, PRIMARY KEY (`sku_id`) NOT ENFORCED ) WITH ( &#x27;connector&#x27; = &#x27;print&#x27; );//3.执行一段 group by 的聚合 SQL 查询insert into sink_table select sku_id, count(*) as count_result, sum(price) as sum_result, avg(price) as avg_result, min(price) as min_result, max(price) as max_result from source_table group by sku_id ; 理论(Theory)标准SQL分类(回顾) DML（Data Manipulation Language）：数据操作语言，用来定义数据库中的记录 DCL（Data Control Language）：数据控制语言，用来定义访问权限和安全级别 DQL（Data Query Language）：数据查询语言，用来查询记录 DDL（Data Definition Language）：数据定义语言，用来定义数据库中的对象 Flink SQL的优势 FlnkSQL比DataStreamAPI、DataSetAPI实现简单、方便 TableAPI和SQL是流批通用的，代码可以完全复用 TableAPI和SQL可以使用Calcite的SQL优化器，可以实现自动程序优化更容易写出执行效率高的应用 Flink1.9版本引入了阿里巴巴的Blink实现流批一体 SQL解析与优化引擎Apache CalciteApache Calcite是一款使用Java编程语言编写的开源动态数据管理框架，它具备很多常用的数据库管理需要的功能，比如：SQL解析、SQL校验、SQL查询优化、SQL生成以及数据连接查询，目前使用Calcite作为SQL解析与优化引擎的有Hive、Drill、Flink、Phoenix和Storm。 Calcite中提供了RBO（Rule-Based Optimization：基于规则）和CBO（Cost-Based Optimization：基于代价）两种优化器，在保证语义的基础上，生成执行成本最低的SQL逻辑树。 TableEnvironment APITableEnvironment：Table API &amp; SQL 的都集成在一个统一上下文（即 TableEnvironment）中，其地位等同于 DataStream API 中的 StreamExecutionEnvironment 的地位 1234TableEnvironment::executeSql：用于 SQL API 中，可以执行一段完整 DDL，DML SQL。举例，方法入参可以是 CREATE TABLE xxx，INSERT INTO xxx SELECT xxx FROM xxx。TableEnvironment::from(xxx)：用于 Table API 中，可以以强类型接口的方式运行。方法入参是一个表名称。TableEnvironment::sqlQuery：用于 SQL API 中，可以执行一段查询 SQL，并把结果以 Table 的形式返回。举例，方法的入参是 SELECT xxx FROM xxxTable::executeInsert：用于将 Table 的结果插入到结果表中。方法入参是写入的目标表。 TableEnvironment 的功能 Catalog 管理：Catalog 可以理解为 Flink 的 MetaStore，类似 Hive MetaStore 对在 Hive 中的地位，关于 Flink Catalog 的详细内容后续进行介绍 表管理：在 Catalog 中注册表 SQL 查询：（这 TMD 还用说，最基本的功能啊），就像 DataStream 中提供了 addSource、map、flatmap 等接口 UDF 管理：注册用户定义（标量函数：一进一出、表函数：一进多出、聚合函数：多进一出）函数 UDF 扩展：加载可插拔 Module（Module 可以理解为 Flink 管理 UDF 的模块，是可插拔的，可以自定义 Module，去支持奇奇怪怪的 UDF 功能） DataStream 和 Table（Table API &amp; SQL 的查询结果）之间进行转换：1.13 版本的只有流任务支持，批任务不支持。1.14 支持流批 创建方式方式一：通过 EnvironmentSettings 创建 TableEnvironment 1234567// 1. 就是设置一些环境信息final EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();// 2. 创建 TableEnvironmentfinal TableEnvironment tEnv = TableEnvironment.create(settings);// 如果是 in_streaming_mode，则最终创建出来的 TableEnvironment 实例为 StreamTableEnvironmentImpl// 如果是 in_batch_mode，则最终创建出来的 TableEnvironment 实例为 TableEnvironmentImpl// 虽然两者都继承了 TableEnvironment 接口，但是 StreamTableEnvironmentImpl 支持的功能更多一些。可以直接去看看接口实验一下，这里就不进行详细介绍。 方式二：通过已有的 StreamExecutionEnvironment 创建 TableEnvironment 12StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); SQL中的表外部表与视图一个表的全名（标识）会由三个部分组成：Catalog 名称.数据库名称.表名称。 如果 Catalog 名称或者数据库名称没有指明，就会使用当前默认值 default。 外部表 TABLE：描述的是外部数据，例如文件（HDFS）、消息队列（Kafka）等。依然拿离线 Hive SQL 举个例子，离线中一个表指的是 Hive 表，也就是所说的外部数据。 视图 VIEW：从已经存在的表中创建，视图一般是一个 SQL 逻辑的查询结果。对比到离线的 Hive SQL 中，在离线的场景（Hive 表）中 VIEW 也都是从已有的表中去创建的。 注意：对于视图不会真的产生一个中间表供下游多个查询去引用，即多个查询不共享这个 Table 的结果，可以理解为是一种中间表的简化写法，不会先产出一个中间表结果，然后将这个结果在下游多个查询中复用，后续的多个查询会将这个 Table 的逻辑执行多次。类似于 with tmp as (DML) 的语法 临时表与永久表 临时表：通常保存于内存中并且仅在创建它们的 Flink session（可以理解为一次 Flink 任务的运行）持续期间存在。这些表对于其它 session（即其他 Flink 任务或非此次运行的 Flink 任务）是不可见的。 永久表：需要外部 Catalog（例如 Hive Metastore）来持久化表的元数据。一旦永久表被创建，它将对任何连接到这个 Catalog 的 Flink session 可见且持续存在，直至从 Catalog 中被明确删除。 如果临时表和永久表使用了相同的名称（Catalog名.数据库名.表名）。那么在这个 Flink session 中，你的任务访问到这个表时，访问到的永远是临时表（即相同名称的表，临时表会屏蔽永久表）。 Flink SQL数据类型 Java八大基本数据类型：byte→short→int→long float→double char boolean String类型：引用类型，底层是字符数组char[] 原子数据类型 字符串类型 CHARCHAR(n) 定长字符串，就和 Java 中的 Char 一样，n 代表字符的定长取值范围 [1-2,147,483,647]。如果不指定 n，则默认为 1。 VARCHARVARCHAR(n)STRING 可变长字符串，就和 Java 中的 String 一样，n 代表字符的最大长度取值范围 [1-2,147,483,647]。如果不指定 n，则默认为 1。STRING 等同于 VARCHAR(2147483647)。 二进制类型 BINARYBINARY(n) 定长二进制字符串，n 代表定长取值范围 [1, 2,147,483,647]。如果不指定 n，则默认为 1。 VARBINARYVARBINARY(n)BYTES 可变长二进制字符串，n 代表字符的最大长度取值范围 [1, 2,147,483,647]。如果不指定 n，则默认为 1。BYTES 等同于 VARBINARY(2147483647)。 数值类型 DECIMALDECIMAL(p)DECIMAL(p, s)DEC、DEC(p)DEC(p, s)NUMERICNUMERIC(p)NUMERIC(p, s) 固定长度和精度的数值类型，就和 Java 中的 BigDecimal 一样p 代表数值位数（长度），取值范围 [1, 38]s 代表小数点后的位数（精度），取值范围 [0, p]如果不指定，p 默认为 10，s 默认为 0。 TINYINT -128 to 127 的 1 字节大小的有符号整数，就和 Java 中的 byte 一样。 SMALLINT -32,768 to 32,767 的 2 字节大小的有符号整数，就和 Java 中的 short 一样。 INTINTEGER -2,147,483,648 to 2,147,483,647 的 4 字节大小的有符号整数，就和 Java 中的 int 一样。 BIGINT -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 的 8 字节大小的有符号整数，就和 Java 中的 long 一样。 FLOAT 4 字节大小的单精度浮点数值，就和 Java 中的 float 一样。 DOUBLEDOUBLE PRECISION 8 字节大小的双精度浮点数值，就和 Java 中的 double 一样。 特殊类型 NULL类型 NULL Raw类型 RAW(‘class’, ‘snapshot’) 。只会在数据发生网络传输时进行序列化，反序列化操作，可以保留其原始数据。以 Java 举例，class 参数代表具体对应的 Java 类型，snapshot 代表类型在发生网络传输时的序列化器 布尔类型 BOOLEAN 时间类型 DATE 由年-月-日组成的 不带时区含义 的日期类型，取值范围 [0000-01-01, 9999-12-31] TIMETIME(p) 由小时：分钟：秒[.小数秒]组成的 不带时区含义 的的时间的数据类型，精度高达纳秒取值范围 [00:00:00.000000000到23:59:59.9999999]。其中 p 代表小数秒的位数，取值范围 [0, 9]，如果不指定 p，默认为 0。 TIMESTAMPTIMESTAMP(p)TIMESTAMP WITHOUT TIME ZONETIMESTAMP(p) WITHOUT TIME ZONE 由 年-月-日 小时：分钟：秒[.小数秒] 组成的不带时区含义的时间类型取值范围 [0000-01-01 00:00:00.000000000, 9999-12-31 23:59:59.999999999]。其中 p 代表小数秒的位数，取值范围 [0, 9]，如果不指定 p，默认为 6。 TIMESTAMP WITH TIME ZONETIMESTAMP(p) WITH TIME ZONE 由 年-月-日 小时：分钟：秒[.小数秒] 时区 组成的 带时区含义 的时间类型取值范围 [0000-01-01 00:00:00.000000000 +14:59, 9999-12-31 23:59:59.999999999 -14:59]。其中 p 代表小数秒的位数，取值范围 [0, 9]，如果不指定 p，默认为 6。 TIMESTAMP_LTZTIMESTAMP_LTZ(p) 由 年-月-日 小时：分钟：秒[.小数秒] 时区 组成的 带时区含义 的时间类型取值范围 [0000-01-01 00:00:00.000000000 +14:59, 9999-12-31 23:59:59.999999999 -14:59]。其中 p 代表小数秒的位数，取值范围 [0, 9]，如果不指定 p，默认为 6。 INTERVAL YEAR TO MONTH INTERVAL DAY TO SECOND interval 的涉及到的种类比较多。INTERVAL 主要是用于给 TIMESTAMP、TIMESTAMP_LTZ 添加偏移量的。举例，比如给 TIMESTAMP 加、减几天、几个月、几年。 TIMESTAMP_LTZ 与 TIMESTAMP WITH TIME ZONE 的区别在于：TIMESTAMP WITH TIME ZONE 的时区信息是携带在数据中的，举例：其输入数据应该是 2022-01-01 00:00:00.000000000 +08:00；TIMESTAMP_LTZ 的时区信息不是携带在数据中的，而是由 Flink SQL 任务的全局配置决定的，我们可以由 table.local-time-zone 参数来设置时区。 INTERVAL演示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960Flink SQL&gt; CREATE TABLE sink_table2 ( result_interval_year TIMESTAMP(3), result_interval_year_p TIMESTAMP(3), result_interval_year_p_to_month TIMESTAMP(3), result_interval_month TIMESTAMP(3), result_interval_day TIMESTAMP(3), result_interval_day_p1 TIMESTAMP(3), result_interval_day_p1_to_hour TIMESTAMP(3), result_interval_day_p1_to_minute TIMESTAMP(3), result_interval_day_p1_to_second_p2 TIMESTAMP(3), result_interval_hour TIMESTAMP(3), result_interval_hour_to_minute TIMESTAMP(3), result_interval_hour_to_second TIMESTAMP(3), result_interval_minute TIMESTAMP(3), result_interval_minute_to_second_p2 TIMESTAMP(3), result_interval_second TIMESTAMP(3), result_interval_second_p2 TIMESTAMP(3)) WITH ( &#x27;connector&#x27; = &#x27;print&#x27;);Flink SQL&gt; INSERT INTO sink_table2SELECT -- Flink SQL 支持的所有 INTERVAL 子句如下，总体可以分为 `年-月`、`日-小时-秒` 两种 -- 1. 年-月。取值范围为 [-9999-11, +9999-11]，其中 p 是指有效位数，取值范围 [1, 4]，默认值为 2。比如如果值为 1000，但是 p = 2，则会直接报错。 -- INTERVAL YEAR f1 + INTERVAL &#x27;10&#x27; YEAR as result_interval_year -- INTERVAL YEAR(p) , f1 + INTERVAL &#x27;100&#x27; YEAR(3) as result_interval_year_p -- INTERVAL YEAR(p) TO MONTH , f1 + INTERVAL &#x27;10-03&#x27; YEAR(3) TO MONTH as result_interval_year_p_to_month -- INTERVAL MONTH , f1 + INTERVAL &#x27;13&#x27; MONTH as result_interval_month -- 2. 日-小时-秒。取值范围为 [-999999 23:59:59.999999999, +999999 23:59:59.999999999]，其中 p1\\p2 都是有效位数，p1 取值范围 [1, 6]，默认值为 2；p2 取值范围 [0, 9]，默认值为 6 -- INTERVAL DAY , f1 + INTERVAL &#x27;10&#x27; DAY as result_interval_day -- INTERVAL DAY(p1) , f1 + INTERVAL &#x27;100&#x27; DAY(3) as result_interval_day_p1 -- INTERVAL DAY(p1) TO HOUR , f1 + INTERVAL &#x27;10 03&#x27; DAY(3) TO HOUR as result_interval_day_p1_to_hour -- INTERVAL DAY(p1) TO MINUTE , f1 + INTERVAL &#x27;10 03:12&#x27; DAY(3) TO MINUTE as result_interval_day_p1_to_minute -- INTERVAL DAY(p1) TO SECOND(p2) , f1 + INTERVAL &#x27;10 00:00:00.004&#x27; DAY TO SECOND(3) as result_interval_day_p1_to_second_p2 -- INTERVAL HOUR , f1 + INTERVAL &#x27;10&#x27; HOUR as result_interval_hour -- INTERVAL HOUR TO MINUTE , f1 + INTERVAL &#x27;10:03&#x27; HOUR TO MINUTE as result_interval_hour_to_minute -- INTERVAL HOUR TO SECOND(p2) , f1 + INTERVAL &#x27;00:00:00.004&#x27; HOUR TO SECOND(3) as result_interval_hour_to_second -- INTERVAL MINUTE , f1 + INTERVAL &#x27;10&#x27; MINUTE as result_interval_minute -- INTERVAL MINUTE TO SECOND(p2) , f1 + INTERVAL &#x27;05:05.006&#x27; MINUTE TO SECOND(3) as result_interval_minute_to_second_p2 -- INTERVAL SECOND , f1 + INTERVAL &#x27;3&#x27; SECOND as result_interval_second -- INTERVAL SECOND(p2) , f1 + INTERVAL &#x27;300&#x27; SECOND(3) as result_interval_second_p2FROM (SELECT TO_TIMESTAMP_LTZ(1640966476500, 3) as f1) 复合数据类型 数组类型 ARRAYt ARRAY 数组最大长度为 2,147,483,647。t 代表数组内的数据类型。举例 ARRAY、ARRAY，其等同于 INT ARRAY、STRING ARRAY Map类型 MAP&lt;kt, vt&gt; Map 类型就和 Java 中的 Map 类型一样，key 是没有重复的。举例 Map&lt;STRING, INT&gt;、Map&lt;BIGINT, STRING&gt; 集合类型 MULTISETt MULTISET 就和 Java 中的 List 类型一样，允许重复的数据。举例 MULTISET，其等同于 INT MULTISET 对象类型 ROW&lt;n0 t0, n1 t1, …&gt;ROW&lt;n0 t0 ‘d0’, n1 t1 ‘d1’, …&gt;ROW(n0 t0, n1 t1, …)ROW(n0 t0 ‘d0’, n1 t1 ‘d1’, …) 就和 Java 中的自定义对象一样。举例：ROW(myField INT, myOtherField BOOLEAN)，其等同于 ROW&lt;myField INT, myOtherField BOOLEAN&gt; 案例演示： 样例数据 123456789101112131415161718192021222324252627282930&#123; &quot;id&quot;:1238123899121, &quot;name&quot;:&quot;itcast&quot;, &quot;date&quot;:&quot;1990-10-14&quot;, &quot;obj&quot;:&#123; &quot;time1&quot;:&quot;12:12:43&quot;, &quot;str&quot;:&quot;sfasfafs&quot;, &quot;lg&quot;:2324342345 &#125;, &quot;arr&quot;:[ &#123; &quot;f1&quot;:&quot;f1str11&quot;, &quot;f2&quot;:134 &#125;, &#123; &quot;f1&quot;:&quot;f1str22&quot;, &quot;f2&quot;:555 &#125; ], &quot;time&quot;:&quot;12:12:43&quot;, &quot;timestamp&quot;:&quot;1990-10-14 12:12:43&quot;, &quot;map&quot;:&#123; &quot;flink&quot;:123 &#125;, &quot;mapinmap&quot;:&#123; &quot;inner_map&quot;:&#123; &quot;key&quot;:234 &#125; &#125;&#125; 开启netcat 1&#123;&quot;id&quot;:1238123899121,&quot;name&quot;:&quot;itcast&quot;,&quot;date&quot;:&quot;1990-10-14&quot;,&quot;obj&quot;:&#123;&quot;time1&quot;:&quot;12:12:43&quot;,&quot;str&quot;:&quot;sfasfafs&quot;,&quot;lg&quot;:2324342345&#125;,&quot;arr&quot;:[&#123;&quot;f1&quot;:&quot;f1str11&quot;,&quot;f2&quot;:134&#125;,&#123;&quot;f1&quot;:&quot;f1str22&quot;,&quot;f2&quot;:555&#125;],&quot;time&quot;:&quot;12:12:43&quot;,&quot;timestamp&quot;:&quot;1990-10-14 12:12:43&quot;,&quot;map&quot;:&#123;&quot;flink&quot;:123&#125;,&quot;mapinmap&quot;:&#123;&quot;inner_map&quot;:&#123;&quot;key&quot;:234&#125;&#125;&#125; 创建映射表 1234567891011121314151617CREATE TABLE json_source ( id BIGINT, name STRING, `date` DATE, obj ROW&lt;time1 TIME,str STRING,lg BIGINT&gt;, arr ARRAY&lt;ROW&lt;f1 STRING,f2 INT&gt;&gt;, `time` TIME, `timestamp` TIMESTAMP(3), `map` MAP&lt;STRING,BIGINT&gt;, mapinmap MAP&lt;STRING,MAP&lt;STRING,INT&gt;&gt;, proctime as PROCTIME() ) WITH ( &#x27;connector&#x27; = &#x27;socket&#x27;, &#x27;hostname&#x27; = &#x27;node1&#x27;, &#x27;port&#x27; = &#x27;9999&#x27;, &#x27;format&#x27; = &#x27;json&#x27;); 查询结果 1select id, name,`date`,obj.str,arr[3].f1,`map`[&#x27;flink&#x27;],mapinmap[&#x27;inner_map&#x27;][&#x27;key&#x27;] from json_source; Flink SQL应用于流流批处理的异同 输入表 处理逻辑 结果表 批处理 静态表：输入数据有限、是有界集合 批式计算：每次执行查询能够访问到完整的输入数据，然后计算，输出完整的结果数据 静态表：数据有限 流处理 动态表：输入数据无限，数据实时增加，并且源源不断 流式计算：执行时不能够访问到完整的输入数据，每次计算的结果都是一个中间结果 动态表：数据无限 要将 SQL 应用于流式任务的三个要解决的核心点： 1.SQL 输入表：分析如何将一个实时的，源源不断的输入流数据表示为 SQL 中的输入表。 2.SQL 处理计算：分析将 SQL 查询逻辑翻译成什么样的底层处理技术才能够实时的处理流式输入数据，然后产出流式输出数据。 3.SQL 输出表：分析如何将 SQL 查询输出的源源不断的流数据表示为一个 SQL 中的输出表。 两种技术方案： 1.动态表：源源不断的输入、输出流数据映射到 动态表 2.连续查询：实时处理输入数据，产出输出数据的实时处理技术 动态表如果把有界数据集当作表，那么无界数据集（流）就是一个随着时间变化持续写入数据的表，Flink中使用动态表表示流，使用静态表表示传统的批处理中的数据集。 流是Flink DataStream中的概念，动态表是Flink SQL中的概念，两者都是无界数据集。动态表在Flink中抽象为Table API 连续查询将SQL查询应用于动态表，会持续执行而不会终止，因为数据会持续的产生，所以连续查询不会给出一个最终结果，而是持续不断地更新结果，实际上给出的总是中间结果。 流上的SQL查询运算与批处理中的SQL查询运算在语义上完全相同，对于相同的数据集计算结果也是相同的。 执行过程 从概念上来说： 1.流转换为动态表 2.在动态表上执行连续查询，生成新的动态表 3.生成的动态表转换回流 从开发上来说： 1.将DataStream注册为Table 2.在Table上应用SQL查询语句，结果为一个新的Table 3.将Table转换为DataStream 第一步-流转换为表第二步-更新和追加查询 1.向结果表中插入新记录、更新旧的记录 2.只会向结果表中插入新记录 同时包含插入（Insert）、更新（Update）的查询必须维护更多的State，消耗更多的CPU、内存资源。 流上使用SQL的限制： 1.需要维护的状态太大 2.计算更新的成本太高 第三步-表转换为流动态表分为三种类型： 1.只有更新行为，表中的结果被持续更新 2.只有插入行为，没有UPDATA和DELETE行为的结果表 3.既有更新行为又有插入行为的结果表 不同的表类型会转换为不同的流对外输出。 Append流 只支持写入行为，输出的结果只有 INSERT 操作的数据。 Retract流 Retract 流包含两种类型的 message： add messages 和 retract messages 。 将 INSERT 操作编码为 add message 将 DELETE 操作编码为 retract message 将 UPDATE 操作编码为更新先前行的 retract message 和更新（新）行的 add message，从而将动态表转换为 retract 流。 Retract 流写入到输出结果表的数据有 -，+ 两种，分别 - 代表撤回旧数据，+ 代表输出最新的数据。这两种数据最终都会写入到输出的数据引擎中。 如果下游还有任务去消费这条流的话，要注意需要正确处理 -，+ 两种数据，防止数据计算重复或者错误。 Upsert流 Upsert 流包含两种类型的 message： upsert messages 和 delete messages。转换为 upsert 流的动态表需要唯一键（唯一键可以由多个字段组合而成）。 将 INSERT 和 UPDATE 操作编码为 upsert message 将 DELETE 操作编码为 delete message Upsert流写入到输出结果表的数据每次输出的结果都是当前根据唯一键的最新结果数据，不会有 Retract流中的 - 回撤数据。 如果下游还有一个任务去消费这条流的话，消费流的算子需要知道唯一键（即 user），以便正确地根据唯一键（user）去拿到每一个 user 当前最新的状态。其与 retract 流的主要区别在于 UPDATE 操作是用单个 message 编码的，因此效率更高。 Flink 中的四大基石Flink之所以能这么流行，离不开它最重要的四个基石：Checkpoint、State、Time、Window。 Flink 中的时间属性三种时间属性 事件时间：必须是由数据本身携带的数据，这个时间标志的是事件产生的时间。 处理时间：指的是算子计算这个数据的时候产生的时间。 到达时间：指的是数据从数据源进入到计算引擎的时间。 指定时间属性 CREATE TABLE DDL 创建表的时候指定 可以在 DataStream 中指定，在后续的 DataStream 转为 Table 中使用 SQL时间案例处理时间案例处理时间语义下，使用当前机器的系统时间作为处理时间。它是时间的最简单概念。它既不需要提取时间戳，也不需要生成watermark。 语法：CREATE TABLE DDL 指定时间戳的方式 12345678CREATE TABLE user_actions ( user_name STRING, data STRING, -- 使用下面这句来将 user_action_time 声明为处理时间 user_action_time AS PROCTIME()) WITH ( ...); 读取’order.csv’文件的数据，在原本的Schema上添加一个虚拟的时间戳列，时间戳列由PROCTIME()函数计算产生。 1234567891011121314-- 创建映射表create table InputTable (`userid` varchar,`timestamp` bigint,`money` double,`category` varchar,`pt` AS PROCTIME()) with (&#x27;connector&#x27; = &#x27;filesystem&#x27;,&#x27;path&#x27; = &#x27;file:///export/data/input/order.csv&#x27;,&#x27;format&#x27; = &#x27;csv&#x27;);-- 描述表desc InputTable ; 事件时间案例Event Time时间语义使用一条数据实际发生的时间作为时间属性，在Table API &amp; SQL中这个字段通常被称为rowtime。这种模式下多次重复计算时，计算结果是确定的。 Event Time时间语义可以保证流处理和批处理的统一。 Event Time时间语义下，我们需要设置每条数据发生时的时间戳，并提供一个Watermark。 语法：CREATE TABLE DDL 指定时间戳的方式 12345678910CREATE TABLE user_actions ( user_name STRING, data STRING, user_action_time TIMESTAMP(3), -- 使用下面这句来将 user_action_time 声明为事件时间，并且声明 watermark 的生成规则，即 user_action_time 减 5 秒 -- 事件时间列的字段类型必须是 TIMESTAMP 或者 TIMESTAMP_LTZ 类型 WATERMARK FOR user_action_time AS user_action_time - INTERVAL &#x27;5&#x27; SECOND) WITH ( ...); 实际应用中时间戳一般都是秒或者是毫秒（BIGINT 类型）需要转换类型 12345678910111213CREATE TABLE user_actions ( user_name STRING, data STRING, -- 1. 这个 ts 就是常见的毫秒级别时间戳 ts BIGINT, -- 2. 将毫秒时间戳转换成 TIMESTAMP_LTZ 类型 time_ltz AS TO_TIMESTAMP_LTZ(ts, 3), -- 3. 使用下面这句来将 user_action_time 声明为事件时间，并且声明 watermark 的生成规则，即 user_action_time 减 5 秒 -- 事件时间列的字段类型必须是 TIMESTAMP 或者 TIMESTAMP_LTZ 类型 WATERMARK FOR time_ltz AS time_ltz - INTERVAL &#x27;5&#x27; SECOND) WITH ( ...); 读取order.csv’文件的数据，定义现有事件时间字段上的 watermark 生成表达式，该表达式将事件时间字段标记为事件时间属性。 12345678910111213141516171819202122232425262728-- 创建映射表-- 这种方式只是增加了一个 rt 的TIMESTAMP列create table InputTable2 ( `userid` varchar, `timestamp` bigint, `money` double, `category` varchar, rt AS TO_TIMESTAMP(FROM_UNIXTIME(`timestamp`)) ) with ( &#x27;connector&#x27; = &#x27;filesystem&#x27;, &#x27;path&#x27; = &#x27;file:///export/data/input/order.csv&#x27;, &#x27;format&#x27; = &#x27;csv&#x27;);-- 描述表desc InputTable2;-- 事件时间需要结合watermark（水位线）使用create table InputTable3 (`userid` varchar,`timestamp` bigint,`money` double,`category` varchar,rt AS TO_TIMESTAMP(FROM_UNIXTIME(`timestamp`)),watermark for rt as rt - interval &#x27;1&#x27; second) with (&#x27;connector&#x27; = &#x27;filesystem&#x27;,&#x27;path&#x27; = &#x27;file:///export/data/input/order.csv&#x27;,&#x27;format&#x27; = &#x27;csv&#x27;); Flink 中的窗口操作窗口概述 在流处理应用中，数据是连续不断的，因此我们不可能等到所有数据都到了才开始处理。当然我们可以每来一个消息就处理一次，但是有时我们需要做一些聚合类的处理，例如：在过去的1分钟内有多少用户点击了我们的网页。在这种情况下，我们必须定义一个窗口，用来收集最近一分钟内的数据，并对这个窗口内的数据进行计算。 Flink 认为 Batch 是 Streaming 的一个特例，所以 Flink 底层引擎是一个流式引擎，在上面实现了流处理和批处理。而窗口（window）就是从 Streaming 到 Batch 的一个桥梁。 一个Window代表有限对象的集合。一个窗口有一个最大的时间戳，该时间戳意味着在其代表的某时间点——所有应该进入这个窗口的元素都已经到达 Window就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。 在Table API &amp; SQL中，主要有两种窗口：Group Windows 和 Over Windows。 Group Windows 根据时间或行计数间隔将组行聚合成有限的组，并对每个组计算一次聚合函数 Over Windows 窗口内聚合为每个输入行在其相邻行范围内计算一个聚合 Group Windows滚动窗口(TumblingTimeWindow) 滚动窗口定义：滚动窗口将每个元素指定给指定窗口大小的窗口。滚动窗口具有固定大小，且不重叠。例如，指定一个大小为 5 分钟的滚动窗口。在这种情况下，Flink 将每隔 5 分钟开启一个新的窗口，其中每一条数都会划分到唯一一个 5 分钟的窗口中： 应用场景：常见的按照一分钟对数据进行聚合，计算一分钟内 PV，UV 数据。 基于DataStream编程12345678910111213141516DataStream&lt;T&gt; input = ...// 基于Event Time的滚动窗口input.keyBy(…).window(TumblingEventTimeWindows.of(Time.seconds(5))).&lt;window function&gt;(...)// 基于Processing Time的滚动窗口input.keyBy(…).window(TumblingProcessingTimeWindows.of(Time.seconds(5))).&lt;window function&gt;(...)// 基于EventTime Time，在小时级滚动窗口上设置15分钟的Offset偏移input.keyBy(…).window(TumblingEventTimeWindows.of(Time.hours(1), Time.minutes(15))).&lt;window function&gt;(...) 注意：时间窗口使用的是timeWindow()也可以使用window()，比如，input.keyBy(…).timeWindow(Time.seconds(1))。timeWindow()是一种简写，传入一个参数则滚动窗口，为窗口大小，若传入两个参数为滑动窗口，第一个参数为窗口大小，第二个参数为滑动时间。该方法在新版本中已过期。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package io.github.windows;import lombok.SneakyThrows;import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.typeinfo.Types;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;public class TumblingWindowExercise &#123; public static void main(String[] args) throws Exception &#123; // 1, 创建DataStream运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度为1, 因为时间窗口是默认是CPU的核数 env.setParallelism(1); // 2, 创建Source算子获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3, 对数据进行处理 SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Long&gt;&gt; tuple3DS = streamSource.flatMap((FlatMapFunction&lt;String, Tuple3&lt;String, Integer, Long&gt;&gt;) (value, out) -&gt; out.collect(Tuple3.of( value.split(&quot;,&quot;)[0], Integer.valueOf(value.split(&quot;,&quot;)[1]), Long.valueOf(value.split(&quot;,&quot;)[2])))) .returns(Types.TUPLE(Types.STRING, Types.INT, Types.LONG)); // 提取时间戳设置事件时间 SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Long&gt;&gt; watermarks = tuple3DS.assignTimestampsAndWatermarks( WatermarkStrategy.&lt;Tuple3&lt;String, Integer, Long&gt;&gt;forMonotonousTimestamps().withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, Integer, Long&gt;&gt;() &#123; @SneakyThrows @Override public long extractTimestamp(Tuple3&lt;String, Integer, Long&gt; element, long recordTimestamp) &#123; // 时间时间指定时的单位为毫秒 return element.f2 * 1000; &#125; &#125;)); // 4, 进行窗口计算 // 基于处理时间的分组窗口(是根据系统时间的推迟触发的计算, 如果没有keyby则会忽略key直接进行字段的计算sum) // 有key的话, 则会一个key一个窗口, 到达指定时间就会触发计算 tuple3DS.keyBy(t -&gt; t.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(10))) .sum(1) .print(); // 基于处理时间的全局窗口 // tuple3DS // .windowAll(TumblingProcessingTimeWindows.of(Time.seconds(3L))) // .sum(1) // .print(); // 基于事件时间的分组窗口(必须要设置水印策略) TumblingEventTimeWindows 换成TumblingProcessingTimeWindows 其他一样 watermarks.keyBy(t -&gt; t.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .sum(1) .print(); // 5, 提交任务并执行 env.execute(); &#125;&#125; 基于SQL编程TUMBLE函数基于时间属性字段将每个元素分配到一个指定大小的窗口中。在流模式下，时间属性字段必须是事件或处理时间属性。在批处理模式下，窗口表函数的时间属性字段必须是TIMESTAMP或TIMESTAMP _LTZ类型的属性。TUMBLE的返回值是一个新的关系，它包括原始关系的所有列，以及另外3列“window_start”、“window_end”、“window_time”，以指示指定的窗口。原始时间属性“timecol”将是窗口TVF之后的常规时间戳列。 TUMBLE函数接受三个必需参数，一个可选参数：TUMBLE(TABLE data, DESCRIPTOR(timecol), size [, offset ]) data: 是一个表参数，可以是与时间属性列的任何关系。 timecol: 是一个列描述符，指示数据的哪些时间属性列应映射到翻转窗口。 size: 是指定滚动窗口宽度的持续时间。 offset: 是一个可选参数，用于指定窗口起始位置的偏移量。 两种 Flink SQL 实现方式： Group Window Aggregation（1.14之前只有此类方案，此方案在 1.14及之后版本已经标记为废弃，不推荐使用） 1234567891011121314151617181920212223-- 数据源表CREATE TABLE source_table ( user_id STRING, price BIGINT, `timestamp` bigint, row_time AS TO_TIMESTAMP(FROM_UNIXTIME(`timestamp`)), watermark for row_time as row_time - interval &#x27;0&#x27; second) WITH ( &#x27;connector&#x27; = &#x27;socket&#x27;, &#x27;hostname&#x27; = &#x27;node1&#x27;, &#x27;port&#x27; = &#x27;9999&#x27;, &#x27;format&#x27; = &#x27;csv&#x27;);-- 数据处理逻辑select user_id, sum(price) as sum_price, UNIX_TIMESTAMP(CAST(tumble_start(row_time, interval &#x27;5&#x27; second) AS STRING)) * 1000 as window_startfrom source_tablegroup by user_id, tumble(row_time, interval &#x27;5&#x27; second); 可以看到 Group Window Aggregation 滚动窗口的 SQL 语法就是把 tumble window 的声明写在了 group by 子句中，即 tumble(row_time, interval ‘1’ minute) 第一个参数为事件时间的时间戳 第二个参数为滚动窗口大小 Windowing TVF（1.14 只支持 Streaming 任务，1.15版本开始支持 Batch\\Streaming 任务） 1234567891011121314SELECT user_id, UNIX_TIMESTAMP(CAST(window_start AS STRING)) * 1000 as window_start, UNIX_TIMESTAMP(CAST(window_end AS STRING)) * 1000 as window_end, window_time, sum(price) as sum_priceFROM TABLE(TUMBLE( TABLE source_table , DESCRIPTOR(row_time) , INTERVAL &#x27;5&#x27; SECOND))GROUP BY window_start, window_end, window_time, user_id; 可以看到 Windowing TVF 滚动窗口的写法就是把 tumble window 的声明写在了数据源的 Table 子句中，即 TABLE(TUMBLE(TABLE source_table, DESCRIPTOR(row_time), INTERVAL ‘60’ SECOND))，包含三部分参数。 第一个参数 TABLE source_table 声明数据源表； 第二个参数 DESCRIPTOR(row_time) 声明数据源的时间戳； 第三个参数 INTERVAL ‘60’ SECOND 声明滚动窗口大小为 1 min。 滑动窗口(SlidingTimeWindow) 滑动窗口定义：滑动窗口也是将元素指定给固定长度的窗口。与滚动窗口功能一样，也有窗口大小的概念。不一样的地方在于，滑动窗口有另一个参数控制窗口计算的频率（滑动窗口滑动的步长）。因此，如果滑动的步长小于窗口大小，则滑动窗口之间每个窗口是可以重叠。在这种情况下，一条数据就会分配到多个窗口当中。举例，有 10 分钟大小的窗口，滑动步长为 5 分钟。这样，每 5 分钟会划分一次窗口，这个窗口包含的数据是过去 10 分钟内的数据，如下图所示： 在流模式下，时间属性字段必须是事件或处理时间属性。 应用场景：比如计算同时在线的数据，要求结果的输出频率是 1 分钟一次，每次计算的数据是过去 5 分钟的数据（有的场景下用户可能在线，但是可能会 2 分钟不活跃，但是这也要算在同时在线数据中，所以取最近 5 分钟的数据就能计算进去了） 基于DataStream编程我们使用Time类中的时间单位来定义Slide和Size，也可以设置offset。同样，timeWindow是一种缩写，根据执行环境中设置的时间语义来选择相应的方法初始化窗口。 12345678910111213141516val input: DataStream[T] = ...// sliding event-time windowsinput.keyBy(...).window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))).&lt;window function&gt;(...)// sliding processing-time windowsinput.keyBy(&lt;...&gt;).window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))).&lt;window function&gt;(...)// sliding processing-time windows offset by -8 hoursinput.keyBy(&lt;...&gt;).window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))).&lt;window function&gt;(...) Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package io.github.windows;import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;import org.apache.flink.util.Collector;import org.apache.flink.api.java.tuple.Tuple3;public class SlidingTimeWindowDemo &#123; public static void main(String[] args) throws Exception &#123; // 1, 构建流处理运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(3); // 2, 构建Socket Source DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3, 对数据进行处理并提取时间戳 SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Long&gt;&gt; tuple3DS = streamSource.flatMap(new FlatMapFunction&lt;String, Tuple3&lt;String, Integer, Long&gt;&gt;() &#123; /* out.collect(new Tuple3(value.split(&quot;,&quot;)[0],value.split(&quot;,&quot;)[0],value.split(&quot;,&quot;)[0])) 可以换成String[] strings = value.split(&quot;,&quot;); Tuple3&lt;String,Integer, Long&gt; tuple3 = new Tuple3&lt;泛型就不写嘞&gt;(strings[0], strings[1], strings[2]); out.collect(tuple3); */ @Override public void flatMap(String value, Collector&lt;Tuple3&lt;String, Integer, Long&gt;&gt; out) throws Exception &#123; out.collect(new Tuple3&lt;&gt;( value.split(&quot;,&quot;)[0], Integer.parseInt(value.split(&quot;,&quot;)[1]), Long.parseLong(value.split(&quot;,&quot;)[2]) )); &#125; // 提取字段的时间戳 因为forMonotonousTimestamps是泛型方法, 需要 &#125;); // 4, 提取 输入数据的时间戳 作为事件时间, // 最终通过tuple3.assignTimeStampAndWatermarks(WaterStrategy.forMonotonousTimeStamps().withTimeStampsAssigner)返回一个水印 SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Long&gt;&gt; watermarks = tuple3DS.assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, Integer, Long&gt;&gt;forMonotonousTimestamps() .withTimestampAssigner((Tuple3&lt;String, Integer, Long&gt; element, long recordTimestamp) -&gt; &#123; return element.f2 * 1000; // 如果导入Scala的包, 很有可能会没有f0这个函数 &#125; )); // 5, 进行窗口计算 (如果代码报红, 先去顶部看看哪个包是灰的, 删除, 然后回来就看到提示导包了, 重新导入就行) watermarks .keyBy(t -&gt; t.f0) // 每隔5秒计算当前10s内的数据 // 如果每隔10秒计算当前5s内的数据, 则数据会漏掉 .window(SlidingEventTimeWindows.of(Time.seconds(10),Time.seconds(5))) .sum(1) .print(); // 6, 提交计算任务 env.execute(); &#125;&#125; 基于SQL编程在批处理模式下，窗口表函数的时间属性字段必须是TIMESTAMP或TIMESTAMP _LTZ类型的属性。HOP的返回值是一个新的关系，它包括原始关系的所有列，以及另外3列“window_start”、“window_end”、“window_time”，以指示指定的窗口。原始时间属性“timecol”将是窗口TVF之后的常规时间戳列。HOP接受四个必需参数，一个可选参数：HOP(TABLE data, DESCRIPTOR(timecol), slide, size [, offset ]) data: 是一个表参数，可以是与时间属性列的任何关系 timecol：是一个列描述符，指示数据的哪些时间属性列应映射到跳跃窗口。 slide: 是一个持续时间，指定顺序跳跃窗口开始之间的持续 size: 是指定跳跃窗口宽度的持续时间。 offset: 是一个可选参数，用于指定窗口起始位置的偏移量。 两种Flink SQL实现方式： Group Window Aggregation 方案（支持 Batch\\Streaming 任务）： 123456789101112131415161718192021-- 数据源表Flink SQL&gt; CREATE TABLE source_table ( user_id STRING, price BIGINT, `timestamp` bigint, row_time AS TO_TIMESTAMP(FROM_UNIXTIME(`timestamp`)), watermark for row_time as row_time - interval &#x27;0&#x27; second) WITH ( &#x27;connector&#x27; = &#x27;socket&#x27;, &#x27;hostname&#x27; = &#x27;node1&#x27;, &#x27;port&#x27; = &#x27;9999&#x27;, &#x27;format&#x27; = &#x27;csv&#x27;);-- 数据处理逻辑Flink SQL&gt; SELECT user_id, UNIX_TIMESTAMP(CAST(hop_start(row_time, interval &#x27;5&#x27; SECOND, interval &#x27;10&#x27; SECOND) AS STRING)) * 1000 as window_start, sum(price) as sum_priceFROM source_tableGROUP BY user_id , hop(row_time, interval &#x27;5&#x27; SECOND, interval &#x27;10&#x27; SECOND); 可以看到 Group Window Aggregation 滚动窗口的写法就是把 hop window 的声明写在了 group by 子句中，即 hop(row_time, interval ‘1’ minute, interval ‘5’ minute)。其中： 第一个参数为事件时间的时间戳； 第二个参数为滑动窗口的滑动步长； 第三个参数为滑动窗口大小。 Windowing TVF 方案（1.14只支持 Streaming 任务，1.15版本开始支持 Batch\\Streaming 任务）： 12345678910111213-- 数据处理逻辑Flink SQL&gt; SELECT user_id,UNIX_TIMESTAMP(CAST(window_start AS STRING)) * 1000 as window_start, UNIX_TIMESTAMP(CAST(window_end AS STRING)) * 1000 as window_end, sum(price) as sum_priceFROM TABLE(HOP( TABLE source_table , DESCRIPTOR(row_time) , interval &#x27;5&#x27; SECOND, interval &#x27;10&#x27; SECOND))GROUP BY window_start, window_end, user_id; 在该模式下，窗口大小必须是滑动距离的整数倍 可以看到 Windowing TVF 滚动窗口的写法就是把 hop window 的声明写在了数据源的 Table 子句中，即 TABLE(HOP(TABLE source_table, DESCRIPTOR(row_time), INTERVAL ‘1’ MINUTES, INTERVAL ‘5’ MINUTES))，包含四部分参数： 第一个参数 TABLE source_table 声明数据源表； 第二个参数 DESCRIPTOR(row_time) 声明数据源的时间戳； 第三个参数 INTERVAL ‘1’ MINUTES 声明滚动窗口滑动步长大小为 1 min。 第四个参数 INTERVAL ‘5’ MINUTES 声明滚动窗口大小为 5 min。 会话窗口 Session 窗口定义：Session 时间窗口和滚动、滑动窗口不一样，其没有固定的持续时间，如果在定义的间隔期（Session Gap）内没有新的数据出现，则 Session 就会窗口关闭。 基于DataStream编程Session window的窗口大小，则是由数据本身决定 12345DataStream input = … DataStream result = input .keyBy(&lt;key selector&gt;) .window(EventTimeSessionWindows.withGap(Time.seconds(&lt;seconds&gt;)) .apply(&lt;window function&gt;) // or reduce() or fold() Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package io.github.windows;import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.typeinfo.Types;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows;import org.apache.flink.streaming.api.windowing.time.Time;public class TimeSessionWindowDemo &#123; public static void main(String[] args) throws Exception &#123; // 1, 构建流处理运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 如果不设置并行度,默认为CPU的核数8,不设置并行度会有8个并行计算 env.setParallelism(1); // 2, 从网络端口中获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3, 对数据进行处理并提取时间戳(两步合为一步) SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Long&gt;&gt; watermarks = streamSource.flatMap((FlatMapFunction&lt;String, Tuple3&lt;String, Integer, Long&gt;&gt;) (value, out) -&gt; out.collect(new Tuple3&lt;&gt;( value.split(&quot;,&quot;)[0], Integer.parseInt(value.split(&quot;,&quot;)[1]), Long.parseLong(value.split(&quot;,&quot;)[2])))) .returns(Types.TUPLE(Types.STRING, Types.INT, Types.LONG)) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, Integer, Long&gt;&gt;forMonotonousTimestamps().withTimestampAssigner(new SerializableTimestampAssigner&lt;Tuple3&lt;String, Integer, Long&gt;&gt;() &#123; @Override public long extractTimestamp(Tuple3 element, long recordTimestamp) &#123; return (long) element.f2 * 1000; &#125; &#125;)); // 4, 进行窗口计算 watermarks.keyBy(t -&gt; t.f0) .window(EventTimeSessionWindows.withGap(Time.seconds(3))) .sum(1) .print(); env.execute(); &#125;&#125; 基于SQL编程session(row_time, interval &#39;5&#39; minute) 其中： 第一个参数为事件时间的时间戳； 第二个参数为 Session gap 间隔。 使用标识函数选出窗口的起始时间或者结束时间，窗口的时间属性用于下级Window的聚合。 Flink SQL 不支持 Session 窗口的 Window TVF: 12345678910111213141516171819202122-- 数据源表，用户购买行为记录表Flink SQL&gt; CREATE TABLE source_table ( user_id STRING, price BIGINT, `timestamp` bigint, row_time AS TO_TIMESTAMP(FROM_UNIXTIME(`timestamp`)), watermark for row_time as row_time - interval &#x27;0&#x27; second) WITH ( &#x27;connector&#x27; = &#x27;socket&#x27;, &#x27;hostname&#x27; = &#x27;node1&#x27;, &#x27;port&#x27; = &#x27;9999&#x27;, &#x27;format&#x27; = &#x27;csv&#x27;)-- 数据处理逻辑Flink SQL&gt; SELECT user_id, UNIX_TIMESTAMP(CAST(session_start(row_time, interval &#x27;5&#x27; SECOND) AS STRING)) * 1000 as window_start, sum(price) as sum_priceFROM source_tableGROUP BY user_id , session(row_time, interval &#x27;5&#x27; SECOND) 其中： 第一个参数为事件时间的时间戳； 第二个参数为 Session gap 间隔。 SQL 任务是在整个 Session 窗口结束之后才会把数据输出。Session 窗口即支持 处理时间 也支持 事件时间。但是处理时间只支持在 Streaming 任务中运行，Batch 任务不支持。 Session gap 间隔是5s，实际上是不包含5s，大于5s才会触发计算 渐进窗口渐进式窗口定义：渐进式窗口在其实就是 固定窗口间隔内提前触发的的滚动窗口，其实就是 Tumble Window + early-fire 的一个事件时间的版本。 在流模式下，时间属性字段必须是事件或处理时间属性。在批处理模式下，窗口表函数的时间属性字段必须是TIMESTAMP或TIMESTAMP _LTZ类型的属性。CUMULATE的返回值是一个新的关系，它包括原始关系的所有列，以及另外3列“window_start”、“window_end”、“window_time”，以指示指定的窗口。原始时间属性“timecol”将是窗口TVF之后的常规时间戳列。CUMULATE接受四个必需参数，一个可选参数：CUMULATE(TABLE data, DESCRIPTOR(timecol), step, size) data: 是一个表参数，可以是与时间属性列的任何关系 timecol：是一个列描述符，指示数据的哪些时间属性列应映射到累积窗口。 step: 是指定连续累积窗口结束之间增加的窗口大小的持续时间 size: 是指定累积窗口的最大宽度的持续时间。size必须是 的整数倍step。 offset: 是一个可选参数，用于指定窗口起始位置的偏移量。 渐进式窗口目前只有 Windowing TVF 方案（1.14 只支持 Streaming 任务，1.15版本开始支持 Batch\\Streaming 任务）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344-- 数据源表Flink SQL&gt; CREATE TABLE source_table ( -- 用户 id user_id BIGINT, -- 用户 money BIGINT, -- 事件时间戳 row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)), -- watermark 设置 WATERMARK FOR row_time AS row_time - INTERVAL &#x27;0&#x27; SECOND) WITH ( &#x27;connector&#x27; = &#x27;datagen&#x27;, &#x27;rows-per-second&#x27; = &#x27;10&#x27;, &#x27;fields.user_id.min&#x27; = &#x27;1&#x27;, &#x27;fields.user_id.max&#x27; = &#x27;100000&#x27;, &#x27;fields.money.min&#x27; = &#x27;1&#x27;, &#x27;fields.money.max&#x27; = &#x27;100000&#x27;);-- 数据汇表Flink SQL&gt; CREATE TABLE sink_table ( window_end bigint, window_start bigint, sum_money BIGINT, count_distinct_id bigint) WITH ( &#x27;connector&#x27; = &#x27;print&#x27;);-- 数据处理逻辑Flink SQL&gt; insert into sink_tableSELECT UNIX_TIMESTAMP(CAST(window_end AS STRING)) * 1000 as window_end, window_start, sum(money) as sum_money, count(distinct user_id) as count_distinct_idFROM TABLE(CUMULATE( TABLE source_table , DESCRIPTOR(row_time) , INTERVAL &#x27;60&#x27; SECOND , INTERVAL &#x27;1&#x27; DAY))GROUP BY window_start, window_end 其中包含四部分参数： 第一个参数 TABLE source_table 声明数据源表； 第二个参数 DESCRIPTOR(row_time) 声明数据源的时间戳； 第三个参数 INTERVAL ‘60’ SECOND 声明渐进式窗口触发的渐进步长为 1 min。 第四个参数 INTERVAL ‘1’ DAY 声明整个渐进式窗口的大小为 1 天，到了第二天新开一个窗口重新累计。 计数窗口计数窗口支持滚动计数、滑动计数、全局计数窗口仅DataStreamAPI支持 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package com.itheima.flink.windows;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.GlobalWindows;import org.apache.flink.streaming.api.windowing.triggers.Trigger;import org.apache.flink.streaming.api.windowing.triggers.TriggerResult;import org.apache.flink.streaming.api.windowing.windows.GlobalWindow;import org.apache.flink.util.Collector;public class CountWindow &#123; public static void main(String[] args) throws Exception &#123; // 1、构建流处理运行环境 Configuration conf = new Configuration(); conf.setInteger(&quot;rest.port&quot;, 8081); StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf); env.setParallelism(1); // 2、从网络端口获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3、对数据进行处理 SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Long&gt;&gt; tuple3 = streamSource.flatMap(new FlatMapFunction&lt;String, Tuple3&lt;String, Integer, Long&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple3&lt;String, Integer, Long&gt;&gt; out) throws Exception &#123; String[] strings = value.split(&quot;,&quot;); Tuple3&lt;String, Integer, Long&gt; tuple3 = new Tuple3&lt;&gt;(strings[0], Integer.valueOf(strings[1]), Long.parseLong(strings[2])); out.collect(tuple3); &#125; &#125;); // 4、进行窗口计算 // 滚动计数窗口 tuple3.keyBy(t -&gt; t.f0) // 每10条数据进行一次窗口计算 .countWindow(10) .sum(1) .print(&quot;滚动计数窗口&quot;); // 滑动计数窗口 tuple3.keyBy(t -&gt; t.f0) // 每来5条数据，把当前10条数据作为一个窗口进行计算 .countWindow(10, 5) .sum(1) .print(&quot;滑动计数窗口&quot;); // 全局计数窗口 tuple3.keyBy(t -&gt; t.f0) // 使用全局窗口，必须自行定义触发器才能实现窗口计算 .window(GlobalWindows.create()) .trigger(new Trigger&lt;Tuple3&lt;String, Integer, Long&gt;, GlobalWindow&gt;() &#123; @Override public TriggerResult onElement(Tuple3&lt;String, Integer, Long&gt; element, long timestamp, GlobalWindow window, TriggerContext ctx) throws Exception &#123; return null; &#125; @Override public TriggerResult onProcessingTime(long time, GlobalWindow window, TriggerContext ctx) throws Exception &#123; return null; &#125; @Override public TriggerResult onEventTime(long time, GlobalWindow window, TriggerContext ctx) throws Exception &#123; return null; &#125; @Override public void clear(GlobalWindow window, TriggerContext ctx) throws Exception &#123; &#125; &#125;) .sum(1) .print(); env.execute(); &#125;&#125; 增量窗口函数为了提高实时性，会将到达窗口数据提前进行计算，将计算结果保存至状态中，当窗口达到结束条件时，直接将结果从状态中取出输出。 归约函数：ReduceFunction 聚合函数：AggregateFunction 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.itheima.flink.windows;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.functions.ReduceFunction;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;public class ReduceWindowFunc &#123; public static void main(String[] args) throws Exception &#123; // 1、构建流处理运行环境 Configuration conf = new Configuration(); conf.setInteger(&quot;rest.port&quot;, 8081); StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf); env.setParallelism(1); // 2、从网络端口获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3、对数据进行处理 SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Long&gt;&gt; tuple3 = streamSource.flatMap(new FlatMapFunction&lt;String, Tuple3&lt;String, Integer, Long&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple3&lt;String, Integer, Long&gt;&gt; out) throws Exception &#123; String[] strings = value.split(&quot;,&quot;); Tuple3&lt;String, Integer, Long&gt; tuple3 = new Tuple3&lt;&gt;(strings[0], Integer.valueOf(strings[1]), Long.parseLong(strings[2])); out.collect(tuple3); &#125; &#125;); // 4、进行窗口计算 tuple3.keyBy(t -&gt; t.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .reduce(new ReduceFunction&lt;Tuple3&lt;String, Integer, Long&gt;&gt;() &#123; /** * @param value1 The first value to combine. * @param value2 The second value to combine. */ @Override public Tuple3&lt;String, Integer, Long&gt; reduce(Tuple3&lt;String, Integer, Long&gt; value1, Tuple3&lt;String, Integer, Long&gt; value2) throws Exception &#123; return Tuple3.of(value1.f0, value1.f1 - value2.f1, value2.f2); &#125; &#125;).print(); env.execute(); &#125;&#125; 全量窗口函数全量窗口函数会先将窗口中的数据缓存起来，等到窗口结束要输出结果时再取出全部进行计算。（不常用） WindowFunction-使用apply算子实现 ProcessWindowFunction-使用process算子实现（推荐） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package com.itheima.flink.windows;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;import org.apache.flink.streaming.api.functions.windowing.WindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.table.planner.expressions.In;import org.apache.flink.util.Collector;public class FullWindowFunc &#123; public static void main(String[] args) throws Exception &#123; // 1、构建流处理运行环境 Configuration conf = new Configuration(); conf.setInteger(&quot;rest.port&quot;, 8081); StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf); env.setParallelism(1); // 2、从网络端口获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3、对数据进行处理 SingleOutputStreamOperator&lt;Tuple3&lt;String, Integer, Long&gt;&gt; tuple3 = streamSource.flatMap(new FlatMapFunction&lt;String, Tuple3&lt;String, Integer, Long&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple3&lt;String, Integer, Long&gt;&gt; out) throws Exception &#123; String[] strings = value.split(&quot;,&quot;); Tuple3&lt;String, Integer, Long&gt; tuple3 = new Tuple3&lt;&gt;(strings[0], Integer.valueOf(strings[1]), Long.parseLong(strings[2])); out.collect(tuple3); &#125; &#125;); // 4、进行窗口计算 tuple3.keyBy(t -&gt; t.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .apply(new WindowFunction&lt;Tuple3&lt;String, Integer, Long&gt;, Tuple3&lt;String, Integer, Long&gt;, String, TimeWindow&gt;() &#123; /** * @param key 当前被计算窗口的key * @param window 当前的窗口. * @param input 当前窗口中的数据 * @param out 计算结果的收集器 * @throws Exception */ @Override public void apply(String key, TimeWindow window, Iterable&lt;Tuple3&lt;String, Integer, Long&gt;&gt; input, Collector&lt;Tuple3&lt;String, Integer, Long&gt;&gt; out) throws Exception &#123; System.out.println(&quot;当前被计算窗口的key: &quot; + key); System.out.println(&quot;当前的窗口: &quot; + window.toString());// input.forEach(System.out::println); input.forEach(out::collect); &#125; &#125;).print(); tuple3.keyBy(t -&gt; t.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .process(new ProcessWindowFunction&lt;Tuple3&lt;String, Integer, Long&gt;, Tuple3&lt;String, Integer, Long&gt;, String, TimeWindow&gt;() &#123; /** * @param key The key for which this window is evaluated. * @param context 当前计算窗口的上下文环境 * @param elements The elements in the window being evaluated. * @param out A collector for emitting elements. */ @Override public void process(String key, ProcessWindowFunction&lt;Tuple3&lt;String, Integer, Long&gt;, Tuple3&lt;String, Integer, Long&gt;, String, TimeWindow&gt;.Context context, Iterable&lt;Tuple3&lt;String, Integer, Long&gt;&gt; elements, Collector&lt;Tuple3&lt;String, Integer, Long&gt;&gt; out) throws Exception &#123; System.out.println(&quot;当前被计算窗口的key: &quot; + key); System.out.println(&quot;当前窗口处理时间: &quot; + context.currentProcessingTime()); elements.forEach(out::collect); &#125; &#125;).print(); env.execute(); &#125;&#125; Over WindowsOver 聚合定义（支持 Batch\\Streaming）：可以理解为是一种特殊的滑动窗口聚合函数。 窗口聚合：不在 group by 中的字段，不能直接在 select 中拿到 Over 聚合：能够保留原始字段 Over 聚合的语法： 123456789101112131415SELECT agg_func(agg_col) OVER ( [PARTITION BY col1[, col2, ...]] ORDER BY time_col range_definition), ...FROM ...-- 示例SELECT order_id, order_time, amount, SUM(amount) OVER ( PARTITION BY product ORDER BY order_time RANGE BETWEEN INTERVAL &#x27;1&#x27; HOUR PRECEDING AND CURRENT ROW ) AS one_hour_prod_amount_sumFROM Orders ORDER BY：必须是时间戳列（事件时间、处理时间） PARTITION BY：标识了聚合窗口的聚合粒度，如上述案例是按照 product 进行聚合 range_definition：这个标识聚合窗口的聚合数据范围，在 Flink 中有两种指定数据范围的方式。第一种为 按照行数聚合，第二种为 按照时间区间聚合。","categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"author":"Johnson Liam"},{"title":"【Flink】Flink算子及分区概念","slug":"2023.09.07","date":"2023-09-06T16:00:00.000Z","updated":"2023-09-10T12:33:56.000Z","comments":true,"path":"2023/09/07/2023.09.07/","link":"","permalink":"http://example.com/2023/09/07/2023.09.07/","excerpt":"","text":"粥所周知, Flink程序由四部分构成, 运行环境 + Source + Transformation + Sink. 接下来一个个说. 运行环境批处理 获取批处理执行环境（用于测试&#x2F;生产） 1ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 流处理 获取流式处理执行环境（用于测试&#x2F;生产） 1StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); 流批一体 获取流批一体处理执行环境（用于测试&#x2F;生产） 123final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 设置为批处理模式env.setRuntimeMode(ExecutionMode.BATCH); 本地环境 创建本地执行环境（用于测试） 1StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); 带有WebUI的本地环境 创建带有webui的本地执行环境（用于测试） 123Configuration conf = new Configuration();conf.setInteger(&quot;rest.port&quot;,8081);StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf); 执行模式 从Flink1.12.0版本起，Flink实现了API上的流批统一。DataStreamAPI新增了一个重要特性可以支持不同的执行模式，通过简单的设置就可以让一段Flink程序在流处理和批处理之间切换。这样一来，DataSet API也就没有存在的必要了。执行模式的分类： 流执行（STREAMING）模式。 STREAMING模式是DataStream API最经典的模式，一般用于需要持续实时处理的无界数据流。在默认情况下，程序使用的就是STREAMING模式。 批执行（BATCH）模式。 BATCH模式是专门用于批处理的执行模式，在这种模式下，Flink处理作业的方式类似于MapReduce框架。对于不会持续计算的有界数据，用这种模式处理会更方便。 自动（AUTOMATIC）模式。 在AUTOMATIC模式下，将由程序根据输入数据源是否有界来自动选择执行模式。 执行模式的配置方法（以BATCH为例，默认是STREAMING模式）： 12# 通过命令行配置（在提交作业时，增加execution.runtime-mode参数，进行指定）bin/flink run -Dexecution.runtime-mode=BATCH ... 123// 通过代码配置（在代码中调用setRuntimeMode方法指定）StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment ();env.setRuntimeMode(RuntimeExecutionMode.BATCH); 建议：不要在代码中配置，而是使用命令行。这同设置并行度是类似的在提交作业时指定参数可以更加灵活，同一段应用在程序写好之后，既可以用于批处理，又可以用于流处理而在代码中进行硬编码的方式的可扩展性比较差，一般都不推荐。 输入算子Source输入算子总结Summary 单并行算子如果显式设置&gt;1的并行度，会抛异常 使用env.fromElements()，这种方式也支持Tuple，自定义对象等复合形式 DataStreamSource &lt;String&gt; words &#x3D; env.fromElements(“flink”, “hadoop”, “flink”); 使用env.fromCollection(),这种方式支持多种Collection的具体类型，如List，Set，Queue 非并行的Source(是一个单并行度的source算子)，可以将一个Collection作为参数传入到该方法中，返回一个DataStreamSource。 List &lt;String&gt; dataList &#x3D; Arrays.asList(“a”, “b”, “a”, “c”); 使用env.fromParallelCollection所返回的source算子，是一个多并行度的source算子 1DataStreamSource&lt;LongValue&gt; parallelCollection = env.fromParallelCollection(new LongValueSequenceIterator(1, 100), TypeInformation.of(LongValue.class)).setParallelism(2); 使用env.generateSequence()方法创建基于Sequence的DataStream 并行的Source（并行度也可以通过调用该方法后，再调用setParallelism来设置）通过指定的起始值和结束值来生成数据序列流； DataStreamSource &lt;Long&gt; sequence &#x3D; env.generateSequence(1, 100); 使用env.fromSequence()方法创建基于开始和结束的DataStream final DataStreamSource sequence2 &#x3D; env.fromSequence(1L, 5L); Example1234567891011121314151617181920// 使用env.fromElements()，这种方式也支持Tuple，自定义对象等复合形式DataStreamSource&lt;String&gt; words = env.fromElements(&quot;flink&quot;, &quot;hadoop&quot;, &quot;flink&quot;);// 使用env.fromCollection(),这种方式支持多种Collection的具体类型，如List，Set，Queue// 非并行的Source，可以将一个Collection作为参数传入到该方法中，返回一个DataStreamSource。List&lt;String&gt; dataList = Arrays.asList(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;c&quot;);// fromCollection方法所返回的source算子，是一个单并行度的source算子DataStreamSource&lt;String&gt; fromCollection = env.fromCollection(dataList)/*.setParallelism(5)*/; // 单并行算子如果显式设置&gt;1的并行度，会抛异常// fromParallelCollection所返回的source算子，是一个多并行度的source算子DataStreamSource&lt;LongValue&gt; parallelCollection = env.fromParallelCollection(new LongValueSequenceIterator(1, 100), TypeInformation.of(LongValue.class)).setParallelism(2);// 使用env.generateSequence()方法创建基于Sequence的DataStream// 并行的Source（并行度也可以通过调用该方法后，再调用setParallelism来设置）通过指定的起始值和结束值来生成数据序列流；DataStreamSource&lt;Long&gt; sequence = env.generateSequence(1, 100);sequence.map(x -&gt; x - 1)/*.print()*/;// 使用env.fromSequence()方法创建基于开始和结束的DataStreamfinal DataStreamSource&lt;Long&gt; sequence2 = env.fromSequence(1L, 5L);sequence2.map(x -&gt; x - 1)/*.print()*/; 基于集合的Source123456789101112131415161718192021222324252627282930package io.github.sourceOperator;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import java.util.Arrays;import java.util.List;public class CollectionSource &#123; public static void main(String[] args) throws Exception &#123; // 1, 创建带有WebUi的运行环境 Configuration conf = new Configuration(); conf.setInteger(&quot;rest.port&quot;, 8081); StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf); // 2, 构建source算子从集合中获取数据 List&lt;String&gt; list = Arrays.asList(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;); // 创建单并行度集合数据源 DataStreamSource&lt;String&gt; listDS = env.fromCollection(list); // 3, 处理数据 - 将每个单词都转为大写 listDS.map((MapFunction&lt;String, String&gt;) value -&gt; value.toUpperCase()).print(); // 4, 提交执行 env.execute(); &#125;&#125; 基于Socket的Source 非并行的Source，DataStream API 支持从 Socket 套接字读取数据。只需要指定要从其中读取数据的主机和端口号即可。读取 Socket 套接字的数据源函数定义如下： socketTextStream(hostName, port) socketTextStream(hostName, port, delimiter)：可指定分隔符，默认行分隔符是”\\n” socketTextStream(hostName, port, delimiter, maxRetry)：还可以指定 API 应该尝试获取数据的最大次数，默认最大重新连接次数为0。 基于文件的Source 基于文件的Source，本质上就是使用指定的FileInputFormat组件读取数据，可以指定TextInputFormat、CsvInputFormat、BinaryInputFormat等格式；底层都是ContinuousFileMonitoringFunction，这个类继承了RichSourceFunction，都是非并行的Source； readFile(FileInputFormat inputFormat, String filePath) 方法可以指定读取文件的FileInputFormat 格式，参数FileProcessingMode，可取值： PROCESS_ONCE，只读取文件中的数据一次，读取完成后，程序退出 PROCESS_CONTINUOUSLY，会一直监听指定的文件，文件的内容发生变化后，会将以前的内容和新的内容全部都读取出来，进而造成数据重复读取。 readTextFile(String filePath) 可以从指定的目录或文件读取数据，默认使用的是TextInputFormat格式读取数据，还有一个重载的方法readTextFile(String filePath, String charsetName)可以传入读取文件指定的字符集，默认是UTF-8编码。该方法是一个有限的数据源，数据读完后，程序就会退出，不能一直运行。该方法底层调用的是readFile方法，FileProcessingMode为PROCESS_ONCE readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是前两个方法内部调用的方法。它基于给定的 fileInputFormat 读取路径 path 上的文件。根据提供的 watchType 的不同，source 可能定期（每 interval 毫秒）监控路径上的新数据（watchType 为 FileProcessingMode.PROCESS_CONTINUOUSLY），或者处理一次当前路径中的数据然后退出（watchType 为 FileProcessingMode.PROCESS_ONCE)。使用 pathFilter，用户可以进一步排除正在处理的文件。 第三方 Source(Kafka举例) 在实际生产环境中，为了保证flink可以高效地读取数据源中的数据，通常是跟一些分布式消息中件结合使用，例如Apache Kafka。Kafka的特点是分布式、多副本、高可用、高吞吐、可以记录偏移量等。Flink和Kafka整合可以高效的读取数据，并且可以保证Exactly Once（精确一次性语义）。 添加依赖 12345&lt;dependency&gt;&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&lt;artifactId&gt;flink-connector-kafka&lt;/artifactId&gt;&lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; Kafka Source 提供了构建类来创建 KafkaSource 的实例。构建 KafkaSource 来消费 “input-topic” 最早位点的数据， 使用消费组 “my-group”，并且将 Kafka 消息体反序列化为字符串： 123456789KafkaSource&lt;String&gt; source = KafkaSource.&lt;String&gt;builder() .setBootstrapServers(brokers) .setTopics(&quot;input-topic&quot;) .setGroupId(&quot;my-group&quot;) .setStartingOffsets(OffsetsInitializer.earliest()) .setValueOnlyDeserializer(new SimpleStringSchema()) .build();env.fromSource(source, WatermarkStrategy.noWatermarks(), &quot;Kafka Source&quot;); 以下属性在构建 KafkaSource 时是必须指定的： Bootstrap server，通过 setBootstrapServers(String) 方法配置 消费者组 ID，通过 setGroupId(String) 配置 要订阅的 Topic &#x2F; Partition， 用于解析 Kafka 消息的反序列化器（Deserializer） Topic &#x2F; Partition订阅 123456789// Topic 列表，订阅 Topic 列表中所有 Partition 的消息：KafkaSource.builder().setTopics(&quot;topic-a&quot;, &quot;topic-b&quot;);// 正则表达式匹配，订阅与正则表达式所匹配的 Topic 下的所有 Partition：KafkaSource.builder().setTopicPattern(&quot;topic.*&quot;);// Partition 列表，订阅指定的 Partition：final HashSet&lt;TopicPartition&gt; partitionSet = new HashSet&lt;&gt;(Arrays.asList( new TopicPartition(&quot;topic-a&quot;, 0), // Partition 0 of topic &quot;topic-a&quot; new TopicPartition(&quot;topic-b&quot;, 5))); // Partition 5 of topic &quot;topic-b&quot;KafkaSource.builder().setPartitions(partitionSet); 消费解析 1234// 代码中需要提供一个反序列化器（Deserializer）来对Kafka的消息进行解析。 反序列化器通过setDeserializer(KafkaRecordDeserializationSchema)来指定，其中 KafkaRecordDeserializationSchema 定义了如何解析Kafka的ConsumerRecord。// 如果只需要 Kafka 消息中的消息体（value）部分的数据，可以使用 KafkaSource 构建类中的 setValueOnlyDeserializer(DeserializationSchema) 方法，其中 DeserializationSchema 定义了如何解析 Kafka 消息体中的二进制数据。// 也可使用 Kafka 提供的解析器 来解析 Kafka 消息体。例如使用 StringDeserializer 来将 Kafka 消息体解析成字符串：KafkaSource.&lt;String&gt;builder() .setDeserializer(KafkaRecordDeserializationSchema.valueOnly(StringDeserializer.class)); 起始偏移量 1234567891011KafkaSource.builder() // 从消费组提交的位点开始消费，不指定位点重置策略 .setStartingOffsets(OffsetsInitializer.committedOffsets()) // 从消费组提交的位点开始消费，如果提交位点不存在，使用最早位点 .setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy.EARLIEST)) // 从时间戳大于等于指定时间的数据开始消费 .setStartingOffsets(OffsetsInitializer.timestamp(1592323200L)) // 从最早位点开始消费 .setStartingOffsets(OffsetsInitializer.earliest()) // 从最末尾位点开始消费 .setStartingOffsets(OffsetsInitializer.latest()); 动态分区检查 1234// 为了在不重启 Flink 作业的情况下处理 Topic 扩容或新建 Topic 等场景，可以将 Kafka Source 配置为在提供的 Topic / Partition 订阅模式下定期检查新分区。要启用动态分区检查，请将 partition.discovery.interval.ms 设置为非负值：// 分区检查功能默认不开启。需要显式地设置分区检查间隔才能启用此功能。KafkaSource.builder().setProperty(&quot;partition.discovery.interval.ms&quot;, &quot;10000&quot;); // 每 10 秒检查一次新分区 CodeDemo123456789101112131415161718192021222324252627282930313233343536373839package io.github.sourceOperator;import javafx.beans.property.SimpleStringProperty;import org.apache.flink.api.common.eventtime.WatermarkStrategy;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.configuration.Configuration;import org.apache.flink.connector.kafka.source.KafkaSource;import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.parquet.format.OffsetIndex;public class KafkaSourceDemo &#123; public static void main(String[] args) throws Exception &#123; // 1, 构建带有WebUI的运行环境 Configuration conf = new Configuration(); conf.setInteger(&quot;rest.port&quot;, 8081); StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf); // 2, 构建Kafka数据源Source KafkaSource&lt;String&gt; kafkaSource = KafkaSource.&lt;String&gt;builder() .setBootstrapServers(&quot;node1:9092&quot;) .setTopics(&quot;test&quot;) .setGroupId(&quot;test1&quot;) .setStartingOffsets(OffsetsInitializer.earliest()) .setValueOnlyDeserializer(new SimpleStringSchema()) .setProperty(&quot;auto.offset.commit&quot;, &quot;true&quot;) .build(); DataStreamSource&lt;String&gt; kafkaDS = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), &quot;kafkaSource&quot;); // 3, 处理数据 kafkaDS.print(); // 4, 提交执行 env.execute(); &#125;&#125; 自定义Source(MySQL举例)123456// 可以实现 SourceFunction 或者 RichSourceFunction , 这两者都是非并行的source算子// 也可实现 ParallelSourceFunction 或者 RichParallelSourceFunction , 这两者都是可并行的source算子// 带 Rich的，都拥有 open() ,close() ,getRuntimeContext() 方法// 带 Parallel的，都可多实例并行执行// 都需要实现的方法：run()，作为数据源，所有数据的产生都在 run() 方法中实现 上面已经使用了自定义数据源和Flink自带的Kafka source，那么接下来就模仿着写一个从 MySQL 中读取数据的 Source。 建表语句 12345678910111213141516171819create database if not exists flinkdemo;use flinkdemo;DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(11) NOT NULL, `username` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `password` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;-- ------------------------------ Records of user-- ----------------------------INSERT INTO `user` VALUES (10, &#x27;dazhuang&#x27;, &#x27;123456&#x27;, &#x27;大壮&#x27;);INSERT INTO `user` VALUES (11, &#x27;erya&#x27;, &#x27;123456&#x27;, &#x27;二丫&#x27;);INSERT INTO `user` VALUES (12, &#x27;sanpang&#x27;, &#x27;123456&#x27;, &#x27;三胖&#x27;);SET FOREIGN_KEY_CHECKS = 1; 添加依赖 123456&lt;!-- 指定mysql-connector的依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.15&lt;/version&gt;&lt;/dependency&gt; CodeDemo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103package io.github.sourceOperator;import com.alibaba.fastjson2.JSON;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;public class CustomSource &#123; public static void main(String[] args) throws Exception &#123; /** * 自定义source读取MySQL中的数据 */ // 1,创建Flink运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度 env.setParallelism(1); // 2, 创建Source算子 DataStreamSource&lt;UserInfo&gt; dataStreamSource = env.addSource(new RichParallelSourceFunction&lt;UserInfo&gt;() &#123; /** * 重写RichParallelSourceFunction中的 open, close方法 * @param parameters * @throws Exception */ private Connection connection = null; // 定义数据库连接对象 private PreparedStatement stat = null; // 定义preparedStatement对象 @Override public void open(Configuration parameters) throws Exception &#123; /** * 使用open方法, 这个方法在实例化类的时候会执行一次, 比较适合用来做数据库连接 * open需要 1, 加载数据库驱动 2, 创建数据库连接 3, 准备preparedStatement对象 */ super.open(parameters); // 1, 加载数据库驱动 Class.forName(&quot;com.mysql.jdbc.Driver&quot;); // 2, 创建数据库连接 String url = &quot;jdbc:mysql://node1:3306/flinkdemo?serverTimezone=UTC&amp;useSSL=false&quot;; this.connection = DriverManager.getConnection(url, &quot;root&quot;, &quot;123456&quot;); // 3, 创建预编译平台对象 String sql = &quot;select id,username,password,name from user;&quot;; this.stat = connection.prepareStatement(sql); &#125; @Override public void close() throws Exception &#123; /** * 使用close方法, 这个方法在销毁实例的时候会执行一次, 比较实用用来关闭连接 */ super.close(); if (this.stat != null) this.stat.close(); if (this.connection != null) this.connection.close(); &#125; @Override public void run(SourceContext&lt;UserInfo&gt; ctx) throws Exception &#123; // 执行查询SQL语句, 获取结果集 ResultSet resultSet = stat.executeQuery(); while (resultSet.next())&#123; ctx.collect(new UserInfo( resultSet.getString(&quot;id&quot;), resultSet.getString(&quot;userName&quot;), resultSet.getString(&quot;passWord&quot;), resultSet.getString(&quot;name&quot;) )); &#125; &#125; @Override public void cancel() &#123; &#125; &#125;).setParallelism(2); // 3,处理数据 - 将对象转为Json字符串并打印 dataStreamSource.map(JSON::toJSONString).print(); // 4, 提交执行 env.execute(); &#125; @Data @AllArgsConstructor @NoArgsConstructor private static class UserInfo&#123; private String id; private String userName; private String passWord; private String name; &#125;&#125; 转换算子Transformation转换算子 Flink中的算子，是对DataStream进行操作，返回一个新的DataStream的过程。Transformation 过程，是将一个或多个DataStream 转换为新的 DataStream，可以将多个转换组合成复杂的数据流拓扑。 Transformation：指数据转换的各种操作。有 map &#x2F; flatMap &#x2F; filter &#x2F; keyBy &#x2F; reduce &#x2F; fold &#x2F; aggregations &#x2F; window &#x2F; windowAll &#x2F; union &#x2F; window join &#x2F; split &#x2F; select &#x2F; project 等，可以将数据转换计算成你想要的数据。 map映射：DataStream→DataStreamDataStream.map(MapFunction&lt;T, R&gt; mapper) 1234567891011121314151617181920/** * @param &lt;T&gt; 输入元素的数据类型. * @param &lt;O&gt; 输出元素的数据类型. */@Public@FunctionalInterfacepublic interface MapFunction&lt;T, O&gt; extends Function, Serializable &#123; /** * The mapping method. Takes an element from the input data set and transforms it into exactly * one element. * * @param 输入元素的数据类型. * @return 输出元素的数据类型. * @throws Exception This method may throw exceptions. Throwing an exception will cause the * operation to fail and may trigger recovery. */ O map(T value) throws Exception;&#125; flatmap扁平化映射：DataStream→DataStreamDataStream.flatMap(FlatMapFunction&lt;T, R&gt; flatMapper) 12345678910111213141516171819/** * @param &lt;T&gt; 输入元素的数据类型. * @param &lt;O&gt; 输出元素的数据类型. */@Public@FunctionalInterfacepublic interface FlatMapFunction&lt;T, O&gt; extends Function, Serializable &#123; /** * The core method of the FlatMapFunction. Takes an element from the input data set and * transforms it into zero, one, or more elements. * * @param 输入元素的数据类型. * @param out 返回元素收集器 * @throws Exception This method may throw exceptions. Throwing an exception will cause the * operation to fail and may trigger recovery. */ void flatMap(T value, Collector&lt;O&gt; out) throws Exception;&#125; filter过滤：DataStream→DataStreamDataStream.filter(FilterFunction filter) 123456789101112131415161718192021/** * @param &lt;T&gt; 输入元素的数据类型 */@Public@FunctionalInterfacepublic interface FilterFunction&lt;T&gt; extends Function, Serializable &#123; /** * The filter function that evaluates the predicate. * * &lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt; The system assumes that the function does not modify the * elements on which the predicate is applied. Violating this assumption can lead to incorrect * results. * * @param value 输入元素的数据类型 * @return 返回值如果为True该元素就保留，返回值如果为False该就过滤掉 * @throws Exception This method may throw exceptions. Throwing an exception will cause the * operation to fail and may trigger recovery. */ boolean filter(T value) throws Exception;&#125; keyBy按key分组：DataStream→KeyedStreamDataStream.keyBy(KeySelector&lt;T, K&gt; key) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * @param &lt;IN&gt; 输入元素，并从该元素中抽取key * @param &lt;KEY&gt; key的数据类型. */@Public@FunctionalInterfacepublic interface KeySelector&lt;IN, KEY&gt; extends Function, Serializable &#123; /** * @param value The object to get the key from. * @return The extracted key. * @throws Exception Throwing an exception will cause the execution of the respective task to * fail, and trigger recovery or cancellation of the program. */ KEY getKey(IN value) throws Exception;&#125;/***************** 单个字段keyBy ********************///用字段位置(已过期)wordAndOne.keyBy(0)//用字段表达式wordAndOne.keyBy(v -&gt; v.f0) /***************** 多个字段keyBy ********************/ //用字段位置（已过期）wordAndOne.keyBy(0, 1);//用KeySelectorwordAndOne.keyBy(new KeySelector&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; getKey(Tuple2&lt;String, Integer&gt; value) throws Exception &#123; return Tuple2.of(value.f0, value.f1); &#125;&#125;);// 用lambda简化wordAndOne.keyBy( (KeySelector&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;&gt;) value -&gt; Tuple2.of(value.f0, value.f1)); /***************** POJO方式 ********************/ public class PeopleCount &#123; private String province; private String city; private Integer counts; public PeopleCount() &#123; &#125; //省略其他代码。。。&#125;// 单个字段source.keyBy(a -&gt; a.getProvince());source.keyBy(PeopleCount::getProvince);// 多个字段source.keyBy(new KeySelector&lt;PeopleCount, Tuple2&lt;String, String&gt;&gt;() &#123; @Override public Tuple2&lt;String, String&gt; getKey(PeopleCount value) throws Exception &#123; return Tuple2.of(value.getProvince(), value.getCity()); &#125;&#125;);map.keyBy( (KeySelector&lt;PeopleCount, Tuple2&lt;String, String&gt;&gt;) value -&gt; Tuple2.of(value.getProvince(), value.getCity())); 简单聚合：KeyedStream→DataStream sum()：在输入流上，对指定的字段做叠加求和的操作。 min()：在输入流上，对指定的字段求最小值。 max()：在输入流上，对指定的字段求最大值。 minBy()：与 min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而 minBy()则会返回包含字段最小值的整条数据。 maxBy()：与 max()类似，在输入流上针对指定字段求最大值。两者区别与min()&#x2F;minBy()完全一致。 reduce归约：KeyedStream→DataSreamKeyedStream.reduce(ReduceFunction reducer) 123456789101112131415161718192021/** * @param &lt;T&gt; 输入元素的数据类型. */@Public@FunctionalInterfacepublic interface ReduceFunction&lt;T&gt; extends Function, Serializable &#123; /** * The core method of ReduceFunction, combining two values into one value of the same type. The * reduce function is consecutively applied to all values of a group until only a single value * remains. * * @param value1 The first value to combine. * @param value2 The second value to combine. * @return The combined value of both input values. * @throws Exception This method may throw exceptions. Throwing an exception will cause the * operation to fail and may trigger recovery. */ T reduce(T value1, T value2) throws Exception;&#125; 转换算子Operator12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package io.github.transformation;import io.github.pojo.Order;import org.apache.avro.data.Json;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.functions.ReduceFunction;import org.apache.flink.api.java.tuple.Tuple;import org.apache.flink.api.java.tuple.Tuple4;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import com.alibaba.fastjson2.JSON;import org.apache.flink.streaming.api.functions.ProcessFunction;import org.apache.flink.util.Collector;import scala.math.BigInt;import java.util.Arrays;public class KeyByOperator &#123; public static void main(String[] args) throws Exception &#123; // 1, 构建运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2, 构建source算子获取数据 DataStreamSource&lt;String&gt; streamSource = env.readTextFile(&quot;/Users/Liguibin/Desktop/opt/Java/FlinkCode/src/main/resources/order.csv&quot;); // 3-1, 将获取到的数据封装至pojo中, 输入的数据是String类型, 返回的数据是Order对象 SingleOutputStreamOperator&lt;Order&gt; orderDS = streamSource.flatMap( (FlatMapFunction&lt;String, Order&gt;) (value, out) -&gt; out.collect(new Order( value.split(&quot;,&quot;)[0], BigInt.apply(value.split(&quot;,&quot;)[1]), Double.parseDouble(value.split(&quot;,&quot;)[2]), value.split(&quot;,&quot;)[3]))) .returns(Order.class); // 3-2 将获取到的数据封装至四元组中 /** * 如果将数据切分封装到元组中, 需要重写flatMap方法, out.collect(new Tuple&lt;&gt;(元组中四个元素)) * 如果FLatMap写成为Lambda表达式, 后面必须要跟returns(Types.Tuple(Types.字段类型 ....)) */ SingleOutputStreamOperator&lt;Tuple4&lt;String, BigInt, Double, String&gt;&gt; tuple4DS = streamSource.flatMap(new FlatMapFunction&lt;String, Tuple4&lt;String, BigInt, Double, String&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple4&lt;String, BigInt, Double, String&gt;&gt; out) throws Exception &#123; out.collect(new Tuple4&lt;&gt;( value.split(&quot;,&quot;)[0], BigInt.apply(value.split(&quot;,&quot;)[1]), Double.parseDouble(value.split(&quot;,&quot;)[2]), value.split(&quot;,&quot;)[3])); &#125; &#125;); // 4, 使用keyBy算子分发数据(按照用户的id进行分发) // KeyedStream&lt;Tuple4&lt;String, BigInt, Double, String&gt;, Tuple&gt; keyedStream1 = tupleDS.keyBy(0); // KeyedStream&lt;Tuple4&lt;String, BigInt, Double, String&gt;, String&gt; keyedStream1 = tuple4DS.keyBy(t -&gt; t.f0); // KeyedStream&lt;Order, String&gt; keyedStream1 = orderDS.keyBy(o -&gt; o.getUserId()); KeyedStream&lt;Order, String&gt; keyedStream = orderDS.keyBy(Order::getUserId); // 5-1 sum 求金额求和// keyedStream.sum(&quot;money&quot;).print(&quot;sum求金额总和&quot;); // 5-2 min求每个用户最小的消费金额// keyedStream.minBy(&quot;money&quot;).print(&quot;min求每个用户最小的消费金额&quot;); // 5-3 使用Reduce规约计算 (求金额的最小值 )// keyedStream.reduce(new ReduceFunction&lt;Order&gt;() &#123;// @Override// public Order reduce(Order value1, Order value2) throws Exception &#123;// if (value1.getMoney() &gt; value2.getMoney())return value2;// else return value1;// &#125;// &#125;).print(&quot;使用Reduce规约计算&quot;); // 5-4 使用stream processAPI keyedStream.process(new ProcessFunction&lt;Order, String&gt;() &#123; /** * order 是入参, String 是返回值参数的类型 * @param value * @param ctx 上下文运行环境, Context 背景语境上下文 * @param out * @throws Exception */ @Override public void processElement(Order value, Context ctx, Collector&lt;String&gt; out) throws Exception &#123; System.out.println(ctx.timestamp()); // 没获取到时间后续再说(略,服了 System.out.println(JSON.toJSONString(value)); // 将一个个传入的对象打印为Json字符串 &#125; &#125;); // 6, 提交任务并执行 env.execute(); &#125;&#125; 物理分区GLOBAL分区 GlobalPartitioner 分区器会将上游所有元素都发送到下游的第一个算子实例上(SubTask Id &#x3D; 0)： 1234@PublicEvolvingpublic DataStream&lt;T&gt; global() &#123; return setConnectionType(new GlobalPartitioner&lt;T&gt;());&#125; 12345678910111213141516171819202122232425262728293031323334/** * Partitioner that sends all elements to the downstream operator with subtask ID=0. * 分区器将所有上游元素发送到下游ID=0（第一个）的算子中 * @param &lt;T&gt; Type of the elements in the Stream being partitioned */@Internalpublic class GlobalPartitioner&lt;T&gt; extends StreamPartitioner&lt;T&gt; &#123; private static final long serialVersionUID = 1L; @Override public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123; return 0; &#125; @Override public StreamPartitioner&lt;T&gt; copy() &#123; return this; &#125; @Override public SubtaskStateMapper getDownstreamSubtaskStateMapper() &#123; return SubtaskStateMapper.FIRST; &#125; @Override public boolean isPointwise() &#123; return false; &#125; @Override public String toString() &#123; return &quot;GLOBAL&quot;; &#125;&#125; FORWARD分区 与 GlobalPartitioner 实现相同，但它只会将数据输出到本地运行的下游算子的第一个实例，而非全局。在上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用 RebalancePartitioner，对于ForwardPartitioner，必须保证上下游算子并行度一致，否则会抛出异常。 123456789/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are forwarded to * the local subtask of the next operation. * * @return The DataStream with forward partitioning set. */public DataStream&lt;T&gt; forward() &#123; return setConnectionType(new ForwardPartitioner&lt;T&gt;());&#125; 123456789101112131415161718192021222324252627282930313233343536373839/** * Partitioner that forwards elements only to the locally running downstream operation. * 分区器将元素发送至本地运行的下游算子，上游与下游并行度必须一致 * @param &lt;T&gt; Type of the elements in the Stream */@Internalpublic class ForwardPartitioner&lt;T&gt; extends StreamPartitioner&lt;T&gt; &#123; private static final long serialVersionUID = 1L; @Override public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123; return 0; &#125; public StreamPartitioner&lt;T&gt; copy() &#123; return this; &#125; @Override public boolean isPointwise() &#123; return true; &#125; @Override public String toString() &#123; return &quot;FORWARD&quot;; &#125; @Override public SubtaskStateMapper getDownstreamSubtaskStateMapper() &#123; return SubtaskStateMapper.UNSUPPORTED; &#125; @Override public SubtaskStateMapper getUpstreamSubtaskStateMapper() &#123; return SubtaskStateMapper.UNSUPPORTED; &#125;&#125; BROADCAST分区广播分区将上游数据集输出到下游Operator的每个实例中。适合于大数据集Join小数据集的场景。 123456789/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are broadcasted * to every parallel instance of the next operation. * 将上游所有元素复制多份发送至下游的所有算子中 * @return The DataStream with broadcast partitioning set. */public DataStream&lt;T&gt; broadcast() &#123; return setConnectionType(new BroadcastPartitioner&lt;T&gt;());&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Partitioner that selects all the output channels. * * @param &lt;T&gt; Type of the elements in the Stream being broadcast */@Internalpublic class BroadcastPartitioner&lt;T&gt; extends StreamPartitioner&lt;T&gt; &#123; private static final long serialVersionUID = 1L; /** * Note: Broadcast mode could be handled directly for all the output channels in record writer, * so it is no need to select channels via this method. */ @Override public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123; throw new UnsupportedOperationException( &quot;Broadcast partitioner does not support select channels.&quot;); &#125; @Override public SubtaskStateMapper getUpstreamSubtaskStateMapper() &#123; return SubtaskStateMapper.UNSUPPORTED; &#125; @Override public SubtaskStateMapper getDownstreamSubtaskStateMapper() &#123; return SubtaskStateMapper.UNSUPPORTED; &#125; @Override public boolean isBroadcast() &#123; return true; &#125; @Override public StreamPartitioner&lt;T&gt; copy() &#123; return this; &#125; @Override public boolean isPointwise() &#123; return false; &#125; @Override public String toString() &#123; return &quot;BROADCAST&quot;; &#125;&#125; SHUFFLE分区随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区，因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。 12345678910/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are shuffled * uniformly randomly to the next operation. * 将上游算子中的所有元素随机发送至下游算子中 * @return The DataStream with shuffle partitioning set. */@PublicEvolvingpublic DataStream&lt;T&gt; shuffle() &#123; return setConnectionType(new ShufflePartitioner&lt;T&gt;());&#125; 123456789101112131415161718192021222324252627282930313233343536/** * Partitioner that distributes the data equally by selecting one output channel randomly. * * @param &lt;T&gt; Type of the Tuple */@Internalpublic class ShufflePartitioner&lt;T&gt; extends StreamPartitioner&lt;T&gt; &#123; private static final long serialVersionUID = 1L; private Random random = new Random(); @Override public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123; return random.nextInt(numberOfChannels); &#125; @Override public SubtaskStateMapper getDownstreamSubtaskStateMapper() &#123; return SubtaskStateMapper.ROUND_ROBIN; &#125; @Override public StreamPartitioner&lt;T&gt; copy() &#123; return new ShufflePartitioner&lt;T&gt;(); &#125; @Override public boolean isPointwise() &#123; return false; &#125; @Override public String toString() &#123; return &quot;SHUFFLE&quot;; &#125;&#125; REBALANCE分区 基于上下游算子的并行度，将元素循环的分配到下游算子的某几个实例上。以上游算子并行度为 2，而下游算子并行度为 4 为例，当使用 RebalancePartitioner时，上游每个实例会轮询发给下游的 4 个实例。但是当使用 RescalePartitioner 时，上游每个实例只需轮询发给下游 2 个实例。因为 Channel 个数变少了，Subpartition 的 Buffer 填充速度能变快，能提高网络效率。当上游的数据比较均匀时，且上下游的并发数成比例时，可以使用 RescalePartitioner 替换 RebalancePartitioner。 12345678910111213141516171819202122/** * Sets the partitioning of the &#123;@link DataStream&#125; so that the output elements are distributed * evenly to a subset of instances of the next operation in a round-robin fashion. * * &lt;p&gt;The subset of downstream operations to which the upstream operation sends elements depends * on the degree of parallelism of both the upstream and downstream operation. For example, if * the upstream operation has parallelism 2 and the downstream operation has parallelism 4, then * one upstream operation would distribute elements to two downstream operations while the other * upstream operation would distribute to the other two downstream operations. If, on the other * hand, the downstream operation has parallelism 2 while the upstream operation has parallelism * 4 then two upstream operations will distribute to one downstream operation while the other * two upstream operations will distribute to the other downstream operations. * * &lt;p&gt;In cases where the different parallelisms are not multiples of each other one or several * downstream operations will have a differing number of inputs from upstream operations. * * @return The DataStream with rescale partitioning set. */@PublicEvolvingpublic DataStream&lt;T&gt; rescale() &#123; return setConnectionType(new RescalePartitioner&lt;T&gt;());&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * Partitioner that distributes the data equally by cycling through the output channels. This * distributes only to a subset of downstream nodes because &#123;@link * org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator&#125; instantiates a &#123;@link * DistributionPattern#POINTWISE&#125; distribution pattern when encountering &#123;@code RescalePartitioner&#125;. * * &lt;p&gt;The subset of downstream operations to which the upstream operation sends elements depends on * the degree of parallelism of both the upstream and downstream operation. For example, if the * upstream operation has parallelism 2 and the downstream operation has parallelism 4, then one * upstream operation would distribute elements to two downstream operations while the other * upstream operation would distribute to the other two downstream operations. If, on the other * hand, the downstream operation has parallelism 2 while the upstream operation has parallelism 4 * then two upstream operations will distribute to one downstream operation while the other two * upstream operations will distribute to the other downstream operations. * * &lt;p&gt;In cases where the different parallelisms are not multiples of each other one or several * downstream operations will have a differing number of inputs from upstream operations. * * @param &lt;T&gt; Type of the elements in the Stream being rescaled */@Internalpublic class RescalePartitioner&lt;T&gt; extends StreamPartitioner&lt;T&gt; &#123; private static final long serialVersionUID = 1L; private int nextChannelToSendTo = -1; @Override public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123; if (++nextChannelToSendTo &gt;= numberOfChannels) &#123; nextChannelToSendTo = 0; &#125; return nextChannelToSendTo; &#125; @Override public SubtaskStateMapper getDownstreamSubtaskStateMapper() &#123; return SubtaskStateMapper.UNSUPPORTED; &#125; @Override public SubtaskStateMapper getUpstreamSubtaskStateMapper() &#123; return SubtaskStateMapper.UNSUPPORTED; &#125; public StreamPartitioner&lt;T&gt; copy() &#123; return this; &#125; @Override public String toString() &#123; return &quot;RESCALE&quot;; &#125; @Override public boolean isPointwise() &#123; return true; &#125;&#125; CUSTOM分区自定义实现元素要发送到相对应的下游算子实例上 123456789101112131415161718/** * Partitions a DataStream on the key returned by the selector, using a custom partitioner. This * method takes the key selector to get the key to partition on, and a partitioner that accepts * the key type. * * &lt;p&gt;Note: This method works only on single field keys, i.e. the selector cannot return tuples * of fields. * * @param partitioner The partitioner to assign partitions to keys. * @param keySelector The KeySelector with which the DataStream is partitioned. * @return The partitioned DataStream. * @see KeySelector */public &lt;K&gt; DataStream&lt;T&gt; partitionCustom( Partitioner&lt;K&gt; partitioner, KeySelector&lt;T, K&gt; keySelector) &#123; return setConnectionType( new CustomPartitionerWrapper&lt;&gt;(clean(partitioner), clean(keySelector)));&#125; Demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package io.github.pratitioner;import com.alibaba.fastjson2.JSON;import io.github.pojo.Order;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.functions.Partitioner;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;import scala.math.BigInt;public class CustomPartitioner &#123; public static void main(String[] args) &#123; // 1, 构建Flink运行环境 Configuration conf = new Configuration(); conf.setInteger(&quot;rest.port&quot;, 8081); StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); // 2, 构建source算子获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;node1&quot;, 9999); // 3, 将获取到的数据封装至pojo中(订单实体类) SingleOutputStreamOperator&lt;Order&gt; orderDS = streamSource.flatMap((FlatMapFunction&lt;String, Order&gt;) (value, out) -&gt; out.collect(new Order( value.split(&quot;,&quot;)[0], BigInt.apply(value.split(&quot;,&quot;)[1]), Double.valueOf(value.split(&quot;,&quot;)[2]), value.split(&quot;,&quot;)[3] ))).returns(Order.class).setParallelism(2); // 4, 进行自定义重分区 // partitionCustom自定义分区方法 // 第一个参数 Partitioner：实现分区器，如：将奇数发给0号分区，将偶数发给1号分区 // 第二个参数 KeySelector：指定分区字段 orderDS.partitionCustom(new MyCustomPartitioner(), Order::getUserId) .map(JSON::toJSONString) .setParallelism(3) .print().setParallelism(3); &#125; private static class MyCustomPartitioner implements Partitioner&lt;String&gt; &#123; // 把包含001的userId发送到下游第一个算子中 @Override public int partition(String key, int numPartitions) &#123; if (key.contains(&quot;001&quot;))&#123; System.out.println(key + &quot;:发送到第一个分区&quot;); return 0; // 这里相当于把数据下发到下游第一个算子中了 &#125;else return 1; // 把其他的数据都发送至第二个算子中 &#125; &#125;&#125; 输出算子SinkFileSink场景描述： 大数据业务场景中，经常有一种场景：外部数据发送到kafka中，Flink作为中间件消费kafka数据并进行业务处理；处理完成之后的数据可能还需要写入到数据库或者文件系统中，比如写入hdfs中。 FileSink就可以用来将分区文件写入到支持 Flink FileSystem 接口的文件系统中，支持Exactly-Once语义。 这种sink实现的****Exactly-Once****都是基于Flink checkpoint来实现的两阶段提交模式来保证的，主要应用在实时数仓、topic拆分、基于小时分析处理等场景下。 实现原理： Bucket：FileSink可向由Flink FileSystem抽象支持的文件系统写入分区文件（因为是流式写入，数据被视为无界）。该分区行为可配，默认按时间，具体来说每小时写入一个Bucket，该Bucket包括若干文件，内容是这一小时间隔内流中收到的所有record。 PartFile：每个Bucket内部分为多个PartFile来存储输出数据，该Bucket生命周期内接收到数据的sink的每个子任务至少有一个PartFile。 FileSink 支持行编码（Row-encoded）和批量编码（Bulk-encoded，比如 Parquet）格式。 配置详解： 1-PartFile：每个Bucket内部分为多个部分文件，该Bucket内接收到数据的sink的每个子任务至少有一个PartFile。而额外文件滚动由可配的滚动策略决定。 生命周期：在每个活跃的Bucket期间，每个Writer的子任务在任何时候都只会有一个单独的In-progress PartFile，但可有多个Peding和Finished状态文件。 In-progress ：当前文件正在写入中 Pending ：当处于 In-progress 状态的文件关闭（closed）了，就变为 Pending 状态 Finished ：在成功的 Checkpoint 后，Pending 状态将变为 Finished 状态,处于 Finished 状态的文件不会再被修改，可以被下游系统安全地读取。 命名规则： In-progress &#x2F; Pending：part--.inprogress.uid Finished：part-- 当 Sink Subtask 实例化时，它的 uid 是一个分配给 Subtask 的随机 ID 值。这个 uid 不具有容错机制，所以当 Subtask 从故障恢复时，uid 会重新生成 Flink 允许用户给 Part 文件名添加一个前缀和&#x2F;或后缀。 使用 OutputFileConfig 来完成。 注意：使用 FileSink 时需要启用 Checkpoint ，每次做 Checkpoint 时写入完成。如果 Checkpoint 被禁用，部分文件（part file）将永远处于 ‘in-progress’ 或 ‘pending’ 状态，下游系统无法安全地读取。 2-序列化编码：FileSink 支持行编码格式和批量编码格式（ 比如：Apache Parquet） 。 FileSink.forRowFormat(basePath, rowEncoder) 必须配置项： 输出数据的BasePath 序列化每行数据写入PartFile的Encoder 使用RowFormatBuilder可选配置项： 自定义RollingPolicy：默认使用DefaultRollingPolicy来滚动文件，可自定义bucketCheckInterval 默认1分钟。该值单位为毫秒，指定按时间滚动文件间隔时间 FileSink sink = FileSink .forRowFormat(new Path(outputPath), new SimpleStringEncoder[String](&quot;UTF-8&quot;)) .withRollingPolicy( DefaultRollingPolicy.builder() .withRolloverInterval(Duration.ofMinutes(15)) .withInactivityInterval(Duration.ofMinutes(5)) .withMaxPartSize(MemorySize.ofMebiBytes(1)) .build()) .build() 1 FileSink.forBulkFormat(basePath, bulkWriterFactory) Bulk-encoded 的 Sink 的创建和 Row-encoded 的相似，但不需要指定 Encoder，而是需要指定 BulkWriter.Factory。 BulkWriter 定义了如何添加和刷新新数据以及如何最终确定一批记录使用哪种编码字符集的逻辑。 需要相关依赖 123456789101112131415161718```xml &lt;!-- 应用FileSink功能所需要的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-parquet&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-avro&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.parquet&lt;/groupId&gt; &lt;artifactId&gt;parquet-avro&lt;/artifactId&gt; &lt;version&gt;$&#123;parquet-avro&#125;&lt;/version&gt; &lt;/dependency&gt; 123- 三种实现方式 - 根据schema将数据写成parquet格式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 1. 先定义GenericRecord的数据模式Schema schema = SchemaBuilder.builder() .record(&quot;DataRecord&quot;) .namespace(&quot;cn.itcast.flink.avro.schema&quot;) .fields() .requiredInt(&quot;gid&quot;) .requiredLong(&quot;ts&quot;) .requiredString(&quot;eventId&quot;) .requiredString(&quot;sessionId&quot;) .name(&quot;eventInfo&quot;) .type() .map() .values() .type(&quot;string&quot;) .noDefault() .endRecord(); // 构造好一个数据流DataStreamSource&lt;EventLog&gt; streamSource = env.addSource(new MySourceFunction()); // 2. 通过定义好的schema模式，来得到一个parquetWriterParquetWriterFactory&lt;GenericRecord&gt; writerFactory = AvroParquetWriters.forGenericRecord(schema); // 3. 利用生成好的parquetWriter，来构造一个 支持列式输出parquet文件的 sink算子FileSink&lt;GenericRecord&gt; sink1 = FileSink.forBulkFormat(new Path(&quot;d:/filesink/bulkformat&quot;), writerFactory) .withBucketAssigner(new DateTimeBucketAssigner&lt;GenericRecord&gt;(&quot;yyyy-MM-dd--HH&quot;)) .withRollingPolicy(OnCheckpointRollingPolicy.build()) .withOutputFileConfig(OutputFileConfig.builder().withPartPrefix(&quot;itcast&quot;).withPartSuffix(&quot;.parquet&quot;).build()) .build(); // 4. 将自定义javabean的流，转成 上述sink算子中parquetWriter所需要的 GenericRecord流SingleOutputStreamOperator&lt;GenericRecord&gt; recordStream = streamSource .map((MapFunction&lt;EventLog, GenericRecord&gt;) eventLog -&gt; &#123; // 构造一个Record对象 GenericData.Record record = new GenericData.Record(schema); // 将数据填入record record.put(&quot;gid&quot;, (int) eventLog.getGuid()); record.put(&quot;eventId&quot;, eventLog.getEventId()); record.put(&quot;ts&quot;, eventLog.getTimeStamp()); record.put(&quot;sessionId&quot;, eventLog.getSessionId()); record.put(&quot;eventInfo&quot;, eventLog.getEventInfo()); return record; &#125;).returns(new GenericRecordAvroTypeInfo(schema)); // 由于avro的相关类、对象需要用avro的序列化器，所以需要显式指定AvroTypeInfo来提供AvroSerializer // 5. 输出数据recordStream.sinkTo(sink1).uid(&quot;fileSink&quot;); env.execute(); 12- 传入特定JavaBean类class，它就能通过调用传入的类上的特定方法，来获得Schema对象- 传入普通JavaBean,然后工具可以自己通过反射手段来获取用户的普通JavaBean中的包名、类名、字段名、字段类型等信息，来翻译成一个符合Avro要求的Schema。 3-桶分配策略 FileSink使用BucketAssigner来确定每条输入的数据应该被放入哪个Bucket，默认情况下，DateTimeBucketAssigner 基于系统默认时区每小时创建一个桶。 Flink 有两个内置的 BucketAssigners ： DateTimeBucketAssigner：默认基于时间的分配器 BasePathBucketAssigner：将所有部分文件（part file）存储在基本路径中的分配器（单个全局桶） 4-滚动策略 滚动策略 RollingPolicy 定义了指定的文件在何时关闭（closed）并将其变为 Pending 状态，随后变为 Finished 状态。处于 Pending 状态的文件会在下一次 Checkpoint 时变为 Finished 状态，通过设置 Checkpoint 间隔时间，可以控制部分文件（part file）对下游读取者可用的速度、大小和数量。 Flink 有两个内置的滚动策略： DefaultRollingPolicy OnCheckpointRollingPolicy 注意：使用Bulk Encoding时，文件滚动就只能使用OnCheckpointRollingPolicy的策略，该策略在每次checkpoint时滚动part-file。 文件合并： 在 Flink1.15 之后为了快速滚动，并且避免小文件的操作，添加了 compact 功能，可以在 checkpoint 的时候进行合并。 12345678910FileSink&lt;Integer&gt; fileSink= FileSink.forRowFormat(new Path(path),new SimpleStringEncoder&lt;Integer&gt;()) .enableCompact( FileCompactStrategy.Builder.newBuilder() .setNumCompactThreads(1024) .enableCompactionOnCheckpoint(5) .build(), new RecordWiseFileCompactor&lt;&gt;( new DecoderBasedReader.Factory&lt;&gt;(SimpleStringDecoder::new))) .build(); 参数设置 setNumcCompactThreads：设置合并的线程数 setSizeThreshold：设置大小的阈值（小于这个大小的会被合并） enableCompactionOnCheckpoint：多少个 checkpoint 信号来了，会进行一次 compact 如果开启了 Compaction，那么必须在 source.sinkTo(fileSink)的时候添加 uid：source.sinkTo(fileSink).uid(“fileSink”); KafkaSinkFlink 与 Kafka 的连接器提供了端到端的精确一次（exactly once）语义保证，这在实际项目中是最高级别的一致性保证。 12345678910111213141516171819202122// 1. 构造一个kafka的sink算子KafkaSink&lt;String&gt; kafkaSink = KafkaSink.&lt;String&gt;builder() .setBootstrapServers(&quot;node1.itcast.cn:9092,node2.itcast.cn:9092&quot;) .setRecordSerializer(KafkaRecordSerializationSchema.&lt;String&gt;builder() .setTopic(&quot;event-log&quot;) .setValueSerializationSchema(new SimpleStringSchema()) .build() ) .setDeliverGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .build();// 2. 把数据流输出到构造好的sink算子streamSource .map(JSON::toJSONString).disableChaining() .sinkTo(kafkaSink);env.execute();// 如果使用DeliveryGuarantee.EXACTLY_ONCE 的语义保证，则需要使用 setTransactionalIdPrefix(String)，如：// .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)// .setTransactionalIdPrefix(&quot;itcast-&quot; + RandomUtils.nextInt(1, 100))// .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG , &quot;36000&quot;) JDBCSink 不保证Exactly-Once 1234567891011121314151617181920212223242526272829303132SinkFunction&lt;EventLog&gt; jdbcSink = JdbcSink.sink( &quot;insert into t_eventlog values (?,?,?,?,?) on duplicate key update sessionId=?,eventId=?,ts=?,eventInfo=? &quot;, new JdbcStatementBuilder&lt;EventLog&gt;() &#123; @Override public void accept(PreparedStatement preparedStatement, EventLog eventLog) throws SQLException &#123; preparedStatement.setLong(1, eventLog.getGuid()); preparedStatement.setString(2, eventLog.getSessionId()); preparedStatement.setString(3, eventLog.getEventId()); preparedStatement.setLong(4, eventLog.getTimeStamp()); preparedStatement.setString(5, JSON.toJSONString(eventLog.getEventInfo())); preparedStatement.setString(6, eventLog.getSessionId()); preparedStatement.setString(7, eventLog.getEventId()); preparedStatement.setLong(8, eventLog.getTimeStamp()); preparedStatement.setString(9, JSON.toJSONString(eventLog.getEventInfo())); &#125; &#125;, JdbcExecutionOptions.builder() .withMaxRetries(3) .withBatchSize(1) .build(), new JdbcConnectionOptions.JdbcConnectionOptionsBuilder() .withUrl(&quot;jdbc:mysql://node1:3306/test?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=UTF-8&quot;) .withUsername(&quot;root&quot;) .withPassword(&quot;123456&quot;) .build());// 输出数据streamSource.addSink(jdbcSink);env.execute(); 保证Exactly-Once 12345678910111213141516171819202122232425262728293031323334353637383940414243SinkFunction&lt;EventLog&gt; exactlyOnceSink = JdbcSink.exactlyOnceSink( &quot;insert into t_eventlog values (?,?,?,?,?) on duplicate key update sessionId=?,eventId=?,ts=?,eventInfo=? &quot;, new JdbcStatementBuilder&lt;EventLog&gt;() &#123; @Override public void accept(PreparedStatement preparedStatement, EventLog eventLog) throws SQLException &#123; preparedStatement.setLong(1, eventLog.getGuid()); preparedStatement.setString(2, eventLog.getSessionId()); preparedStatement.setString(3, eventLog.getEventId()); preparedStatement.setLong(4, eventLog.getTimeStamp()); preparedStatement.setString(5, JSON.toJSONString(eventLog.getEventInfo())); preparedStatement.setString(6, eventLog.getSessionId()); preparedStatement.setString(7, eventLog.getEventId()); preparedStatement.setLong(8, eventLog.getTimeStamp()); preparedStatement.setString(9, JSON.toJSONString(eventLog.getEventInfo())); &#125; &#125;, JdbcExecutionOptions.builder() .withMaxRetries(3) .withBatchSize(1) .build(), JdbcExactlyOnceOptions.builder() // mysql不支持同一个连接上存在并行的多个事务，必须把该参数设置为true .withTransactionPerConnection(true) .build(), new SerializableSupplier&lt;XADataSource&gt;() &#123; @Override public XADataSource get() &#123; // XADataSource就是jdbc连接，不过它是支持分布式事务的连接 // 而且它的构造方法，不同的数据库构造方法不同 MysqlXADataSource xaDataSource = new MysqlXADataSource(); xaDataSource.setUrl(&quot;jdbc:mysql://node1:3306/test?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=UTF-8&quot;); xaDataSource.setUser(&quot;root&quot;); xaDataSource.setPassword(&quot;123456&quot;); return xaDataSource; &#125; &#125;);// 输出数据streamSource.addSink(exactlyOnceSink);env.execute();","categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"author":"Johnson Liam"},{"title":"【Flink】Flink作业提交流程及Java编程模型之WordCount","slug":"2023.09.06","date":"2023-09-05T16:00:00.000Z","updated":"2023-09-07T08:37:12.000Z","comments":true,"path":"2023/09/06/2023.09.06/","link":"","permalink":"http://example.com/2023/09/06/2023.09.06/","excerpt":"","text":"作业(Job)提交流程高层级抽象视角 Flink 的提交流程，随着部署模式、资源管理平台的不同，会有不同的变化。首先我们从一个高层级的视角，来做一下抽象提炼，看一看作业提交时宏观上各组件是怎样交互协作的。 具体步骤如下： （1） 一般情况下，由客户端（App）通过分发器提供的 REST 接口，将作业提交给JobManager。 （2）由分发器启动 JobMaster，并将作业（包含 JobGraph）提交给 JobMaster。 （3）JobMaster 将 JobGraph 解析为可执行的 ExecutionGraph，得到所需的资源数量，然后向资源管理器请求资源（slots）。 （4）资源管理器判断当前是否由足够的可用资源；如果没有，启动新的 TaskManager。 （5）TaskManager 启动之后，向 ResourceManager 注册自己的可用任务槽（slots）。 （6）资源管理器通知 TaskManager 为新的作业提供 slots。 （7）TaskManager 连接到对应的 JobMaster，提供 slots。 （8）JobMaster 将需要执行的任务分发给 TaskManager。 （9）TaskManager 执行任务，互相之间可以交换数据。 如果部署模式不同，或者集群环境不同（例如 Standalone、YARN、K8S 等），其中一些步骤可能会不同或被省略，也可能有些组件会运行在同一个 JVM 进程中。比如我们在上一章实践过的独立集群环境的会话模式，就是需要先启动集群，如果资源不够，只能等待资源释放，而不会直接启动新的 TaskManager。 Standalone集群 在独立模式（Standalone）下， TaskManager 都需要手动启动，所以当 ResourceManager 收到 JobMaster 的请求时，会直接要求 TaskManager 提供资源。提交的整体流程如图所示。 我们发现除去第 4 步不会启动 TaskManager，而且直接向已有的 TaskManager 要求资源，其他步骤与上一节所讲抽象流程完全一致。 YARN 集群会话（Session）模式 作业的流程，如上图所示： （1）客户端通过 REST 接口，将作业提交给分发器。 （2）分发器启动 JobMaster，并将作业（包含 JobGraph）提交给 JobMaster。 （3）JobMaster 向资源管理器请求资源（slots）。 （4）资源管理器向 YARN 的资源管理器请求 container 资源。 （5）YARN 启动新的 TaskManager 容器。 （6）TaskManager 启动之后，向 Flink 的资源管理器注册自己的可用任务槽。 （7）资源管理器通知 TaskManager 为新的作业提供 slots。 （8）TaskManager 连接到对应的 JobMaster，提供 slots。 （9）JobMaster 将需要执行的任务分发给 TaskManager，执行任务。 可见，整个流程除了请求资源时要“上报”YARN 的资源管理器，其他与 7.5.1 节所述抽象流程几乎完全一样。 单作业（Per-Job）模式 (1) 客户端将作业提交给 YARN 的资源管理器，这一步中会同时将 Flink 的 Jar 包和配置上传到 HDFS，以便后续启动 Flink 相关组件的容器。 (2) YARN 的资源管理器分配 Container 资源，启动 Flink JobManager，并将作业提交给JobMaster。这里省略了 Dispatcher 组件。 (3) JobMaster 向资源管理器请求资源（slots）。 (4) 资源管理器向 YARN 的资源管理器请求 container 资源。 (5) YARN 启动新的 TaskManager 容器。 (6) TaskManager 启动之后，向 Flink 的资源管理器注册自己的可用任务槽。 (7) 资源管理器通知 TaskManager 为新的作业提供 slots。 (8) TaskManager 连接到对应的 JobMaster，提供 slots。 (9) JobMaster 将需要执行的任务分发给 TaskManager，执行任务。 可见，区别只在于 JobManager 的启动方式，以及省去了分发器。当第 2 步作业提交给JobMaster，之后的流程就与会话模式完全一样了。 应用（Application）模式应用模式与单作业模式的提交流程非常相似，只是初始提交给 YARN 资源管理器的不再是具体的作业，而是整个应用。一个应用中可能包含了多个作业，这些作业都将在 Flink 集群中启动各自对应的 JobMaster。 standalone：集群提前启动，包括JM和TM，而且TM不能动态扩展 yarn session：集群提前启动，只启动JM，而TM根据需要动态启动（由flink的rm向yarn的rm请求资源，并由yarn的rm启动TM） yarn per-job：集群不需要提前启动，客户端直接将作业提交给yarn的rm，由yarn的rm启动flink的JM（包括有JobMaster和rm，不再有分发器） Flink编程模型 Flink 提供了不同的抽象级别以开发流式或批处理应用。 最顶层：SQL&#x2F;Table API 提供了操作关系表、执行SQL语句分析的API库，供我们方便的开发SQL相关程序 中层：流和批处理API层，提供了一系列流和批处理的API和算子供我们对数据进行处理和分析 最底层：运行时层，提供了对Flink底层关键技术的操纵，如对Event、state、time、window等进行精细化控制的操作API Flink编程实现Flink程序构建流程 众嗦粥汁, Flink程序由四部分组成, 分别为运行环境, Source, Transformtion, Sink 四部分组成. 构建flink程序的上下文环境 构建source 处理数据 构建sink Flink工程搭建 构建Maven工程 导入pom依赖 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306&lt;!-- 指定仓库位置，依次为aliyun、apache和cloudera仓库 --&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;aliyun&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;apache&lt;/id&gt; &lt;url&gt;https://repository.apache.org/content/repositories/snapshots/&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;!--版本信息全局变量--&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;flink.version&gt;1.15.2&lt;/flink.version&gt; &lt;hive.version&gt;3.1.2&lt;/hive.version&gt; &lt;hadoop.version&gt;3.3.0&lt;/hadoop.version&gt; &lt;flink-shaded-hadoop.version&gt;3.1.1.7.2.9.0-173-9.0&lt;/flink-shaded-hadoop.version&gt; &lt;mysql.version&gt;5.1.48&lt;/mysql.version&gt; &lt;log4j.version&gt;2.17.1&lt;/log4j.version&gt; &lt;lombok.version&gt;1.18.22&lt;/lombok.version&gt; &lt;kafka.version&gt;3.0.0&lt;/kafka.version&gt; &lt;!-- sdk --&gt; &lt;java.version&gt;11&lt;/java.version&gt; &lt;scala.version&gt;2.12&lt;/scala.version&gt; &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt; &lt;maven.compiler.source&gt;$&#123;java.version&#125;&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;$&#123;java.version&#125;&lt;/maven.compiler.target&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!--Maven插件相关依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Apache Flink 的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 用于通过自定义功能，格式等扩展表生态系统的通用模块--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-cep&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-runtime&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-queryable-state-runtime&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-scala-bridge_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- web ui的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-runtime-web&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- flink连接器--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-csv&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-json&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-parquet&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-files&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-sql-connector-kafka&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-base&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;$&#123;kafka.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- hadoop相关依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-hadoop-compatibility_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-shaded-hadoop-3-uber&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-shaded-hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-1.2-api&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--lombok插件--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;$&#123;lombok.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;sourceDirectory&gt;src/main/java&lt;/sourceDirectory&gt; &lt;plugins&gt; &lt;!-- 编译插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;!--&lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt;--&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.18.1&lt;/version&gt; &lt;configuration&gt; &lt;useFile&gt;false&lt;/useFile&gt; &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt; &lt;includes&gt; &lt;include&gt;**/*Test.*&lt;/include&gt; &lt;include&gt;**/*Suite.*&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 打包插件(会包含所有依赖) --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;!-- zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;!-- 设置jar包的入口类(可选) --&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; Flink批处理实现构建测试数据123456789101112131415161718192021222324# wordcount.txt文件数据Total,time,BUILD,SUCCESSFinal,Memory,Finished,atTotal,time,BUILD,SUCCESSFinal,Memory,Finished,atTotal,time,BUILD,SUCCESSFinal,Memory,Finished,atBUILD,SUCCESSBUILD,SUCCESSBUILD,SUCCESSBUILD,SUCCESSBUILD,SUCCESSBUILD,SUCCESS# order.csv文件数据user_001,1621718199,10.1,电脑user_001,1621718201,14.1,手机user_002,1621718202,82.5,手机user_001,1621718205,15.6,电脑user_004,1621718207,10.2,家电user_001,1621718208,15.8,电脑user_005,1621718212,56.1,电脑user_002,1621718260,40.3,家电user_001,1621718580,11.5,家居user_001,1621718860,61.6,家居 DataStreamAPI实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package io.github.batch;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.operators.*;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.util.Collector;import org.omg.CORBA.Environment;public class WordCountBatchDataStream &#123; public static void main(String[] args) throws Exception &#123; /** * 实现步骤： * 1、初始化flink批处理运行环境 * 2、从指定文件中读取数据 * 3、对获取的数据按指定分隔符切分 * 4、对切分后的每个单词计数1 * 5、对相同单词的进行分组操作 * 6、对分组后的单词进行累加操作 * 7、打印输出 * 8、启动作业、提交任务 */ // 1, 初始化Spark运行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 2, 从指定文件中读取数据 DataSource&lt;String&gt; text = env.readTextFile(&quot;/Users/Liguibin/Desktop/opt/Java/FlinkCode/src/main/resources/wordcount.txt&quot;); // 3, 对获取的数据按指定分隔符切分 FlatMapOperator&lt;String, String&gt; wordDS = text.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; /** * 按照逗号进行切割 * @param value 输入的字符串 * @param out 返回结果的收集器 * @throws Exception */ @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; // 将输入的value按照&#x27;,&#x27;进行切割 String[] strings = value.split(&quot;,&quot;); // 使用增强for循环遍历数组(快捷键 foreach for (String str : strings) &#123; out.collect(str); &#125; &#125; &#125;); // 4, 对切分后的每个单词计数 1, 注意Flink提供了一种类似于map的数据形式Tuple2, 二元组形式. MapOperator&lt;String, Tuple2&lt;String,Integer&gt;&gt; mapDs = wordDS.map(new MapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123; /** * * @param value 输入的字符串 * @return (word, 1) * @throws Exception */ @Override public Tuple2&lt;String,Integer&gt; map(String value) throws Exception &#123; return Tuple2.of(value, 1); &#125; &#125;); // 5, 对相同的单词进行分组操作 UnsortedGrouping&lt;Tuple2&lt;String,Integer&gt;&gt; groupBy = mapDs.groupBy(0); // 6, 对分组后的单词进行累加操作 AggregateOperator&lt;Tuple2&lt;String,Integer&gt;&gt; sumDs = groupBy.sum(1); // 7, 打印输出 sumDs.print(); &#125;&#125; TableAPI123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package io.github.batch;import org.apache.flink.table.api.*;import org.omg.CORBA.Environment;import javax.swing.text.TableView;import static org.apache.flink.table.api.Expressions.$;public class WordCountBatchTable &#123; public static void main(String[] args) &#123; /** * 1、构建flink table批处理运行环境 * 2、创建Source Table获取数据 * 3、对获取到的数据进行计算 * 4、创建Sink Table用于输出数据 * 5、将计算结果写入到Sink Table */ // 1, 构建Flink批处理运行环境 EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build(); TableEnvironment tEnv = TableEnvironment.create(settings); // 2, 创建Source Table获取数据 tEnv.createTemporaryTable(&quot;sourceTable&quot;, TableDescriptor.forConnector(&quot;filesystem&quot;) .schema(Schema.newBuilder() .column(&quot;userId&quot;, DataTypes.STRING()) .column(&quot;timestamp&quot;,DataTypes.BIGINT()) .column(&quot;money&quot;, DataTypes.DOUBLE()) .column(&quot;category&quot;, DataTypes.STRING()) .build()) .option(&quot;path&quot;, &quot;/Users/Liguibin/Desktop/opt/Java/FlinkCode/src/main/resources/order.csv&quot;) .option(&quot;format&quot;, &quot;csv&quot;) .build()); // 3, 对获取到的数据进行计算 Table result = tEnv.from(&quot;sourceTable&quot;) .groupBy($(&quot;userId&quot;)) .select($(&quot;userId&quot;), $(&quot;money&quot;).sum().as(&quot;totalMoney&quot;)); //4, 创建Sink Table用于输出数据 // 结果表与输出表的schema必须一致(列的数量和列的数据类型) tEnv.createTemporaryTable(&quot;sinkTable&quot;, TableDescriptor.forConnector(&quot;print&quot;) .schema(Schema.newBuilder() .column(&quot;userId&quot;, DataTypes.STRING()) .column(&quot;totalMoney&quot;, DataTypes.DOUBLE()) .build()) .build()); // 5, 将计算结果写入到SinkTable result.executeInsert(&quot;sinkTable&quot;); &#125;&#125; SQLAPI1234567891011121314151617181920212223242526272829303132333435363738394041package io.github.batch;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.TableEnvironment;public class WordCountSql &#123; public static void main(String[] args) &#123; // 1, 构建Flink Table运行环境 EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build(); TableEnvironment tEnv = TableEnvironment.create(settings); //2, 创建Source Table String DDLSource = &quot;create table sourceTable (\\n&quot; + &quot; userId STRING,\\n&quot; + &quot; `timestamp` BIGINT,\\n&quot; + &quot; money DOUBLE,\\n&quot; + &quot; category STRING\\n&quot; + &quot; ) WITH (\\n&quot; + &quot; &#x27;connector&#x27; = &#x27;filesystem&#x27;,\\n&quot; + &quot; &#x27;path&#x27; = &#x27;/Users/Liguibin/Desktop/opt/Java/FlinkCode/src/main/resources/order.csv&#x27;,\\n&quot; + &quot; &#x27;format&#x27; = &#x27;csv&#x27;\\n&quot; + &quot;);&quot;; tEnv.executeSql(DDLSource); // 3, 创建Sink Table String DDLSink = &quot;create table sinkTable(\\n&quot; + &quot; userId STRING,\\n&quot; + &quot; totalMoney DOUBLE\\n&quot; + &quot; )WITH (\\n&quot; + &quot; &#x27;connector&#x27; = &#x27;print&#x27;\\n&quot; + &quot;);&quot;; tEnv.executeSql(DDLSink); // 4, 计算结果并写入输出表 String DMLResult = &quot;insert into sinkTable select userId, SUM(money) as totalMoney from sourceTable group by userId;&quot;; tEnv.executeSql(DMLResult); &#125;&#125; Flink流处理实现DataStreamAPI12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package io.github.stream;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.KeyedStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;import java.util.Arrays;public class WordCountDataStream &#123; public static void main(String[] args) throws Exception &#123; /** * 1、构建flink流处理的运行环境 * 2、从网络端口中获取数据 * 3、对数据按分隔符切分 * 4、对切分后的单词计数 1 * 5、根据单词进行分组 * 6、对分组之后的单词进行累加计数 * 7、将计算结果进行输出 * 8、提交任务并执行 */ // 1, 构建Flink流处理的运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2, 从网络端口中获取数据 DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(&quot;192.168.88.161&quot;, 9999); // 3, 对数据按分隔符切分 SingleOutputStreamOperator&lt;String&gt; wordOp = streamSource.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; String[] strings = value.split(&quot; &quot;); Arrays.stream(strings).forEach(out::collect); &#125; &#125;); // 4, 对切分后的单词计数1 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordAndOneOp = wordOp.map(new MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; map(String value) throws Exception &#123; return Tuple2.of(value, 1); &#125; &#125;); // 5, 根据单词进行分词 KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyedStream = wordAndOneOp.keyBy(t -&gt; t.f0); // 6, 对分组之后的单词进行累加计数 1-&gt; 代表二元组中的第二个元素的下标 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; summed = keyedStream.sum(1); // 7, 将计算结果进行输出 summed.print(); // 8, 提交任务并执行 env.execute(); &#125;&#125; TableAPI12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package io.github.stream;import org.apache.flink.connector.datagen.table.DataGenConnectorOptions;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.DataTypes;import org.apache.flink.table.api.Schema;import org.apache.flink.table.api.Table;import org.apache.flink.table.api.TableDescriptor;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;import static org.apache.flink.table.api.Expressions.$;public class WordCountTableAPI &#123; public static void main(String[] args) &#123; /** * 1、构建flink table流处理运行环境 * 2、创建source表 * 3、对source表中的数据进行计算 * 4、创建sink表 * 5、将计算结果写入到sink表中,提交任务并行 */ // 1, 构建Flink table流处理运行环境 StreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度 sEnv.setParallelism(2); // 创建表的运行环境 StreamTableEnvironment tEnv = StreamTableEnvironment.create(sEnv); // 2, 创建Source表 (表的数据类似于Spark的rate, 自动产出数据源, option里面指定的数据源的产生策略) tEnv.createTemporaryTable(&quot;sourceTable&quot;, TableDescriptor.forConnector(&quot;datagen&quot;) .schema(Schema.newBuilder() .column(&quot;word&quot;, DataTypes.STRING()) .column(&quot;frequency&quot;, DataTypes.BIGINT()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L) .option(&quot;fields.word.kind&quot;, &quot;random&quot;) .option(&quot;fields.word.length&quot;, &quot;1&quot;) .option(&quot;fields.frequency.min&quot;, &quot;1&quot;) .option(&quot;fields.frequency.max&quot;, &quot;9&quot;) .build()); // 3, 对Source表中的数据进行计算, 最终得到res结果表 Table res = tEnv.from(&quot;sourceTable&quot;).groupBy($(&quot;word&quot;)) .select($(&quot;word&quot;), $(&quot;frequency&quot;).sum().as(&quot;frequency&quot;)); // 4, 创建Sink表 tEnv.createTemporaryTable(&quot;sinkTable&quot;, TableDescriptor.forConnector(&quot;print&quot;) .schema(Schema.newBuilder() .column(&quot;word&quot;, DataTypes.STRING()) .column(&quot;frequency&quot;, DataTypes.BIGINT()) .build()) .build()); // 5, 将计算结果写入到sink表中 res.executeInsert(&quot;sinkTable&quot;); &#125;&#125; SQLAPI12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package io.github.stream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;public class WordCountSql &#123; public static void main(String[] args) &#123; /** * 1、构建flink table流处理运行环境 * 2、创建source table * 3、创建sink table * 4、执行计算结果写入输出表 */ // 1, 构建Flink Table 流处理运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); // 2, 创建SourceTable(表的数据是通过datagen随机生成的) String DDLSource = &quot;create table sourceTable(\\n&quot; + &quot; `word` STRING,\\n&quot; + &quot; frequency BIGINT\\n&quot; + &quot;) WITH (\\n&quot; + &quot; &#x27;connector&#x27; = &#x27;datagen&#x27;,\\n&quot; + &quot; &#x27;rows-per-second&#x27; = &#x27;1&#x27;,\\n&quot; + &quot; &#x27;fields.word.kind&#x27; = &#x27;random&#x27;,\\n&quot; + &quot; &#x27;fields.word.length&#x27; = &#x27;1&#x27;,\\n&quot; + &quot; &#x27;fields.frequency.min&#x27; = &#x27;1&#x27;,\\n&quot; + &quot; &#x27;fields.frequency.max&#x27; = &#x27;2&#x27;\\n&quot; + &quot;);\\n&quot;; // 执行创表Sql语句 tEnv.executeSql(DDLSource); // 3, 创建sinkTable String DDLSink = &quot;create table sinkTable(\\n&quot; + &quot; `word` STRING,\\n&quot; + &quot; frequency BIGINT\\n&quot; + &quot;) WITH (\\n&quot; + &quot; &#x27;connector&#x27; = &#x27;print&#x27;\\n&quot; + &quot;);&quot;; tEnv.executeSql(DDLSink); // 4, 执行计算 并将结果写入输出表 String DMLComputeSql = &quot;INSERT INTO sinkTable select `word` , SUM(frequency) as frequency from sourceTable group by `word`;&quot;; tEnv.executeSql(DMLComputeSql); &#125;&#125; Lambda表达式处理DataStreamAPI123456789101112131415161718192021222324252627282930313233343536package io.github.stream;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.typeinfo.Types;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;import java.lang.reflect.Array;import java.util.Arrays;import java.util.function.Function;public class WordCountLambda &#123; public static void main(String[] args) throws Exception &#123; // 1, 构建Flink流处理运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 2, 从网络端口中获取数据 // 先用flatMap函数切分, 让系统提示自动转为Lambda表达式 , // 且每次使用过Lambda后都要通过returns明确返回值的类型. // 通过使用Lambda表达式按照 t(word) 的 0号元素进行分组 // 使用sum算子, 按照元组的1号元素进行求和 env.socketTextStream(&quot;node1&quot;, 9999) .flatMap((FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;) (value, out) -&gt; Arrays.stream(value.split(&quot; &quot;)) .map(word -&gt; Tuple2.of(word,1)) .forEach(out::collect)) .returns(Types.TUPLE(Types.STRING, Types.INT)) .keyBy(t -&gt; t.f0) .sum(1) .print(); // 3, 提交任务并执行 env.execute(); &#125;&#125; 提交Flink任务到服务器Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package io.github.submit;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.typeinfo.Types;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;import java.util.Arrays;import java.util.function.Function;public class WordCount &#123; public static void main(String[] args) throws Exception &#123; // 1, 判断入参设置输出结果路径 ParameterTool parameterTool = ParameterTool.fromArgs(args); String outPut = &quot;&quot;; // 计算结果的输出路径 if(parameterTool.has(&quot;output&quot;))&#123; outPut = parameterTool.get(&quot;output&quot;); &#125;else &#123; outPut = &quot;hdfs://node1:8020/wordcount/output01_&quot;; &#125; // 2, 构建Flink运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 3, 创建Source获取数据 DataStreamSource&lt;String&gt; streamSource = env.fromElements(&quot;hadoop spark flink&quot;, &quot;hadoop spark sqoop&quot;); // 4, 计算数据 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; res = streamSource.flatMap((FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;) (value, out) -&gt; &#123; Arrays.stream(value.split(&quot; &quot;)) .map((Function&lt;String, Tuple2&gt;) s -&gt; Tuple2.of(s, 1)) .forEach(out::collect); &#125;).returns(Types.TUPLE(Types.STRING, Types.INT)) .keyBy(s -&gt; s.f0) .sum(1); // 5, 将计算结果输出至指定路径 (如果执行报hdfs 权限相关错误, 可以执行hadoop fs -chmod -R 777) System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;); // 设置用户名 // 将结果输出, 但是writerAsText已经被废弃, 后续可能会用到自定义sink的方式将数据写出 res.writeAsText(outPut + System.currentTimeMillis()).setParallelism(1); // 6, 提交任务并执行 env.execute(); &#125;&#125; ProcessMaven打包 这里使用了打包插件(Shade), 又是一个小细节. 提交到8081端口 拷贝主程序入口(路径) 在Linux提交Flink任务 Yarn三种模式下的任务提交 还有一种是StandAlone下的任务提交, 这里就由于不太清楚就不多介绍了. Yarn的三种模式Session, Per-job, Application三种模式. YARN Session提交任务前提 这种模式下会启动yarn session，并且会启动Flink的两个必要服务：JobManager和Task-managers，然后你可以向集群提交作业。同一个Session中可以提交多个Flink作业。需要注意的是，这种模式下Hadoop的版本至少是2.2，而且必须安装了HDFS（因为启动YARN session的时候会向HDFS上提交相关的jar文件和配置文件） 启动YarnSession和提交任务方法1234# 启动yarn session : 通过./bin/yarn-session.sh脚本启动YARN Sessionyarn-session.sh -tm 1024 -s 4 -d# 提交任务flink run -p 8 /export/server/flink/examples/batch/WordCount.jar 启动YARN Session报错一： 解决办法： 缺失依赖：flink-shaded-hadoop-3-uber-3.1.1.7.2.9.0-173-9.0.jar，将之上传至flink根目录下的lib目录下 启动YARN Session报错二： 解决办法： 缺失依赖：commons-cli-1.5.0.jar，将之上传至flink根目录下的lib目录下 相关参数1234567-n(--container)：TaskManager的数量。(1.10 已经废弃)-s(--slots)： 每个TaskManager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1，有时可以多一些taskmanager，做冗余。-jm：JobManager的内存（单位MB)。-q：显示可用的YARN资源（内存，内核）;-tm：每个TaskManager容器的内存（默认值：MB）-nm：yarn 的appName(现在yarn的ui上的名字)。 -d：后台执行。 停止当前任务123456# 第一种推荐echo &quot;stop&quot; | yarn-session.sh -id application_1662342426082_0001# 第二种不推荐yarn application -kill application_1662342426082_0001# 会话模式将在/tmp中创建一个隐藏的 YARN 属性文件，提交作业时，命令行界面将选取该文件以进行群集发现。/tmp/.yarn-properties-&lt;username&gt;。# 采用第一种方式，会自动删除该文件，如果采用kill直接杀掉该任务，不会删除该隐藏文件。 停止任务后再次提交任务报错 当前使用yarn application -kill方式停止yarn-session集群后，再次使用yarn standalone集群提交任务会报错 解决办法： 需要手动删除yarn-session运行时留下的临时文件 YARN Per-Job提交任务 上面的YARN session是在Hadoop YARN环境下启动一个Flink cluster集群，里面的资源是可以共享给其他的Flink作业。我们还可以在YARN上启动一个Flink作业，这里我们还是使用.&#x2F;bin&#x2F;flink，但是不需要事先启动YARN session： 使用flink直接提交任务1flink run -m yarn-cluster /export/server/flink/examples/batch/WordCount.jar 常用参数：12345678910--p 程序默认并行度下面的参数仅可用于 -m yarn-cluster 模式--yjm JobManager可用内存，单位兆--ynm YARN程序的名称--yq 查询YARN可用的资源--yqu 指定YARN队列是哪一个--ys 每个TM会有多少个Slot--ytm 每个TM所在的Container可申请多少内存，单位兆--yD 动态指定Flink参数--yd 分离模式（后台运行，不指定-yd, 终端会卡在提交的页面上） 停止yarn-cluster123456789101112# 该模式正常状态，完成任务后会自动关闭集群# 手动关闭yarn application -kill application的ID# 注意：# 在创建集群的时候，集群的配置参数就写好了，但是往往因为业务需要，要更改一些配置参数，这个时候可以不必因为一个实例的提交而修改conf/flink-conf.yaml;# 可以通过：-yD &lt;arg&gt; Dynamic properties# 来覆盖原有的配置信息：比如：flink run \\-m yarn-cluster \\-yD fs.overwrite-files=true examples/batch/WordCount.jar \\-yD fs.overwrite-files=true \\ -yD taskmanager.network.numberOfBuffers=16368 Application Mode提交任务 application 模式使用 flink run-application 提交作业；通过 -t 指定部署环境，目前 application 模式支持部署在 yarn 上(-t yarn-application) 和 k8s 上(-t kubernetes-application）；并支持通过 -D 参数指定通用的 运行配置，比如 jobmanager&#x2F;taskmanager 内存、checkpoint 时间间隔等。 Tips通过 flink run-application -h 可以看到 -D&#x2F;-t 的详细说明 任务提交 带有 JM 和 TM 内存设置的命令提交, 并且自己设置 TaskManager slots 个数为3，以及指定并发数为3 123456flink run-application -t yarn-application -p 3 \\-Djobmanager.memory.process.size=1024m \\-Dtaskmanager.memory.process.size=1024m \\-Dyarn.application.name=&quot;MyFlinkWordCount&quot; \\-Dtaskmanager.numberOfTaskSlots=3 \\/export/server/flink/examples/batch/WordCount.jar --output hdfs://node1:8020/wordcount/output_52 指定并发还可以使用如下命令-Dparallelism.default=3来代替 -p 3 12345678# 指定并发还可以使用 -Dparallelism.default=3，而且社区目前倾向使用 -D+通用配置代替客户端命令参数(比如 -p)。flink run-application -t yarn-application \\-Dparallelism.default=3 \\-Djobmanager.memory.process.size=1024m \\-Dtaskmanager.memory.process.size=1024m \\-Dyarn.application.name=&quot;MyFlinkWordCount&quot; \\-Dtaskmanager.numberOfTaskSlots=3 \\/export/server/flink/examples/batch/WordCount.jar --output hdfs://node1:8020/wordcount/output_53 任务提交yarn.provided.lib.dirs参数 yarn.provided.lib.dirs参数一起使用，可以充分发挥 application 部署模式的优势 但是, 如果自己指定 yarn.provided.lib.dirs，有以下注意事项： 需要将 lib 包和 plugins 包地址用;分开，从上面的例子中也可以看到，将 plugins 包放在 lib 目录下可能会有包冲突错误 plugins 包路径地址必须以 plugins 结尾，例如上面例子中的 hdfs:&#x2F;&#x2F;node1.itcast.cn:8020&#x2F;flink&#x2F;plugins hdfs 路径必须指定 nameservice(或 active namenode 地址)，而不能使用简化方式(例如 hdfs:&#x2F;&#x2F;node1.itcast.cn:8020&#x2F;flink&#x2F;libs) 该种模式的操作使得 flink 作业提交变得很轻量，因为所需的 Flink jar 包和应用程序 jar 将到指定的远程位置获取，而不是由客户端下载再发送到集群。这也是社区在 flink-1.11 版本引入新的部署模式的意义所在。 1234567891011121314151617181920212223242526272829303132333435# 上传 Flink 相关 plugins 到hdfscd /export/server/flink/pluginshdfs dfs -mkdir /flink/pluginshdfs dfs -put \\external-resource-gpu/flink-external-resource-gpu-1.15.2.jar \\metrics-datadog/flink-metrics-datadog-1.15.2.jar \\metrics-graphite/flink-metrics-graphite-1.15.2.jar \\metrics-influx/flink-metrics-influxdb-1.15.2.jar \\metrics-jmx/flink-metrics-jmx-1.15.2.jar \\metrics-prometheus/flink-metrics-prometheus-1.15.2.jar \\metrics-slf4j/flink-metrics-slf4j-1.15.2.jar \\metrics-statsd/flink-metrics-statsd-1.15.2.jar \\/flink/plugins# 根据自己业务需求上传相关的 jarcd /export/server/flink/libhdfs dfs -mkdir /flink/libhdfs dfs -put ./* /flink/lib# 上传用户需要运行的作业jar 到 hdfscd /export/server/flinkhdfs dfs -mkdir /flink/user-libshdfs dfs -put ./examples/batch/WordCount.jar /flink/user-libs# 提交任务flink run-application -t yarn-application \\-Djobmanager.memory.process.size=1024m \\-Dtaskmanager.memory.process.size=1024m \\-Dtaskmanager.numberOfTaskSlots=2 \\-Dparallelism.default=2 \\-Dyarn.provided.lib.dirs=&quot;hdfs://node1.itcast.cn:8020/flink/lib;hdfs://node1.itcast.cn:8020/flink/plugins&quot; \\-Dyarn.application.name=&quot;batchWordCount&quot; \\hdfs://node1.itcast.cn:8020/flink/user-libs/WordCount.jar \\--output hdfs://node1:8020/wordcount/output_54# 也可以将 yarn.provided.lib.dirs 配置到 conf/flink-conf.yaml，这时提交作业就和普通作业没有区别了： 模式切换注意事项 如果使用的是flink on yarn方式，想切换回standalone模式的话，需要删除文件：&#x2F;tmp&#x2F;.yarn-properties-因为默认查找当前yarn集群中已有的yarn-session信息中的jobmanager 如果是分离模式运行的YARN JOB后，其运行完成会自动删除这个文件 但是会话模式的话，如果是kill掉任务，其不会执行自动删除这个文件的步骤，所以需要我们手动删除这个文件。","categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"author":"Johnson Liam"},{"title":"【Flink】Flink技术栈(Theory及集群部署)","slug":"2023.09.05","date":"2023-09-04T16:00:00.000Z","updated":"2023-09-15T13:04:28.000Z","comments":true,"path":"2023/09/05/2023.09.05/","link":"","permalink":"http://example.com/2023/09/05/2023.09.05/","excerpt":"","text":"流式计算什么是数据流?什么是数据集 批处理对应的是数据集 流处理对应的是数据流 无界数据流和有界数据流 有界数据流：明确定义开始和结束的数据流，计算之前获取的所有数据-mysql、日志文件 无界数据流：只有开始没有结束的数据流，获取数据立即处理，无法等待所有数据到达-socket、kafka 硬核Flink介绍基本介绍这里先贴上一个官方链接🔗保命: https://flink.apache.org/ 第1代—Hadoop MapReduce 首先第一代的计算引擎，无疑就是 Hadoop 承载的 MapReduce。它将计算分为两个阶段，分别为 Map 和 Reduce。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 第2代—DAG框架（Tez） + MapReduce 由于这样的弊端，催生了支持 DAG 框架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。 第3代—Spark 接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及强调的实时计算。在这里，很多人也会认为第三代计算引擎也能够很好的运行批处理的 Job。 离线计算、实时计算、SQL高层API 自带DAG 内存迭代计算，性能大幅提升 第4代—Flink 随着第三代计算引擎的出现，促进了上层应用快速发展，例如各种迭代计算的性能以及对流计算和 SQL 等的支持。Flink 的诞生就被归在了第四代。这应该主要表现在 Flink 对流计算的支持，以及更一步的实时性上面。当然 Flink 也可以支持 Batch 的任务，以及 DAG 的运算。 离线计算、实时计算、SQL高层API 自带DAG 流式计算性能更好，可靠性更高 Flink流处理特性 支持高吞吐、低延迟（亚秒）、高性能的流处理 支持带有事件时间的窗口（Window）操作 支持有状态计算端到端的Exactly-once语义 支持高度灵活的窗口（Window）操作，支持基于time、count、session，以及data-driven的窗口操作 支持具有Backpressure功能的持续流模型 支持基于轻量级分布式快照（Snapshot）实现的容错 一个运行时同时支持Batch on Streaming处理和Streaming处理 Flink在JVM内部实现了自己的内存管理 支持迭代计算 支持程序自动优化：避免特定情况下Shuffle、排序等昂贵操作，中间结果有必要进行缓存 Flink的应用场景 Event-driven Applications（事件驱动） 信用卡交易、刷单、监控 Data Analytics Applications（数据分析） 双11大屏、库存分析 Data Pipeline Applications（数据管道） 用于数据提取-转换-加载在存储系统间进行数据转换和迁移的管道 对于批处理ETL是一个周期性的任务 对于流处理ETL是一个持续提取-转换-加载的管道 Flink架构体系 Flink是典型的主从架构(浅浅总结一下吧!) 主从架构：hadoop(hdfs mapreduce yarn)、spark、hbase、flink 去中心化架构：zookeeper、Kafka 所有的 Flink 程序都可以归纳为由三部分构成： Source：表示“源算子”，负责读取数据源。 Transformation：表示“转换算子”，利用各种算子进行处理加工。 Sink：表示“下沉算子”，负责数据的输出。 系统整体构成 Client：Flink 客户端是 F1ink 提供的 CLI 命令行工具，用来提交 Flink 作业到 Flink 集群，在客户端中负责 StreamGraph (流图)和 Job Graph (作业图)的构建。 JobManager：JobManager处理器也称之为Master，JobManager根据并行度将Flink客户端提交的Flink应用分解为子任务，从资源管理器 ResourceManager 申请所需的计算资源，资源具备之后，开始分发任务到TaskManager 执行 Task，并负责应用容错，跟踪作业的执行状态，发现异常则恢复作业等。 TaskManager：TaskManager处理器也称之为Worker，TaskManager 接收 JobManage 分发的子任务，根据自身的资源情况管理子任务的启动、 停止、销毁、异常恢复等生命周期阶段。Flink运行时至少会存在一个worker处理器。 运行时架构 作业管理器-JobManager(包含Dispatcher, JobMaster, ResourceManager) 分发器（Dispatcher） Dispatcher 主要负责提供一个 REST 接口，用来提交应用，并且负责为每一个新提交的作业启动一个新的 JobMaster 组件。Dispatcher 也会启动一个 Web UI，用来方便地展示和监控作业执行的信息。 作业处理器（JobMaster） JobMaster 是 JobManager 中最核心的组件，负责处理单独的作业（Job），JobMaster和每个作业是一一对应的。 资源管理器（ResourceManager） ResourceManager 主要负责资源的分配和管理，在 Flink 集群中只有一个。所谓“资源”，主要是指 TaskManager 的任务槽（task slots）。任务槽就是 Flink 集群中的资源调配单元，包含了机器用来执行计算的一组 CPU 和内存资源。每一个任务（Task）都需要分配到一个 slot 上执行。 任务管理器-TaskManager Slot 任务执行槽 物理概念，一个TM(TaskManager)内会划分出多个Slot，1个Slot内最多可以运行1个Task(Subtask)或一组由Task(Subtask)组成的任务链。 多个Slot之间会共享平分当前TM的内存空间。Slot是对一个TM的资源进行固定分配的工具，每个Slot在TM启动后，可以获得固定的资源。 flink集群只要启动，TaskManager设置的资源就是固定，就意味着每个slot资源也是固定。 通过集群的配置文件来设定 TaskManager 的 slot 数量taskmanager.numberOfTaskSlots:8 注意：slot 目前仅仅用来隔离内存，不会涉及 CPU 的隔离。在具体应用时，可以将 slot 数量配置为机器的 CPU 核心数，尽量避免不同任务之间对 CPU 的竞争。这也是开发环境默认并行度设为机器 CPU 数量的原因。 Flink计算优化Flink并行计算 如何设置： 代码中设置 123456# 我们在代码中，可以很简单地在算子后跟着调用 setParallelism()方法，来设置当前算子的并行度,这种方式设置的并行度，只针对当前算子有效：stream.map(word -&gt; Tuple2.of(word, 1L)).setParallelism(2);# 我们也可以直接调用执行环境的 setParallelism()方法，全局设定并行度：env.setParallelism(2);# 这样代码中所有算子，默认的并行度就都为 2 了。我们一般不会在程序中设置全局并行度，因为如果在程序中对全局并行度进行硬编码，会导致无法动态扩容。 提交应用时设置 12# 在使用 flink run 命令提交应用时，可以增加-p 参数来指定当前应用程序执行的并行度，它的作用类似于执行环境的全局设置：bin/flink run –p 2 –c org.apache.flink.examples.java.wordcount.WordCount /export/server/flink/examples/batch/WordCount.jar 配置文件中设置 12# 直接在集群的配置文件 flink-conf.yaml 中直接更改默认并行度：parallelism.default: 2 优先级：算子 &gt; 代码全局 &gt; 命令行参数 &gt; 配置文件 Flink合并算子链（Operator Chain）什么是算子链: (算子间的数据传输) 一对一（One-to-one，forwarding） 这种模式下，数据流维护着分区以及元素的顺序。比如图中的 source 和 map 算子，source算子读取数据之后，可以直接发送给 map 算子做处理，它们之间不需要重新分区，也不需要调整数据的顺序。这就意味着 map 算子的子任务，看到的元素个数和顺序跟 source 算子的子任务产生的完全一样，保证着“一对一”的关系。map、filter、flatMap 等算子都是这种 one-to-one的对应关系。这种关系类似于 Spark 中的窄依赖。 重分区（Redistributing） 在这种模式下，数据流的分区会发生改变。比图中的 map 和后面的 keyBy&#x2F;window 算子之间（这里的 keyBy 是数据传输算子，后面的 window、apply 方法共同构成了 window 算子）,以及 keyBy&#x2F;window 算子和 Sink 算子之间，都是这样的关系。每一个算子的子任务，会根据数据传输的策略，把数据发送到不同的下游目标任务。例如，keyBy()是分组操作，本质上基于键（key）的哈希值（hashCode）进行了重分区；而当并行度改变时，比如从并行度为 2 的 window 算子，要传递到并行度为 1 的 Sink 算子，这时的数据传输方式是再平衡（rebalance），会把数据均匀地向下游子任务分发出去。这些传输方式都会引起重分区（redistribute）的过程，这一过程类似于 Spark 中的 shuffle。这种算子间的关系类似于 Spark 中的宽依赖。 合并算子链 合并算子链是flink作业自动进行的优化，当然可以手动关闭 在 Flink 中，并行度相同的一对一（one to one）算子操作，可以直接链接在一起形成一个“大”的任务（task），这样原来的算子就成为了真正任务里的一部分，每个 task会被一个线程执行。这样的技术被称为“算子链”（Operator Chain）。 将算子链接成 task 是非常有效的优化：可以减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。 关闭合并算子链, 一般不这么做 12345在代码中对算子做一些特定的设置：// 禁用算子链.map(word -&gt; Tuple2.of(word, 1L)).disableChaining();// 从当前算子开始新链.map(word -&gt; Tuple2.of(word, 1L)).startNewChain() Flink任务槽的共享 引入, 因为一个算子就要占用一个slot, 且多个资源相互隔离有什么问题？ 存在比较多的网络io Flink 默认是允许 slot 共享的 如果希望某个算子对应的任务完全独占一个 slot，或者只有某一部分算子共享 slot，我们也可以通过设置“slot 共享组”（SlotSharingGroup）手动指定：.map(word -&gt; Tuple2.of(word, 1L)).slotSharingGroup(“1”); 什么是任务槽的共享每个任务节点的并行子任务一字排开，占据不同的 slot；而不同的任务节点的子任务可以共享 slot。一个 slot 中，可以将程序处理的所有任务都放在这里执行，我们把它叫作保存了整个作业的运行管道（pipeline）。 只要属于同一个作业，那么对于不同任务节点的并行子任务，就可以放到同一个 slot 上执行。 任务槽和并行度的关系※ 概念上 slot是静态的概念, 是指TaskManager具有的并发执行能力, 可以通过参数taskmanager.numberOfTaskSlots 进行配置；而并行度（parallelism）是动态概念，也就是TaskManager 运行程序时实际使用的并发能力，可以通过参数 parallelism.default 进行配置。 同一个任务节点的并行子任务是不能共享 slot 的，所以允许 slot 共享之后，运行作业所需的 slot 数量正好就是作业中所有算子并行度的最大值。 同一个slot中不能有同一个任务的两个及以上的并行子任务 流图转换 由 Flink 程序直接映射成的数据流图（dataflow graph），也被称为逻辑流图（logicalStreamGraph），因为它们表示的是计算逻辑的高级视图。到具体执行环节时，我们还要考虑并行子任务的分配、数据在任务间的传输，以及合并算子链的优化。为了说明最终应该怎样执行一个流处理程序，Flink 需要将逻辑流图进行解析，转换为物理数据流图。 在这个转换过程中，有几个不同的阶段，会生成不同层级的图，其中最重要的就是作业图（JobGraph）和执行图（ExecutionGraph）。Flink 中任务调度执行的图，按照生成顺序可以分成四层： 逻辑流图（StreamGraph）→ 作业图（JobGraph）→ 执行图（ExecutionGraph）→ 物理图（Physical Graph） 1.逻辑流图（一般在客户端完成）dataflow 这个是根据用户自己写的DataStream API代码生成的DAG图，用来表示程序的拓扑结构。 2.作业图（一般在客户端完成）job graph 逻辑流图经过优化之后生成的就是作业图，会随着作业一起提交给JobManager。 主要优化的是将多个符合条件的节点进行合并，形成算子链。 3.执行图（在JobMaster中完成）execute graph JobMaster收到作业图后，会生成执行图，执行图是作业图的并行化版本，按照真正的并行度进行拆分。 4.物理图 Jobmaster生成执行图后，会将执行图分发给TaskManager，由TM根据执行图生成“物理图”。 Flink集群搭建 Flink支持多种安装模式。 local（本地）——本地模式 standalone——独立模式，Flink自带集群，开发测试环境使用 standaloneHA—独立集群高可用模式，Flink自带集群，开发测试环境使用 yarn——计算资源统一由Hadoop YARN管理，生产环境测试 Standalone Flink程序需要提交给JobClient JobClient将作业提交给obManager JobManager负责协调资源分配和作业执行。 资源分配完成后，任务将提交给相应的TaskManager TaskManager启动一个线程以开始执行。TaskManager会向JobManager报告状态更改。例如开始执行，正在进行或已完成。 作业执行完成后，结果将发送回客户端（JobClient） 环境准备 jdk11及以上【配置JAVA_HOME环境变量】 ssh免密码登录【集群内节点之间免密登录】 下载安装包https://dlcdn.apache.org/flink/flink-1.15.2/flink-1.15.2-bin-scala_2.12.tgz 服务器规划服务器: node1(Master +Worker) 安装步骤 上传Flink压缩包到指定目录 解压缩flink到 &#x2F;export&#x2F;server 目录 1tar -zxvf flink-1.15.2-bin-scala_2.12.tgz -C /export/server/ 改名或创建软链接：方便后期升级 12cd /export/serverln -s flink-1.15.2/ /export/server/flink 修改配置文件flink-conf.yaml 12cd /export/server/flinkvim conf/flink-conf.yaml 将下面两个参数注释掉或者值改为node1（这里我们采取第一种注释掉） 第二种方式 启动Flink 1bin/start-cluster.sh 通过jps查看进程信息 访问web界面http://node1:8081 运行测试任务 1bin/flink run /export/server/flink/examples/batch/WordCount.jar 日志的查看JobManager 和 TaskManager 的启动日志可以在 Flink binary 目录下的 log 子目录中找到 log 目录中以“flink-${user}-standalonesession-${id}-${hostname}”为前缀的文件对应的即是 JobManager 的输出，其中有三个文件： 1234- flink-$&#123;user&#125;-standalonesession-$&#123;id&#125;-$&#123;hostname&#125;.log：代码中的日志输出 - flink-$&#123;user&#125;-standalonesession-$&#123;id&#125;-$&#123;hostname&#125;.out：进程执行时的 stdout 输出 - flink-$&#123;user&#125;-standalonesession-$&#123;id&#125;-$&#123;hostname&#125;-gc.log：JVM 的 GC 的日志log 目录中以“flink-$&#123;user&#125;-taskexecutor-$&#123;id&#125;-$&#123;hostname&#125;”为前缀的文件对应的是 TaskManager 的输出，也包括三个文件，和 JobManager 的输出一致。 日志的配置文件在 Flink binary 目录的 conf 子目录下： 12345678910111213- log4j-cli.properties：用 Flink 命令行时用的 log 配置，比如执行“flink run”命令- log4j-console.properties：JobManagers/TaskManagers 在前台模式运行时使用（例如 Kubernetes）；- log4j-session.properties：是用 yarn-session.sh或Kubernetes session时启动时命令行执行时用的 log 配置log4j.properties：无论是 standalone 还是 yarn 模式，JobManager 和 TaskManager 上用 的 log 配置都是 log4j.properties这三个“log4j.*properties”文件分别有三个“logback.*xml”文件与之对应:- log4j-console.properties -&gt; logback-console.xml - log4j-session.properties -&gt; logback-session.xml - log4j.properties -&gt; logback.xml如果想使用 logback 的同学， 需要从 lib 目录中移除 log4j-slf4j-impl jars；向 lib 目录中添加 logback-core 和 logback-classic jars。如果启用了 logback，则会自动使用”logback.*xml”这些文件需要注意的是，“flink-$&#123;user&#125;-standalonesession-$&#123;id&#125;-$&#123;hostname&#125;”和“flink-$&#123;user&#125;- taskexecutor-$&#123;id&#125;-$&#123;hostname&#125;”都带有“$&#123;id&#125;”，“$&#123;id&#125;”表示本进程在本机上该角色（JobManager 或 TaskManager）的所有进程中的启动顺序，默认从 0 开始。 yarn集群环境准备工作 jdk1.8及以上（推荐jdk11）【配置JAVA_HOME环境变量】 ssh免密码登录【集群内节点之间免密登录】 至少hadoop2.8.5 hdfs &amp; yarn均启动 集群规划服务器: node1(Master +Worker) 服务器: node2(Worker) 服务器: node3(Worker) 修改hadoop的配置参数 打开yarn配置页面（每台hadoop节点都需要修改） 12345678vim /export/server/hadoop/etc/hadoop/yarn-site.xml# 添加&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;# 是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。# 在这里面我们需要关闭，因为对于flink使用yarn模式下，很容易内存超标，这个时候yarn会自动杀掉job 分发yarn-site.xml到其它服务器节点 12scp yarn-site.xml node2:$PWDscp yarn-site.xml node3:$PWD 启动ZK、HDFS、YARN集群 Flink On Yarn在企业实际开发中，使用Flink时，更多的使用方式是Flink On Yarn模式，原因如下： Yarn的资源可以按需使用，提高集群的资源利用率 Yarn的任务有优先级，根据优先级运行作业 基于Yarn调度系统，能够自动化地处理各个角色的 Failover(容错) JobManager 进程和 TaskManager 进程都由 Yarn NodeManager 监控 如果 JobManager 进程异常退出，则 Yarn ResourceManager 会重新调度 JobManager 到其他机器 如果 TaskManager 进程异常退出，JobManager 会收到消息并重新向 Yarn ResourceManager 申请资源，重新启动 TaskManager 运行机制 Yarn的客户端需要获取hadoop的配置信息，连接Yarn的ResourceManager。所以要有设置有 YARN_CONF_DIR或者HADOOP_CONF_DIR或者HADOOP_CONF_PATH,只要设置了其中一个环境变量，就会被读取。如果读取上述的变量失败了，那么将会选择hadoop_home的环境变量，读取成功将会尝试加载$HADOOP_HOME&#x2F;etc&#x2F;hadoop的配置文件。 Session模式 这种模式会预先在yarn或者k8s上启动一个flink集群，然后将作业提交到这个集群上，这种模式，集群中的作业使用相同的资源，如果某一个作业出现了问题导致整个集群挂掉，那就得重启集群中的所有任务，这样就会给集群造成很大的负面影响。 特点：需要事先启动一个flink集群申请资源，提前启动JobManager和TaskManager 优点：不需要每次提交作业都申请资源，而是使用已经申请好的资源，从而执行效率高 缺点：作业每次执行完不会释放资源，因此会一起占用资源 应用场景：比较适合于提交比较频繁的场景，小作业比较多 Per-Job模式（1.15废弃）考虑到集群的资源隔离情况，一般生产上的任务都会选择per job模式，也就是每个作业启动一个flink集群，各个集群之间独立运行，互不影响,且每个集群可以设置独立的配置。 特点：每次提交作业都需要申请一次资源（每个作业都会创建一个JobManager） 优点：作业运行完成后，资源会被释放 缺点：每次提交都重新申请资源，会影响执行效率 应用场景：适合作业比较少的场景，大作业的情况下 Application模式flink-1.11引入了一种新的部署模式，即 Application 模式。目前，flink-1.11 已经可以支持基于 Yarn 和 Kubernetes 的 Application 模式。 Session模式：所有作业共享集群资源，隔离性差，JM 负载瓶颈，main 方法在客户端执行。 Per-Job模式：每个作业单独启动集群，隔离性好，JM 负载均衡，main 方法在客户端执行。 通过以上两种模式的特点描述，可以看出，main方法都是在客户端执行，社区考虑到在客户端执行 main() 方法来获取 flink 运行时所需的依赖项，并生成 JobGraph，提交到集群的操作都会在实时平台所在的机器上执行，那么将会给服务器造成很大的压力。尤其在大量用户共享客户端时，问题更加突出。 Application 模式下，用户程序的 main 方法将在集群中而不是客户端运行，用户将程序逻辑和依赖打包进一个可执行的 jar 包里，集群的入口程序 (ApplicationClusterEntryPoint) 负责调用其中的 main 方法来生成 JobGraph。Application 模式为每个提交的应用程序创建一个集群，该集群可以看作是在特定应用程序的作业之间共享的会话集群，并在应用程序完成时终止。 Fink On Yarn三种模式比较 yarn session：需要事先在yarn集群上启动一个flink集群，所有的flink作业共享这个集群，资源利用率高，程序执行效率高，但是程序运行结束不会释放资源，程序之间的隔离性较差，同时main方法运行客户端上，作业提交多 yarn per-job：不需要事先启动flink集群，每提交一个flink作业会启动一个flink集群，作业与作业之间资源互不影响，隔离性较好，作业运行结束会释放资源，同时main方法运行客户端上，作业提交少 yarn application：main方法运行在集群上，其它与per-job相同","categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"},{"name":"Interview","slug":"Interview","permalink":"http://example.com/tags/Interview/"}],"author":"Johnson Liam"}],"categories":[],"tags":[{"name":"Flink","slug":"Flink","permalink":"http://example.com/tags/Flink/"},{"name":"Interview","slug":"Interview","permalink":"http://example.com/tags/Interview/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"HQL","slug":"HQL","permalink":"http://example.com/tags/HQL/"}]}